<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 10.0.1"/>
    <title>mridc.utils.exp_manager API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;}nav.pdoc{--pad:1.75rem;--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc li{display:block;margin:0;padding:.2rem 0 .2rem var(--indent);transition:all 100ms;}nav.pdoc > div > ul > li{padding-left:0;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.pdoc details{filter:opacity(1);}.pdoc details:not([open]){height:0;}.pdoc details > summary{position:absolute;top:-35px;right:0;font-size:.75rem;color:var(--muted);padding:0 .7em;user-select:none;cursor:pointer;}.pdoc details > summary:focus{outline:0;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc > section:first-of-type > .docstring{margin-bottom:2.5rem;}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc .headerlink{position:absolute;width:0;margin-left:-1.5rem;line-height:1.4rem;font-size:1.5rem;font-weight:normal;transition:all 100ms ease-in-out;opacity:0;user-select:none;}.pdoc .attr > .headerlink{margin-left:-2.5rem;}.pdoc *:hover > .headerlink,.pdoc *:target > .attr > .headerlink{opacity:1;}.pdoc .attr{display:block;color:var(--text);margin:.5rem 0 .5rem;padding:.4rem 5rem .4rem 1rem;background-color:var(--accent);}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{white-space:pre-wrap;}.pdoc .annotation{color:var(--annotation);}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../utils.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;mridc.utils</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



        <h2>API Documentation</h2>
            <ul class="memberlist">
            <li>
                    <a class="class" href="#NotFoundError">NotFoundError</a>
                            <ul class="memberlist">
                </ul>

            </li>
            <li>
                    <a class="class" href="#LoggerMisconfigurationError">LoggerMisconfigurationError</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LoggerMisconfigurationError.__init__">LoggerMisconfigurationError</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#CheckpointMisconfigurationError">CheckpointMisconfigurationError</a>
                            <ul class="memberlist">
                </ul>

            </li>
            <li>
                    <a class="class" href="#CallbackParams">CallbackParams</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#CallbackParams.__init__">CallbackParams</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.filepath">filepath</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.dirpath">dirpath</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.filename">filename</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.monitor">monitor</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.verbose">verbose</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.save_last">save_last</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.save_top_k">save_top_k</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.save_weights_only">save_weights_only</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.mode">mode</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.every_n_epochs">every_n_epochs</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.prefix">prefix</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.postfix">postfix</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.save_best_model">save_best_model</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.always_save_mridc">always_save_mridc</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.save_mridc_on_train_end">save_mridc_on_train_end</a>
                        </li>
                        <li>
                                <a class="variable" href="#CallbackParams.model_parallel_size">model_parallel_size</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#StepTimingParams">StepTimingParams</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#StepTimingParams.__init__">StepTimingParams</a>
                        </li>
                        <li>
                                <a class="variable" href="#StepTimingParams.reduction">reduction</a>
                        </li>
                        <li>
                                <a class="variable" href="#StepTimingParams.sync_cuda">sync_cuda</a>
                        </li>
                        <li>
                                <a class="variable" href="#StepTimingParams.buffer_size">buffer_size</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#ExpManagerConfig">ExpManagerConfig</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ExpManagerConfig.__init__">ExpManagerConfig</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.explicit_log_dir">explicit_log_dir</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.exp_dir">exp_dir</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.name">name</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.version">version</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.use_datetime_version">use_datetime_version</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.resume_if_exists">resume_if_exists</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.resume_past_end">resume_past_end</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.resume_ignore_no_checkpoint">resume_ignore_no_checkpoint</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.create_tensorboard_logger">create_tensorboard_logger</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.summary_writer_kwargs">summary_writer_kwargs</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.create_wandb_logger">create_wandb_logger</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.wandb_logger_kwargs">wandb_logger_kwargs</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.create_checkpoint_callback">create_checkpoint_callback</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.checkpoint_callback_params">checkpoint_callback_params</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.files_to_copy">files_to_copy</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.log_step_timing">log_step_timing</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.step_timing_kwargs">step_timing_kwargs</a>
                        </li>
                        <li>
                                <a class="variable" href="#ExpManagerConfig.model_parallel_size">model_parallel_size</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="class" href="#TimingCallback">TimingCallback</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#TimingCallback.__init__">TimingCallback</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_train_batch_start">on_train_batch_start</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_train_batch_end">on_train_batch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_validation_batch_start">on_validation_batch_start</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_validation_batch_end">on_validation_batch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_test_batch_start">on_test_batch_start</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_test_batch_end">on_test_batch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_before_backward">on_before_backward</a>
                        </li>
                        <li>
                                <a class="function" href="#TimingCallback.on_after_backward">on_after_backward</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#exp_manager">exp_manager</a>
            </li>
            <li>
                    <a class="function" href="#error_checks">error_checks</a>
            </li>
            <li>
                    <a class="function" href="#check_resume">check_resume</a>
            </li>
            <li>
                    <a class="function" href="#check_explicit_log_dir">check_explicit_log_dir</a>
            </li>
            <li>
                    <a class="function" href="#get_log_dir">get_log_dir</a>
            </li>
            <li>
                    <a class="function" href="#get_git_hash">get_git_hash</a>
            </li>
            <li>
                    <a class="function" href="#get_git_diff">get_git_diff</a>
            </li>
            <li>
                    <a class="class" href="#LoggerList">LoggerList</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#LoggerList.__init__">LoggerList</a>
                        </li>
                        <li>
                                <a class="variable" href="#LoggerList.name">name</a>
                        </li>
                        <li>
                                <a class="variable" href="#LoggerList.version">version</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#configure_loggers">configure_loggers</a>
            </li>
            <li>
                    <a class="class" href="#MRIDCModelCheckpoint">MRIDCModelCheckpoint</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#MRIDCModelCheckpoint.__init__">MRIDCModelCheckpoint</a>
                        </li>
                        <li>
                                <a class="function" href="#MRIDCModelCheckpoint.mridc_topk_check_previous_run">mridc_topk_check_previous_run</a>
                        </li>
                        <li>
                                <a class="function" href="#MRIDCModelCheckpoint.on_save_checkpoint">on_save_checkpoint</a>
                        </li>
                        <li>
                                <a class="function" href="#MRIDCModelCheckpoint.on_train_end">on_train_end</a>
                        </li>
                </ul>

            </li>
            <li>
                    <a class="function" href="#configure_checkpointing">configure_checkpointing</a>
            </li>
            <li>
                    <a class="function" href="#check_slurm">check_slurm</a>
            </li>
            <li>
                    <a class="class" href="#StatelessTimer">StatelessTimer</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#StatelessTimer.on_save_checkpoint">on_save_checkpoint</a>
                        </li>
                        <li>
                                <a class="function" href="#StatelessTimer.on_load_checkpoint">on_load_checkpoint</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section>
                    <h1 class="modulename">
<a href="./../../mridc.html">mridc</a><wbr>.<a href="./../utils.html">utils</a><wbr>.exp_manager    </h1>

                
                        <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="c1"># encoding: utf-8</span>
<span class="n">__author__</span> <span class="o">=</span> <span class="s2">&quot;Dimitrios Karkalousos&quot;</span>

<span class="c1"># Taken and adapted from: https://github.com/NVIDIA/NeMo/blob/main/nemo/utils/exp_manager.py</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">timedelta</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">shutil</span> <span class="kn">import</span> <span class="n">copy</span><span class="p">,</span> <span class="n">move</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">hydra.core.hydra_config</span> <span class="kn">import</span> <span class="n">HydraConfig</span>
<span class="kn">from</span> <span class="nn">hydra.utils</span> <span class="kn">import</span> <span class="n">get_original_cwd</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">OmegaConf</span><span class="p">,</span> <span class="n">open_dict</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">Callback</span><span class="p">,</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks.timer</span> <span class="kn">import</span> <span class="n">Timer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.loggers</span> <span class="kn">import</span> <span class="n">LoggerCollection</span> <span class="k">as</span> <span class="n">_LoggerCollection</span><span class="p">,</span> <span class="n">TensorBoardLogger</span><span class="p">,</span> <span class="n">WandbLogger</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.plugins.training_type.ddp</span> <span class="kn">import</span> <span class="n">DDPPlugin</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.trainer.states</span> <span class="kn">import</span> <span class="n">RunningStage</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.distributed</span> <span class="kn">import</span> <span class="n">rank_zero_info</span>

<span class="kn">from</span> <span class="nn">mridc.constants</span> <span class="kn">import</span> <span class="n">MRIDC_ENV_VARNAME_TESTING</span><span class="p">,</span> <span class="n">MRIDC_ENV_VARNAME_VERSION</span>
<span class="kn">from</span> <span class="nn">mridc.utils</span> <span class="kn">import</span> <span class="n">logging</span><span class="p">,</span> <span class="n">timers</span>
<span class="kn">from</span> <span class="nn">mridc.utils.app_state</span> <span class="kn">import</span> <span class="n">AppState</span>
<span class="kn">from</span> <span class="nn">mridc.utils.env_var_parsing</span> <span class="kn">import</span> <span class="n">get_envbool</span>
<span class="kn">from</span> <span class="nn">mridc.utils.exceptions</span> <span class="kn">import</span> <span class="n">MRIDCBaseException</span>
<span class="kn">from</span> <span class="nn">mridc.utils.get_rank</span> <span class="kn">import</span> <span class="n">is_global_rank_zero</span>
<span class="kn">from</span> <span class="nn">mridc.utils.lightning_logger_patch</span> <span class="kn">import</span> <span class="n">add_filehandlers_to_pl_logger</span>


<span class="k">class</span> <span class="nc">NotFoundError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a file or folder is not found&quot;&quot;&quot;</span>


<span class="k">class</span> <span class="nc">LoggerMisconfigurationError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a mismatch between trainer.logger and exp_manager occurs&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">message</span> <span class="o">+</span> <span class="s2">&quot;You can disable lightning&#39;s trainer from creating a logger by passing logger=False to its &quot;</span>
            <span class="s2">&quot;constructor. &quot;</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CheckpointMisconfigurationError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a mismatch between trainer.callbacks and exp_manager occurs&quot;&quot;&quot;</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CallbackParams</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Parameters for a callback&quot;&quot;&quot;</span>

    <span class="n">filepath</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Deprecated</span>
    <span class="n">dirpath</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">monitor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">save_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">save_top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">save_weights_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;min&quot;</span>
    <span class="n">every_n_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">postfix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;.mridc&quot;</span>
    <span class="n">save_best_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">always_save_mridc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">save_mridc_on_train_end</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Automatically save .mridc file during on_train_end hook</span>
    <span class="n">model_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StepTimingParams</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Parameters for the step timing callback.&quot;&quot;&quot;</span>

    <span class="n">reduction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>
    <span class="c1"># if True torch.cuda.synchronize() is called on start/stop</span>
    <span class="n">sync_cuda</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># if positive, defines the size of a sliding window for computing mean</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>


<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ExpManagerConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Configuration for the experiment manager.&quot;&quot;&quot;</span>

    <span class="c1"># Log dir creation parameters</span>
    <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">use_datetime_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">resume_if_exists</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">resume_past_end</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Logging parameters</span>
    <span class="n">create_tensorboard_logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">summary_writer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">create_wandb_logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">wandb_logger_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Checkpointing parameters</span>
    <span class="n">create_checkpoint_callback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">checkpoint_callback_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackParams</span><span class="p">]</span> <span class="o">=</span> <span class="n">CallbackParams</span><span class="p">()</span>
    <span class="c1"># Additional exp_manager arguments</span>
    <span class="n">files_to_copy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># logs timing of train/val/test steps</span>
    <span class="n">log_step_timing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">step_timing_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StepTimingParams</span><span class="p">]</span> <span class="o">=</span> <span class="n">StepTimingParams</span><span class="p">()</span>
    <span class="n">model_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>


<span class="k">class</span> <span class="nc">TimingCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logs execution time of train/val/test steps&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize TimingCallback&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">timer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">=</span> <span class="n">timers</span><span class="o">.</span><span class="n">NamedTimer</span><span class="p">(</span><span class="o">**</span><span class="n">timer_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_on_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the beginning of each batch&quot;&quot;&quot;</span>
        <span class="c1"># reset only if we do not return mean of a sliding window</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">buffer_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of each batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">pl_module</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the beginning of each training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_validation_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_validation_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_test_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_test_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_before_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken for backward pass&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Note: this is called after the optimizer step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">exp_manager</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    exp_manager is a helper function used to manage folders for experiments. It follows the pytorch lightning paradigm</span>
<span class="sd">    of exp_dir/model_or_experiment_name/version. If the lightning trainer has a logger, exp_manager will get exp_dir,</span>
<span class="sd">    name, and version from the logger. Otherwise it will use the exp_dir and name arguments to create the logging</span>
<span class="sd">    directory. exp_manager also allows for explicit folder creation via explicit_log_dir.</span>
<span class="sd">    The version can be a datetime string or an integer. Datetime version can be disabled if use_datetime_version is set</span>
<span class="sd">     to False. It optionally creates TensorBoardLogger, WandBLogger, ModelCheckpoint objects from pytorch lightning.</span>
<span class="sd">    It copies sys.argv, and git information if available to the logging directory. It creates a log file for each</span>
<span class="sd">    process to log their output into.</span>
<span class="sd">    exp_manager additionally has a resume feature (resume_if_exists) which can be used to continuing training from</span>
<span class="sd">    the constructed log_dir. When you need to continue the training repeatedly (like on a cluster which you need</span>
<span class="sd">    multiple consecutive jobs), you need to avoid creating the version folders. Therefore from v1.0.0, when</span>
<span class="sd">    resume_if_exists is set to True, creating the version folders is ignored.</span>
<span class="sd">    Args:</span>
<span class="sd">        trainer (Trainer): The lightning trainer.</span>
<span class="sd">        cfg (DictConfig, dict): Can have the following keys:</span>
<span class="sd">            - explicit_log_dir (str, Path): Can be used to override exp_dir/name/version folder creation. Defaults to</span>
<span class="sd">                None, which will use exp_dir, name, and version to construct the logging directory.</span>
<span class="sd">            - exp_dir (str, Path): The base directory to create the logging directory. Defaults to None, which logs to</span>
<span class="sd">                ./mridc_experiments.</span>
<span class="sd">            - name (str): The name of the experiment. Defaults to None which turns into &quot;default&quot; via name = name or</span>
<span class="sd">                &quot;default&quot;.</span>
<span class="sd">            - version (str): The version of the experiment. Defaults to None which uses either a datetime string or</span>
<span class="sd">                lightning&#39;s TensorboardLogger system of using version_{int}.</span>
<span class="sd">            - use_datetime_version (bool): Whether to use a datetime string for version. Defaults to True.</span>
<span class="sd">            - resume_if_exists (bool): Whether this experiment is resuming from a previous run. If True, it sets</span>
<span class="sd">                trainer.checkpoint_connector.resume_from_checkpoint_fit_path so that the trainer should auto-resume.</span>
<span class="sd">                exp_manager will move files under log_dir to log_dir/run_{int}. Defaults to False. From v1.0.0, when</span>
<span class="sd">                resume_if_exists is True, we would not create version folders to make it easier to find the log folder</span>
<span class="sd">                 for next runs.</span>
<span class="sd">            - resume_past_end (bool): exp_manager errors out if resume_if_exists is True and a checkpoint matching</span>
<span class="sd">                *end.ckpt indicating a previous training run fully completed. This behaviour can be disabled, in which</span>
<span class="sd">                case the *end.ckpt will be loaded by setting resume_past_end to True. Defaults to False.</span>
<span class="sd">            - resume_ignore_no_checkpoint (bool): exp_manager errors out if resume_if_exists is True and no checkpoint</span>
<span class="sd">                could be found. This behaviour can be disabled, in which case exp_manager will print a message and</span>
<span class="sd">                continue without restoring, by setting resume_ignore_no_checkpoint to True. Defaults to False.</span>
<span class="sd">            - create_tensorboard_logger (bool): Whether to create a tensorboard logger and attach it to the pytorch</span>
<span class="sd">                lightning trainer. Defaults to True.</span>
<span class="sd">            - summary_writer_kwargs (dict): A dictionary of kwargs that can be passed to lightning&#39;s TensorboardLogger</span>
<span class="sd">                class. Note that log_dir is passed by exp_manager and cannot exist in this dict. Defaults to None.</span>
<span class="sd">            - create_wandb_logger (bool): Whether to create a Weights and Biases logger and attach it to the pytorch</span>
<span class="sd">                lightning trainer. Defaults to False.</span>
<span class="sd">            - wandb_logger_kwargs (dict): A dictionary of kwargs that can be passed to lightning&#39;s WandBLogger</span>
<span class="sd">                class. Note that name and project are required parameters if create_wandb_logger is True.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            - create_checkpoint_callback (bool): Whether to create a ModelCheckpoint callback and attach it to the</span>
<span class="sd">                pytorch lightning trainer. The ModelCheckpoint saves the top 3 models with the best &quot;val_loss&quot;, the</span>
<span class="sd">                most recent checkpoint under *last.ckpt, and the final checkpoint after training completes under</span>
<span class="sd">                *end.ckpt. Defaults to True.</span>
<span class="sd">            - files_to_copy (list): A list of files to copy to the experiment logging directory. Defaults to None which</span>
<span class="sd">                copies no files.</span>
<span class="sd">    returns:</span>
<span class="sd">        log_dir (Path): The final logging directory where logging files are saved. Usually the concatenation of</span>
<span class="sd">            exp_dir, name, and version.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add rank information to logger</span>
    <span class="c1"># Note: trainer.global_rank and trainer.is_global_zero are not set until trainer.fit, so have to hack around it</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">node_rank</span> <span class="o">*</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">+</span> <span class="n">local_rank</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">global_rank</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">world_size</span>

    <span class="k">if</span> <span class="n">cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;exp_manager did not receive a cfg argument. It will be disabled.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fast_dev_run</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Trainer was called with fast_dev_run. exp_manager will return without any functionality.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># Ensure passed cfg is compliant with ExpManagerConfig</span>
    <span class="n">schema</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">structured</span><span class="p">(</span><span class="n">ExpManagerConfig</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cfg was type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span><span class="si">}</span><span class="s2">. Expected either a dict or a DictConfig&quot;</span><span class="p">)</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>

    <span class="n">error_checks</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>  <span class="c1"># Ensures that trainer options are compliant with MRIDC and exp_manager arguments</span>

    <span class="n">log_dir</span><span class="p">,</span> <span class="n">exp_dir</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">version</span> <span class="o">=</span> <span class="n">get_log_dir</span><span class="p">(</span>
        <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span>
        <span class="n">exp_dir</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_dir</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">version</span><span class="p">,</span>
        <span class="n">explicit_log_dir</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">explicit_log_dir</span><span class="p">,</span>
        <span class="n">use_datetime_version</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">use_datetime_version</span><span class="p">,</span>
        <span class="n">resume_if_exists</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">:</span>
        <span class="n">check_resume</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">log_dir</span><span class="p">),</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_past_end</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_ignore_no_checkpoint</span><span class="p">)</span>

    <span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="c1"># If name returned from get_log_dir is &quot;&quot;, use cfg.name for checkpointing</span>
    <span class="k">if</span> <span class="n">checkpoint_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">checkpoint_name</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;default&quot;</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>  <span class="c1"># Used for configure_loggers so that the log_dir is properly set even if name is &quot;&quot;</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">version</span>

    <span class="c1"># update app_state with log_dir, exp_dir, etc</span>
    <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">log_dir</span> <span class="o">=</span> <span class="n">log_dir</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">exp_dir</span> <span class="o">=</span> <span class="n">exp_dir</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">version</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_name</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">create_checkpoint_callback</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_checkpoint_callback</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">checkpoint_callback_params</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_callback_params</span>

    <span class="c1"># Create the logging directory if it does not exist</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Cannot limit creation to global zero as all ranks write to own log file</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Experiments will be logged at </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">_default_root_dir</span> <span class="o">=</span> <span class="n">log_dir</span>

    <span class="c1"># Handle logging to file</span>
    <span class="k">if</span> <span class="n">get_envbool</span><span class="p">(</span><span class="n">MRIDC_ENV_VARNAME_TESTING</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">world_size</span> <span class="o">&lt;=</span> <span class="mi">32</span><span class="p">:</span>
        <span class="c1"># If MRIDC_TESTING is set (debug mode) or if less than 32 ranks save all log files</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">world_size</span> <span class="o">&lt;=</span> <span class="mi">256</span> <span class="ow">and</span> <span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># If less than 256 ranks, try to save 1 log file per &quot;machine&quot;</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># If running more than 256 ranks, only save 1 log file</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>

    <span class="c1"># For some reason, LearningRateLogger requires trainer to have a logger. Safer to create logger on all ranks</span>
    <span class="c1"># not just global rank 0.</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span> <span class="ow">or</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">:</span>
        <span class="n">configure_loggers</span><span class="p">(</span>
            <span class="n">trainer</span><span class="p">,</span>
            <span class="p">[</span><span class="n">Path</span><span class="p">(</span><span class="n">exp_dir</span><span class="p">)],</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">version</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">summary_writer_kwargs</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">wandb_logger_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># add loggers timing callbacks</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">log_step_timing</span><span class="p">:</span>
        <span class="n">timing_callback</span> <span class="o">=</span> <span class="n">TimingCallback</span><span class="p">(</span><span class="n">timer_kwargs</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">step_timing_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timing_callback</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_checkpoint_callback</span><span class="p">:</span>
        <span class="n">configure_checkpointing</span><span class="p">(</span>
            <span class="n">trainer</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">checkpoint_name</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_callback_params</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
        <span class="c1"># Move files_to_copy to folder and add git information if present</span>
        <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">files_to_copy</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_file</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">files_to_copy</span><span class="p">:</span>
                <span class="n">copy</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">_file</span><span class="p">),</span> <span class="n">log_dir</span><span class="p">)</span>

        <span class="c1"># Create files for cmd args and git info</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;cmd-args.log&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">_file</span><span class="p">:</span>
            <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">))</span>

        <span class="c1"># Try to get git hash</span>
        <span class="n">git_repo</span><span class="p">,</span> <span class="n">git_hash</span> <span class="o">=</span> <span class="n">get_git_hash</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">git_repo</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;git-info.log&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">_file</span><span class="p">:</span>
                <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;commit hash: </span><span class="si">{</span><span class="n">git_hash</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">get_git_diff</span><span class="p">())</span>

        <span class="c1"># Add err_file logging to global_rank zero</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_err_file_handler</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;mridc_error_log.txt&quot;</span><span class="p">)</span>

        <span class="c1"># Add lightning file logging to global_rank zero</span>
        <span class="n">add_filehandlers_to_pl_logger</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;lightning_logs.txt&quot;</span><span class="p">,</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;mridc_error_log.txt&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">log_dir</span>


<span class="k">def</span> <span class="nf">error_checks</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks that the passed trainer is compliant with MRIDC and exp_manager&#39;s passed configuration. Checks that:</span>
<span class="sd">        - Throws error when hydra has changed the working directory. This causes issues with lightning&#39;s DDP</span>
<span class="sd">        - Throws error when trainer has loggers defined but create_tensorboard_logger or create_WandB_logger is True</span>
<span class="sd">        - Prints error messages when 1) run on multi-node and not Slurm, and 2) run on multi-gpu without DDP</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">HydraConfig</span><span class="o">.</span><span class="n">initialized</span><span class="p">()</span> <span class="ow">and</span> <span class="n">get_original_cwd</span><span class="p">()</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Hydra changed the working directory. This interferes with ExpManger&#39;s functionality. Please pass &quot;</span>
            <span class="s2">&quot;hydra.run.dir=. to your python script.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span> <span class="ow">or</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, and either &quot;</span>
            <span class="s2">&quot;create_tensorboard_logger or create_wandb_logger was set to True. These can only be used if trainer does &quot;</span>
            <span class="s2">&quot;not already have a logger.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_nodes</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_slurm</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="s2">&quot;You are running multi-node training without SLURM handling the processes.&quot;</span>
            <span class="s2">&quot; Please note that this is not tested in MRIDC and could result in errors.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="p">,</span> <span class="n">DDPPlugin</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="s2">&quot;You are running multi-gpu without ddp.Please note that this is not tested in MRIDC and could result in &quot;</span>
            <span class="s2">&quot;errors.&quot;</span>
        <span class="p">)</span>


<span class="k">def</span> <span class="nf">check_resume</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">log_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">resume_past_end</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks that resume=True was used correctly with the arguments pass to exp_manager. Sets</span>
<span class="sd">    trainer.checkpoint_connector.resume_from_checkpoint_fit_path as necessary.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">    Raises:</span>
<span class="sd">        NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.</span>
<span class="sd">        ValueError: If resume is True, and there were more than 1 checkpoint could found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">log_dir</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming requires the log_dir </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2"> to be passed to exp_manager&quot;</span><span class="p">)</span>

    <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">end_checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*end.ckpt&quot;</span><span class="p">))</span>
    <span class="n">last_checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*last.ckpt&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;There was no checkpoint folder at checkpoint_dir :</span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Training from scratch.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="n">NotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There was no checkpoint folder at checkpoint_dir :</span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Cannot resume.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end_checkpoints</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_past_end</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> indicating that the last training run has already completed.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;mp_rank&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiple checkpoints </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="si">}</span><span class="s2"> that matches *end.ckpt.&quot;</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming from </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">last_checkpoints</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were no checkpoints found in </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Training from scratch.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="n">NotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were no checkpoints found in </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Cannot resume.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">last_checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;mp_rank&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiple checkpoints </span><span class="si">{</span><span class="n">last_checkpoints</span><span class="si">}</span><span class="s2"> that matches *last.ckpt.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming from </span><span class="si">{</span><span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_from_checkpoint_fit_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">files_to_move</span> <span class="o">:=</span> <span class="p">[</span><span class="n">child</span> <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span><span class="o">.</span><span class="n">iterdir</span><span class="p">()</span> <span class="k">if</span> <span class="n">child</span><span class="o">.</span><span class="n">is_file</span><span class="p">()]:</span>
            <span class="c1"># Move old files to a new folder</span>
            <span class="n">other_run_dirs</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;run_*&quot;</span><span class="p">)</span>
            <span class="n">run_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">bool</span><span class="p">(</span><span class="n">fold</span><span class="o">.</span><span class="n">is_dir</span><span class="p">())</span> <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="n">other_run_dirs</span><span class="p">)</span>
            <span class="n">new_run_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;run_</span><span class="si">{</span><span class="n">run_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">new_run_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_file</span> <span class="ow">in</span> <span class="n">files_to_move</span><span class="p">:</span>
                <span class="n">move</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">_file</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">new_run_dir</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">check_explicit_log_dir</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="n">exp_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">version</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Checks that the passed arguments are compatible with explicit_log_dir.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">    Raise:</span>
<span class="sd">        LoggerMisconfigurationError</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger and explicit_log_dir: &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2"> was pass to exp_manager. Please remove the logger from the lightning trainer.&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Checking only (explicit_log_dir) vs (exp_dir and version).</span>
    <span class="c1"># The `name` will be used as the actual name of checkpoint/archive.</span>
    <span class="k">if</span> <span class="n">exp_dir</span> <span class="ow">or</span> <span class="n">version</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;exp_manager received explicit_log_dir: </span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2"> and at least one of exp_dir: </span><span class="si">{</span><span class="n">exp_dir</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;or version: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2">. Please note that exp_dir, name, and version will be ignored.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">()</span> <span class="ow">and</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">))</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exp_manager is logging to </span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2">, but it already exists.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span>


<span class="k">def</span> <span class="nf">get_log_dir</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_datetime_version</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_if_exists</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtains the log_dir used for exp_manager.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">        explicit_log_dir (str): The explicit path to the log folder. Defaults to False.</span>
<span class="sd">        use_datetime_version (bool): Uses date and time as the version of the log folder. Defaults to True.</span>
<span class="sd">        resume_if_exists (bool): if resume_if_exists of the exp_manager&#39;s config is enabled or not. When enabled, the</span>
<span class="sd">            version folders would not get created.</span>
<span class="sd">    Raise:</span>
<span class="sd">        LoggerMisconfigurationError: If trainer is incompatible with arguments</span>
<span class="sd">        NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.</span>
<span class="sd">        ValueError: If resume is True, and there were more than 1 checkpoint could found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">explicit_log_dir</span><span class="p">:</span>  <span class="c1"># If explicit log_dir was passed, short circuit</span>
        <span class="k">return</span> <span class="n">check_explicit_log_dir</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="p">[</span><span class="n">Path</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">)],</span> <span class="nb">str</span><span class="p">(</span><span class="n">exp_dir</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">))</span>

    <span class="c1"># Default exp_dir to ./mridc_experiments if None was passed</span>
    <span class="n">_exp_dir</span> <span class="o">=</span> <span class="n">exp_dir</span>
    <span class="k">if</span> <span class="n">exp_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_exp_dir</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span> <span class="o">/</span> <span class="s2">&quot;mridc_experiments&quot;</span><span class="p">)</span>

    <span class="c1"># If the user has already defined a logger for the trainer, use the logger defaults for logging directory</span>
    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">exp_dir</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
                    <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, the logger&#39;s &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;save_dir was not None, and exp_dir (</span><span class="si">{</span><span class="n">exp_dir</span><span class="si">}</span><span class="s2">) was not None. If trainer.logger.save_dir &quot;</span>
                    <span class="s2">&quot;exists, exp_manager will use trainer.logger.save_dir as the logging directory and exp_dir &quot;</span>
                    <span class="s2">&quot;must be None.&quot;</span>
                <span class="p">)</span>
            <span class="n">_exp_dir</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span>
        <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
                <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, and name: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> was also passed to exp_manager. If the trainer contains a &quot;</span>
                <span class="s2">&quot;logger, exp_manager will use trainer.logger.name, and name passed to exp_manager must be None.&quot;</span>
            <span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">name</span>
        <span class="n">version</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;version_</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Use user-defined exp_dir, project_name, exp_name, and versioning options</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;default&quot;</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">version</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MRIDC_ENV_VARNAME_VERSION</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">version</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">resume_if_exists</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;No version folders would be created under the log folder as &#39;resume_if_exists&#39; is enabled.&quot;</span>
                <span class="p">)</span>
                <span class="n">version</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">use_datetime_version</span><span class="p">:</span>
                    <span class="n">version</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="n">_exp_dir</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
                    <span class="n">version</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;version_</span><span class="si">{</span><span class="n">tensorboard_logger</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">MRIDC_ENV_VARNAME_VERSION</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">version</span>

    <span class="n">log_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">_exp_dir</span><span class="p">))</span> <span class="o">/</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span> <span class="o">/</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">log_dir</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">_exp_dir</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">get_git_hash</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function that tries to get the commit hash if running inside a git folder</span>
<span class="sd">    returns:</span>
<span class="sd">        Bool: Whether the git subprocess ran without error</span>
<span class="sd">        str: git subprocess output or error message</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;git&quot;</span><span class="p">,</span> <span class="s2">&quot;rev-parse&quot;</span><span class="p">,</span> <span class="s2">&quot;HEAD&quot;</span><span class="p">],</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">get_git_diff</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function that tries to get the git diff if running inside a git folder</span>
<span class="sd">    returns:</span>
<span class="sd">        Bool: Whether the git subprocess ran without error</span>
<span class="sd">        str: git subprocess output or error message</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;git&quot;</span><span class="p">,</span> <span class="s2">&quot;diff&quot;</span><span class="p">],</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">LoggerList</span><span class="p">(</span><span class="n">_LoggerCollection</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A thin wrapper on Lightning&#39;s LoggerCollection such that name and version are better aligned with exp_manager&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_logger_iterable</span><span class="p">,</span> <span class="n">mridc_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mridc_version</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_logger_iterable</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_name</span> <span class="o">=</span> <span class="n">mridc_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_version</span> <span class="o">=</span> <span class="n">mridc_version</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The name of the experiment.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">version</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The version of the experiment. If the logger was created with a version, this will be the version.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_version</span>


<span class="k">def</span> <span class="nf">configure_loggers</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">create_tensorboard_logger</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">summary_writer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">create_wandb_logger</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">wandb_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates TensorboardLogger and/or WandBLogger and attach them to trainer. Raises ValueError if</span>
<span class="sd">    summary_writer_kwargs or wandb_kwargs are miss configured.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Potentially create tensorboard logger and/or WandBLogger</span>
    <span class="n">logger_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">create_tensorboard_logger</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">summary_writer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">summary_writer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">elif</span> <span class="s2">&quot;log_dir&quot;</span> <span class="ow">in</span> <span class="n">summary_writer_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You cannot pass `log_dir` as part of `summary_writer_kwargs`. `log_dir` is handled by lightning&#39;s &quot;</span>
                <span class="s2">&quot;TensorBoardLogger logger.&quot;</span>
            <span class="p">)</span>
        <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span>
            <span class="n">save_dir</span><span class="o">=</span><span class="n">exp_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="o">**</span><span class="n">summary_writer_kwargs</span>
        <span class="p">)</span>
        <span class="n">logger_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensorboard_logger</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;TensorboardLogger has been set up&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">create_wandb_logger</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">wandb_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wandb_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;name&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wandb_kwargs</span> <span class="ow">and</span> <span class="s2">&quot;project&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wandb_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;name and project are required for wandb_logger&quot;</span><span class="p">)</span>
        <span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="n">exp_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="o">**</span><span class="n">wandb_kwargs</span><span class="p">)</span>

        <span class="n">logger_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wandb_logger</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;WandBLogger has been set up&quot;</span><span class="p">)</span>

    <span class="n">logger_list</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">LoggerList</span><span class="p">(</span><span class="n">logger_list</span><span class="p">,</span> <span class="n">mridc_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">mridc_version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logger_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">logger_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">logger_connector</span><span class="o">.</span><span class="n">configure_logger</span><span class="p">(</span><span class="n">logger_list</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MRIDCModelCheckpoint</span><span class="p">(</span><span class="n">ModelCheckpoint</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Light wrapper around Lightning&#39;s ModelCheckpoint to force a saved checkpoint on train_end&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">always_save_mridc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">save_mridc_on_train_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_best_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">postfix</span><span class="o">=</span><span class="s2">&quot;.mridc&quot;</span><span class="p">,</span>
        <span class="n">n_resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">model_parallel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            always_save_mridc (): (Default value = False)</span>
<span class="sd">            save_mridc_on_train_end (): (Default value = True)</span>
<span class="sd">            save_best_model (): (Default value = False)</span>
<span class="sd">            postfix (): (Default value = &quot;.mridc&quot;)</span>
<span class="sd">            n_resume (): (Default value = False)</span>
<span class="sd">            model_parallel_size (): (Default value = None)</span>
<span class="sd">            **kwargs (): (Default value = {})</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Parse and store &quot;extended&quot; parameters: save_best model and postfix.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span> <span class="o">=</span> <span class="n">always_save_mridc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span> <span class="o">=</span> <span class="n">save_mridc_on_train_end</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="o">=</span> <span class="n">save_best_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Found save_best_model is True and save_mridc_on_train_end is False. &quot;</span>
                    <span class="s2">&quot;Set save_mridc_on_train_end to True to automatically save the best model.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span> <span class="o">=</span> <span class="n">postfix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">=</span> <span class="n">model_parallel_size</span>

        <span class="c1"># `prefix` is deprecated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;prefix&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;prefix&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="c1"># Call the parent class constructor with the remaining kwargs.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">n_resume</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Checking previous runs&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mridc_topk_check_previous_run</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">mridc_topk_check_previous_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if there are previous runs with the same topk value.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="n">checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">)</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*.ckpt&quot;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uninject_mp_rank</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">checkpoint</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">==</span> <span class="s2">&quot;-last.ckpt&quot;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Find monitor in str + 1 for &#39;=&#39;</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">match</span> <span class="o">:=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;[A-z]&quot;</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span><span class="p">:]):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># -1 due to separator hypen</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># No saved checkpoints yet</span>

        <span class="n">_reverse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;min&quot;</span>

        <span class="n">best_k_models</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">_reverse</span><span class="p">)</span>

        <span class="c1"># This section should be ok as rank zero will delete all excess checkpoints, since all other ranks are</span>
        <span class="c1"># instantiated after rank zero. models_to_delete should be 0 for all other ranks.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of models to delete: </span><span class="si">{</span><span class="n">models_to_delete</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">models_to_delete</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_del_model_without_trainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed checkpoint: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_uninject_mp_rank</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Injects the rank of the current process into the checkpoint filepath.</span>

<span class="sd">        Args:</span>
<span class="sd">            filepath (): Path to the checkpoint file.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: Path to the checkpoint file with the rank of the current process injected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">))</span>
        <span class="n">basename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
        <span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">basename</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">filepath</span>

    <span class="c1"># TODO: remove _save_last_checkpoint after fix https://github.com/PyTorchLightning/pytorch-lightning/issues/11451</span>
    <span class="k">def</span> <span class="nf">_save_last_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">monitor_candidates</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save the last checkpoint.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_last</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">filepath</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">format_checkpoint_name</span><span class="p">(</span><span class="n">monitor_candidates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CHECKPOINT_NAME_LAST</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">!=</span> <span class="n">filepath</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="o">.</span><span class="n">remove_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">filepath</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_weights_only</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the default on_save_checkpoint to save the best model if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): The trainer object.</span>
<span class="sd">            pl_module (): The LightningModule object.</span>
<span class="sd">            checkpoint (): The checkpoint object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_save_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="c1"># Load the best model and then re-save it</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;always_save_nemo is not implemented for model parallel models.&quot;</span><span class="p">)</span>

        <span class="c1"># since we are creating tarfile artifacts we need to update .nemo path</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span>
            <span class="n">old_state_dict</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pl_module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;state_dict&quot;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span>

            <span class="c1"># get a new instance of the model</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">old_state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is called at the end of training.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): the trainer object</span>
<span class="sd">            pl_module (): the pl_module object</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fast_dev_run</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Call parent on_train_end() to save the -last checkpoint</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

        <span class="c1"># Load the best model and then re-save it</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> was told to save the best checkpoint at the end of training, but no saved checkpoints &quot;</span>
                    <span class="s2">&quot;were found. Saving latest model instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_del_model_without_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete a model without a trainer.</span>

<span class="sd">        Args:</span>
<span class="sd">            filepath (): path to the model to delete</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># filepath needs to be updated to include mp_rank</span>
            <span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
            <span class="n">basename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
            <span class="n">filepath</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dirname</span><span class="si">}</span><span class="s2">/mp_rank_</span><span class="si">{</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_rank</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">basename</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># each model parallel rank needs to remove its model</span>
        <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">data_parallel_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fs</span><span class="o">.</span><span class="n">rm</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed checkpoint: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tried to remove checkpoint: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2"> but failed.&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">configure_checkpointing</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">resume</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="s2">&quot;DictConfig&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds ModelCheckpoint to trainer. Raises CheckpointMisconfigurationError if trainer already has a ModelCheckpoint</span>
<span class="sd">    callback or if trainer.weights_save_path was passed to Trainer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">CheckpointMisconfigurationError</span><span class="p">(</span>
                <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a ModelCheckpoint &quot;</span>
                <span class="s2">&quot;and create_checkpoint_callback was set to True. Please either set create_checkpoint_callback &quot;</span>
                <span class="s2">&quot;to False, or remove ModelCheckpoint from the lightning trainer&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">weights_save_path</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">():</span>
        <span class="k">raise</span> <span class="n">CheckpointMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning was passed weights_save_path. This variable is ignored by exp_manager&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Create the callback and attach it to trainer</span>
    <span class="k">if</span> <span class="s2">&quot;filepath&quot;</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filepath</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;filepath is deprecated. Please switch to dirpath and filename instead&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
            <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
        <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">del</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;filepath&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">--</span><span class="se">{{</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="si">}</span><span class="s2">:.4f</span><span class="se">}}</span><span class="s2">-</span><span class="se">{{</span><span class="s2">epoch</span><span class="se">}}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">MRIDCModelCheckpoint</span><span class="o">.</span><span class="n">CHECKPOINT_NAME_LAST</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">filename</span><span class="si">}</span><span class="s2">-last&quot;</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">dirpath</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filename</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">prefix</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">&lt;</span> <span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span>
        <span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="s2">&quot;The checkpoint callback was told to monitor a validation value but trainer.max_epochs(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span><span class="si">}</span><span class="s2">) was less than trainer.check_val_every_n_epoch(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span><span class="si">}</span><span class="s2">). It is very likely this run will fail with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;ModelCheckpoint(monitor=&#39;</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="si">}</span><span class="s2">&#39;) not found in the returned metrics. Please ensure that &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;validation is run within trainer.max_epochs.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The checkpoint callback was told to monitor a validation value and trainer&#39;s max_steps was set to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span><span class="si">}</span><span class="s2">. Please ensure that max_steps will run for at least &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span><span class="si">}</span><span class="s2"> epochs to ensure that checkpointing will not error out.&quot;</span>
            <span class="p">)</span>

    <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">MRIDCModelCheckpoint</span><span class="p">(</span><span class="n">n_resume</span><span class="o">=</span><span class="n">resume</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_from_checkpoint_fit_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">params</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">MRIDCModelCheckpoint</span><span class="o">.</span><span class="n">_uninject_mp_rank</span><span class="p">(</span>
            <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span>
        <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">checkpoint_callback</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_slurm</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the trainer is running on a slurm cluster. If so, it will check if the trainer is running on the master</span>
<span class="sd">    node. If it is not, it will exit.</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer (): The trainer to check.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the trainer is running on the master node, False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">trainer</span><span class="o">.</span><span class="n">accelerator_connector</span><span class="o">.</span><span class="n">is_slurm_managing_tasks</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>


<span class="k">class</span> <span class="nc">StatelessTimer</span><span class="p">(</span><span class="n">Timer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of PTL timers to be per run.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not save the state of the timer.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">callback_state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not load the state of the timer.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_check_time_remaining</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not check the time remaining.&quot;&quot;&quot;</span>
        <span class="c1"># Default timer only checks for train time exceeding max_time, this includes time for all stages.</span>
        <span class="n">train_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">TRAINING</span><span class="p">)</span>
        <span class="n">validation_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">VALIDATING</span><span class="p">)</span>
        <span class="n">test_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">TESTING</span><span class="p">)</span>
        <span class="n">total_duration</span> <span class="o">=</span> <span class="n">train_duration</span> <span class="o">+</span> <span class="n">validation_duration</span> <span class="o">+</span> <span class="n">test_duration</span>
        <span class="n">should_stop</span> <span class="o">=</span> <span class="n">total_duration</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_duration</span>
        <span class="n">should_stop</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="o">.</span><span class="n">reduce_boolean_decision</span><span class="p">(</span><span class="n">should_stop</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">should_stop</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">should_stop</span> <span class="ow">or</span> <span class="n">should_stop</span>
        <span class="k">if</span> <span class="n">should_stop</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span><span class="p">:</span>
            <span class="n">rank_zero_info</span><span class="p">(</span><span class="s2">&quot;Time limit reached. Signaling Trainer to stop.&quot;</span><span class="p">)</span>
            <span class="n">rank_zero_info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Spent </span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">train_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on training, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">validation_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on validation and &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">test_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on testing&quot;</span>
            <span class="p">)</span>
</pre></div>

        </details>

            </section>
                <section id="NotFoundError">
                                <div class="attr class">
        <a class="headerlink" href="#NotFoundError">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">NotFoundError</span><wbr>(<span class="base"><a href="exceptions.html#MRIDCBaseException">mridc.utils.exceptions.MRIDCBaseException</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">NotFoundError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a file or folder is not found&quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Raised when a file or folder is not found</p>
</div>


                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>builtins.Exception</dt>
                                <dd id="NotFoundError.__init__" class="function">Exception</dd>

            </div>
            <div><dt>builtins.BaseException</dt>
                                <dd id="NotFoundError.with_traceback" class="function">with_traceback</dd>
                <dd id="NotFoundError.args" class="variable">args</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="LoggerMisconfigurationError">
                                <div class="attr class">
        <a class="headerlink" href="#LoggerMisconfigurationError">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">LoggerMisconfigurationError</span><wbr>(<span class="base"><a href="exceptions.html#MRIDCBaseException">mridc.utils.exceptions.MRIDCBaseException</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LoggerMisconfigurationError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a mismatch between trainer.logger and exp_manager occurs&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">message</span> <span class="o">+</span> <span class="s2">&quot;You can disable lightning&#39;s trainer from creating a logger by passing logger=False to its &quot;</span>
            <span class="s2">&quot;constructor. &quot;</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Raised when a mismatch between trainer.logger and exp_manager occurs</p>
</div>


                            <div id="LoggerMisconfigurationError.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#LoggerMisconfigurationError.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">LoggerMisconfigurationError</span><span class="signature">(message)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
        <span class="n">message</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">message</span> <span class="o">+</span> <span class="s2">&quot;You can disable lightning&#39;s trainer from creating a logger by passing logger=False to its &quot;</span>
            <span class="s2">&quot;constructor. &quot;</span>
        <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
</pre></div>

        </details>

    

                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>builtins.BaseException</dt>
                                <dd id="LoggerMisconfigurationError.with_traceback" class="function">with_traceback</dd>
                <dd id="LoggerMisconfigurationError.args" class="variable">args</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="CheckpointMisconfigurationError">
                                <div class="attr class">
        <a class="headerlink" href="#CheckpointMisconfigurationError">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">CheckpointMisconfigurationError</span><wbr>(<span class="base"><a href="exceptions.html#MRIDCBaseException">mridc.utils.exceptions.MRIDCBaseException</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">CheckpointMisconfigurationError</span><span class="p">(</span><span class="n">MRIDCBaseException</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Raised when a mismatch between trainer.callbacks and exp_manager occurs&quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Raised when a mismatch between trainer.callbacks and exp_manager occurs</p>
</div>


                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>builtins.Exception</dt>
                                <dd id="CheckpointMisconfigurationError.__init__" class="function">Exception</dd>

            </div>
            <div><dt>builtins.BaseException</dt>
                                <dd id="CheckpointMisconfigurationError.with_traceback" class="function">with_traceback</dd>
                <dd id="CheckpointMisconfigurationError.args" class="variable">args</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="CallbackParams">
                                <div class="attr class">
        <a class="headerlink" href="#CallbackParams">#&nbsp;&nbsp</a>

                <div class="decorator">@dataclass</div>

        <span class="def">class</span>
        <span class="name">CallbackParams</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">CallbackParams</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Parameters for a callback&quot;&quot;&quot;</span>

    <span class="n">filepath</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># Deprecated</span>
    <span class="n">dirpath</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">filename</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">monitor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span>
    <span class="n">verbose</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">save_last</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">save_top_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">save_weights_only</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;min&quot;</span>
    <span class="n">every_n_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># If None, exp_manager will attempt to handle the filepath</span>
    <span class="n">postfix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;.mridc&quot;</span>
    <span class="n">save_best_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">always_save_mridc</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">save_mridc_on_train_end</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># Automatically save .mridc file during on_train_end hook</span>
    <span class="n">model_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>

        </details>

            <div class="docstring"><p>Parameters for a callback</p>
</div>


                            <div id="CallbackParams.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#CallbackParams.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">CallbackParams</span><span class="signature">(
    filepath: Optional[str] = None,
    dirpath: Optional[str] = None,
    filename: Optional[str] = None,
    monitor: Optional[str] = &#39;val_loss&#39;,
    verbose: Optional[bool] = True,
    save_last: Optional[bool] = True,
    save_top_k: Optional[int] = 3,
    save_weights_only: Optional[bool] = False,
    mode: Optional[str] = &#39;min&#39;,
    every_n_epochs: Optional[int] = 1,
    prefix: Optional[str] = None,
    postfix: str = &#39;.mridc&#39;,
    save_best_model: bool = False,
    always_save_mridc: bool = False,
    save_mridc_on_train_end: Optional[bool] = True,
    model_parallel_size: Optional[int] = None
)</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.filepath" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.filepath">#&nbsp;&nbsp</a>

        <span class="name">filepath</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.dirpath" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.dirpath">#&nbsp;&nbsp</a>

        <span class="name">dirpath</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.filename" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.filename">#&nbsp;&nbsp</a>

        <span class="name">filename</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.monitor" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.monitor">#&nbsp;&nbsp</a>

        <span class="name">monitor</span><span class="annotation">: Optional[str]</span><span class="default_value"> = &#39;val_loss&#39;</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.verbose" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.verbose">#&nbsp;&nbsp</a>

        <span class="name">verbose</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.save_last" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.save_last">#&nbsp;&nbsp</a>

        <span class="name">save_last</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.save_top_k" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.save_top_k">#&nbsp;&nbsp</a>

        <span class="name">save_top_k</span><span class="annotation">: Optional[int]</span><span class="default_value"> = 3</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.save_weights_only" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.save_weights_only">#&nbsp;&nbsp</a>

        <span class="name">save_weights_only</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.mode" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.mode">#&nbsp;&nbsp</a>

        <span class="name">mode</span><span class="annotation">: Optional[str]</span><span class="default_value"> = &#39;min&#39;</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.every_n_epochs" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.every_n_epochs">#&nbsp;&nbsp</a>

        <span class="name">every_n_epochs</span><span class="annotation">: Optional[int]</span><span class="default_value"> = 1</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.prefix" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.prefix">#&nbsp;&nbsp</a>

        <span class="name">prefix</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.postfix" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.postfix">#&nbsp;&nbsp</a>

        <span class="name">postfix</span><span class="annotation">: str</span><span class="default_value"> = &#39;.mridc&#39;</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.save_best_model" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.save_best_model">#&nbsp;&nbsp</a>

        <span class="name">save_best_model</span><span class="annotation">: bool</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.always_save_mridc" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.always_save_mridc">#&nbsp;&nbsp</a>

        <span class="name">always_save_mridc</span><span class="annotation">: bool</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.save_mridc_on_train_end" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.save_mridc_on_train_end">#&nbsp;&nbsp</a>

        <span class="name">save_mridc_on_train_end</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="CallbackParams.model_parallel_size" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#CallbackParams.model_parallel_size">#&nbsp;&nbsp</a>

        <span class="name">model_parallel_size</span><span class="annotation">: Optional[int]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                </section>
                <section id="StepTimingParams">
                                <div class="attr class">
        <a class="headerlink" href="#StepTimingParams">#&nbsp;&nbsp</a>

                <div class="decorator">@dataclass</div>

        <span class="def">class</span>
        <span class="name">StepTimingParams</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">StepTimingParams</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Parameters for the step timing callback.&quot;&quot;&quot;</span>

    <span class="n">reduction</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>
    <span class="c1"># if True torch.cuda.synchronize() is called on start/stop</span>
    <span class="n">sync_cuda</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># if positive, defines the size of a sliding window for computing mean</span>
    <span class="n">buffer_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>

        </details>

            <div class="docstring"><p>Parameters for the step timing callback.</p>
</div>


                            <div id="StepTimingParams.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#StepTimingParams.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">StepTimingParams</span><span class="signature">(
    reduction: Optional[str] = &#39;mean&#39;,
    sync_cuda: Optional[bool] = False,
    buffer_size: Optional[int] = 1
)</span>
    </div>

    
    

                            </div>
                            <div id="StepTimingParams.reduction" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#StepTimingParams.reduction">#&nbsp;&nbsp</a>

        <span class="name">reduction</span><span class="annotation">: Optional[str]</span><span class="default_value"> = &#39;mean&#39;</span>
    </div>

    
    

                            </div>
                            <div id="StepTimingParams.sync_cuda" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#StepTimingParams.sync_cuda">#&nbsp;&nbsp</a>

        <span class="name">sync_cuda</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="StepTimingParams.buffer_size" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#StepTimingParams.buffer_size">#&nbsp;&nbsp</a>

        <span class="name">buffer_size</span><span class="annotation">: Optional[int]</span><span class="default_value"> = 1</span>
    </div>

    
    

                            </div>
                </section>
                <section id="ExpManagerConfig">
                                <div class="attr class">
        <a class="headerlink" href="#ExpManagerConfig">#&nbsp;&nbsp</a>

                <div class="decorator">@dataclass</div>

        <span class="def">class</span>
        <span class="name">ExpManagerConfig</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">ExpManagerConfig</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Configuration for the experiment manager.&quot;&quot;&quot;</span>

    <span class="c1"># Log dir creation parameters</span>
    <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">use_datetime_version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">resume_if_exists</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">resume_past_end</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="c1"># Logging parameters</span>
    <span class="n">create_tensorboard_logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">summary_writer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">create_wandb_logger</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">wandb_logger_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># Checkpointing parameters</span>
    <span class="n">create_checkpoint_callback</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">checkpoint_callback_params</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CallbackParams</span><span class="p">]</span> <span class="o">=</span> <span class="n">CallbackParams</span><span class="p">()</span>
    <span class="c1"># Additional exp_manager arguments</span>
    <span class="n">files_to_copy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="c1"># logs timing of train/val/test steps</span>
    <span class="n">log_step_timing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">step_timing_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">StepTimingParams</span><span class="p">]</span> <span class="o">=</span> <span class="n">StepTimingParams</span><span class="p">()</span>
    <span class="n">model_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>

        </details>

            <div class="docstring"><p>Configuration for the experiment manager.</p>
</div>


                            <div id="ExpManagerConfig.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ExpManagerConfig.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">ExpManagerConfig</span><span class="signature">(
    explicit_log_dir: Optional[str] = None,
    exp_dir: Optional[str] = None,
    name: Optional[str] = None,
    version: Optional[str] = None,
    use_datetime_version: Optional[bool] = True,
    resume_if_exists: Optional[bool] = False,
    resume_past_end: Optional[bool] = False,
    resume_ignore_no_checkpoint: Optional[bool] = False,
    create_tensorboard_logger: Optional[bool] = True,
    summary_writer_kwargs: Optional[Dict[Any, Any]] = None,
    create_wandb_logger: Optional[bool] = False,
    wandb_logger_kwargs: Optional[Dict[Any, Any]] = None,
    create_checkpoint_callback: Optional[bool] = True,
    checkpoint_callback_params: Optional[<a href="#CallbackParams">mridc.utils.exp_manager.CallbackParams</a>] = CallbackParams(filepath=None, dirpath=None, filename=None, monitor=&#39;val_loss&#39;, verbose=True, save_last=True, save_top_k=3, save_weights_only=False, mode=&#39;min&#39;, every_n_epochs=1, prefix=None, postfix=&#39;.mridc&#39;, save_best_model=False, always_save_mridc=False, save_mridc_on_train_end=True, model_parallel_size=None),
    files_to_copy: Optional[List[str]] = None,
    log_step_timing: Optional[bool] = True,
    step_timing_kwargs: Optional[<a href="#StepTimingParams">mridc.utils.exp_manager.StepTimingParams</a>] = StepTimingParams(reduction=&#39;mean&#39;, sync_cuda=False, buffer_size=1),
    model_parallel_size: Optional[int] = None
)</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.explicit_log_dir" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.explicit_log_dir">#&nbsp;&nbsp</a>

        <span class="name">explicit_log_dir</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.exp_dir" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.exp_dir">#&nbsp;&nbsp</a>

        <span class="name">exp_dir</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.name" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.name">#&nbsp;&nbsp</a>

        <span class="name">name</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.version" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.version">#&nbsp;&nbsp</a>

        <span class="name">version</span><span class="annotation">: Optional[str]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.use_datetime_version" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.use_datetime_version">#&nbsp;&nbsp</a>

        <span class="name">use_datetime_version</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.resume_if_exists" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.resume_if_exists">#&nbsp;&nbsp</a>

        <span class="name">resume_if_exists</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.resume_past_end" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.resume_past_end">#&nbsp;&nbsp</a>

        <span class="name">resume_past_end</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.resume_ignore_no_checkpoint" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.resume_ignore_no_checkpoint">#&nbsp;&nbsp</a>

        <span class="name">resume_ignore_no_checkpoint</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.create_tensorboard_logger" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.create_tensorboard_logger">#&nbsp;&nbsp</a>

        <span class="name">create_tensorboard_logger</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.summary_writer_kwargs" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.summary_writer_kwargs">#&nbsp;&nbsp</a>

        <span class="name">summary_writer_kwargs</span><span class="annotation">: Optional[Dict[Any, Any]]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.create_wandb_logger" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.create_wandb_logger">#&nbsp;&nbsp</a>

        <span class="name">create_wandb_logger</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = False</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.wandb_logger_kwargs" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.wandb_logger_kwargs">#&nbsp;&nbsp</a>

        <span class="name">wandb_logger_kwargs</span><span class="annotation">: Optional[Dict[Any, Any]]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.create_checkpoint_callback" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.create_checkpoint_callback">#&nbsp;&nbsp</a>

        <span class="name">create_checkpoint_callback</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.checkpoint_callback_params" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.checkpoint_callback_params">#&nbsp;&nbsp</a>

        <span class="name">checkpoint_callback_params</span><span class="annotation">: Optional[<a href="#CallbackParams">mridc.utils.exp_manager.CallbackParams</a>]</span><span class="default_value"> = CallbackParams(filepath=None, dirpath=None, filename=None, monitor=&#39;val_loss&#39;, verbose=True, save_last=True, save_top_k=3, save_weights_only=False, mode=&#39;min&#39;, every_n_epochs=1, prefix=None, postfix=&#39;.mridc&#39;, save_best_model=False, always_save_mridc=False, save_mridc_on_train_end=True, model_parallel_size=None)</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.files_to_copy" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.files_to_copy">#&nbsp;&nbsp</a>

        <span class="name">files_to_copy</span><span class="annotation">: Optional[List[str]]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.log_step_timing" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.log_step_timing">#&nbsp;&nbsp</a>

        <span class="name">log_step_timing</span><span class="annotation">: Optional[bool]</span><span class="default_value"> = True</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.step_timing_kwargs" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.step_timing_kwargs">#&nbsp;&nbsp</a>

        <span class="name">step_timing_kwargs</span><span class="annotation">: Optional[<a href="#StepTimingParams">mridc.utils.exp_manager.StepTimingParams</a>]</span><span class="default_value"> = StepTimingParams(reduction=&#39;mean&#39;, sync_cuda=False, buffer_size=1)</span>
    </div>

    
    

                            </div>
                            <div id="ExpManagerConfig.model_parallel_size" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ExpManagerConfig.model_parallel_size">#&nbsp;&nbsp</a>

        <span class="name">model_parallel_size</span><span class="annotation">: Optional[int]</span><span class="default_value"> = None</span>
    </div>

    
    

                            </div>
                </section>
                <section id="TimingCallback">
                                <div class="attr class">
        <a class="headerlink" href="#TimingCallback">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">TimingCallback</span><wbr>(<span class="base">pytorch_lightning.callbacks.base.Callback</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">TimingCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Logs execution time of train/val/test steps&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize TimingCallback&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">timer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">=</span> <span class="n">timers</span><span class="o">.</span><span class="n">NamedTimer</span><span class="p">(</span><span class="o">**</span><span class="n">timer_kwargs</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_on_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the beginning of each batch&quot;&quot;&quot;</span>
        <span class="c1"># reset only if we do not return mean of a sliding window</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">buffer_size</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the end of each batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="o">.</span><span class="n">stop</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
        <span class="n">pl_module</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">timer</span><span class="p">[</span><span class="n">name</span><span class="p">],</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the beginning of each training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_validation_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_validation_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_test_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_test_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_before_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken for backward pass&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Note: this is called after the optimizer step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs execution time of train/val/test steps</p>
</div>


                            <div id="TimingCallback.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">TimingCallback</span><span class="signature">(timer_kwargs=None)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">timer_kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize TimingCallback&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">timer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">timer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timer</span> <span class="o">=</span> <span class="n">timers</span><span class="o">.</span><span class="n">NamedTimer</span><span class="p">(</span><span class="o">**</span><span class="n">timer_kwargs</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Initialize TimingCallback</p>
</div>


                            </div>
                            <div id="TimingCallback.on_train_batch_start" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_train_batch_start">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_train_batch_start</span><span class="signature">(self, trainer, pl_module, batch, batch_idx, **kwargs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_train_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Called at the beginning of each training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Called at the beginning of each training batch</p>
</div>


                            </div>
                            <div id="TimingCallback.on_train_batch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_train_batch_end">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_train_batch_end</span><span class="signature">(self, trainer, pl_module, outputs, batch, batch_idx, **kwargs)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_train_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the training batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs the time taken by the training batch</p>
</div>


                            </div>
                            <div id="TimingCallback.on_validation_batch_start" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_validation_batch_start">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_validation_batch_start</span><span class="signature">(self, trainer, pl_module, batch, batch_idx, dataloader_idx)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_validation_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation batch&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs the time taken by the validation batch</p>
</div>


                            </div>
                            <div id="TimingCallback.on_validation_batch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_validation_batch_end">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_validation_batch_end</span><span class="signature">(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_validation_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken by the validation step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;validation_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs the time taken by the validation step</p>
</div>


                            </div>
                            <div id="TimingCallback.on_test_batch_start" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_test_batch_start">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_test_batch_start</span><span class="signature">(self, trainer, pl_module, batch, batch_idx, dataloader_idx)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_test_batch_start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs execution time of test steps</p>
</div>


                            </div>
                            <div id="TimingCallback.on_test_batch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_test_batch_end">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_test_batch_end</span><span class="signature">(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_test_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs execution time of test steps&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;test_step_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs execution time of test steps</p>
</div>


                            </div>
                            <div id="TimingCallback.on_before_backward" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_before_backward">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_before_backward</span><span class="signature">(self, trainer, pl_module, loss)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_before_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Logs the time taken for backward pass&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_start</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Logs the time taken for backward pass</p>
</div>


                            </div>
                            <div id="TimingCallback.on_after_backward" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#TimingCallback.on_after_backward">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_after_backward</span><span class="signature">(self, trainer, pl_module)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_after_backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Note: this is called after the optimizer step&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_on_batch_end</span><span class="p">(</span><span class="s2">&quot;train_backward_timing&quot;</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Note: this is called after the optimizer step</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>pytorch_lightning.callbacks.base.Callback</dt>
                                <dd id="TimingCallback.state_key" class="variable">state_key</dd>
                <dd id="TimingCallback.on_configure_sharded_model" class="function">on_configure_sharded_model</dd>
                <dd id="TimingCallback.on_before_accelerator_backend_setup" class="function">on_before_accelerator_backend_setup</dd>
                <dd id="TimingCallback.setup" class="function">setup</dd>
                <dd id="TimingCallback.teardown" class="function">teardown</dd>
                <dd id="TimingCallback.on_init_start" class="function">on_init_start</dd>
                <dd id="TimingCallback.on_init_end" class="function">on_init_end</dd>
                <dd id="TimingCallback.on_fit_start" class="function">on_fit_start</dd>
                <dd id="TimingCallback.on_fit_end" class="function">on_fit_end</dd>
                <dd id="TimingCallback.on_sanity_check_start" class="function">on_sanity_check_start</dd>
                <dd id="TimingCallback.on_sanity_check_end" class="function">on_sanity_check_end</dd>
                <dd id="TimingCallback.on_train_epoch_start" class="function">on_train_epoch_start</dd>
                <dd id="TimingCallback.on_train_epoch_end" class="function">on_train_epoch_end</dd>
                <dd id="TimingCallback.on_validation_epoch_start" class="function">on_validation_epoch_start</dd>
                <dd id="TimingCallback.on_validation_epoch_end" class="function">on_validation_epoch_end</dd>
                <dd id="TimingCallback.on_test_epoch_start" class="function">on_test_epoch_start</dd>
                <dd id="TimingCallback.on_test_epoch_end" class="function">on_test_epoch_end</dd>
                <dd id="TimingCallback.on_predict_epoch_start" class="function">on_predict_epoch_start</dd>
                <dd id="TimingCallback.on_predict_epoch_end" class="function">on_predict_epoch_end</dd>
                <dd id="TimingCallback.on_epoch_start" class="function">on_epoch_start</dd>
                <dd id="TimingCallback.on_epoch_end" class="function">on_epoch_end</dd>
                <dd id="TimingCallback.on_batch_start" class="function">on_batch_start</dd>
                <dd id="TimingCallback.on_predict_batch_start" class="function">on_predict_batch_start</dd>
                <dd id="TimingCallback.on_predict_batch_end" class="function">on_predict_batch_end</dd>
                <dd id="TimingCallback.on_batch_end" class="function">on_batch_end</dd>
                <dd id="TimingCallback.on_train_start" class="function">on_train_start</dd>
                <dd id="TimingCallback.on_train_end" class="function">on_train_end</dd>
                <dd id="TimingCallback.on_pretrain_routine_start" class="function">on_pretrain_routine_start</dd>
                <dd id="TimingCallback.on_pretrain_routine_end" class="function">on_pretrain_routine_end</dd>
                <dd id="TimingCallback.on_validation_start" class="function">on_validation_start</dd>
                <dd id="TimingCallback.on_validation_end" class="function">on_validation_end</dd>
                <dd id="TimingCallback.on_test_start" class="function">on_test_start</dd>
                <dd id="TimingCallback.on_test_end" class="function">on_test_end</dd>
                <dd id="TimingCallback.on_predict_start" class="function">on_predict_start</dd>
                <dd id="TimingCallback.on_predict_end" class="function">on_predict_end</dd>
                <dd id="TimingCallback.on_keyboard_interrupt" class="function">on_keyboard_interrupt</dd>
                <dd id="TimingCallback.on_exception" class="function">on_exception</dd>
                <dd id="TimingCallback.on_save_checkpoint" class="function">on_save_checkpoint</dd>
                <dd id="TimingCallback.on_load_checkpoint" class="function">on_load_checkpoint</dd>
                <dd id="TimingCallback.on_before_optimizer_step" class="function">on_before_optimizer_step</dd>
                <dd id="TimingCallback.on_before_zero_grad" class="function">on_before_zero_grad</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="exp_manager">
                            <div class="attr function"><a class="headerlink" href="#exp_manager">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">exp_manager</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    cfg: Union[omegaconf.dictconfig.DictConfig, Dict, NoneType] = None
) -&gt; Optional[pathlib.Path]</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">exp_manager</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Path</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    exp_manager is a helper function used to manage folders for experiments. It follows the pytorch lightning paradigm</span>
<span class="sd">    of exp_dir/model_or_experiment_name/version. If the lightning trainer has a logger, exp_manager will get exp_dir,</span>
<span class="sd">    name, and version from the logger. Otherwise it will use the exp_dir and name arguments to create the logging</span>
<span class="sd">    directory. exp_manager also allows for explicit folder creation via explicit_log_dir.</span>
<span class="sd">    The version can be a datetime string or an integer. Datetime version can be disabled if use_datetime_version is set</span>
<span class="sd">     to False. It optionally creates TensorBoardLogger, WandBLogger, ModelCheckpoint objects from pytorch lightning.</span>
<span class="sd">    It copies sys.argv, and git information if available to the logging directory. It creates a log file for each</span>
<span class="sd">    process to log their output into.</span>
<span class="sd">    exp_manager additionally has a resume feature (resume_if_exists) which can be used to continuing training from</span>
<span class="sd">    the constructed log_dir. When you need to continue the training repeatedly (like on a cluster which you need</span>
<span class="sd">    multiple consecutive jobs), you need to avoid creating the version folders. Therefore from v1.0.0, when</span>
<span class="sd">    resume_if_exists is set to True, creating the version folders is ignored.</span>
<span class="sd">    Args:</span>
<span class="sd">        trainer (Trainer): The lightning trainer.</span>
<span class="sd">        cfg (DictConfig, dict): Can have the following keys:</span>
<span class="sd">            - explicit_log_dir (str, Path): Can be used to override exp_dir/name/version folder creation. Defaults to</span>
<span class="sd">                None, which will use exp_dir, name, and version to construct the logging directory.</span>
<span class="sd">            - exp_dir (str, Path): The base directory to create the logging directory. Defaults to None, which logs to</span>
<span class="sd">                ./mridc_experiments.</span>
<span class="sd">            - name (str): The name of the experiment. Defaults to None which turns into &quot;default&quot; via name = name or</span>
<span class="sd">                &quot;default&quot;.</span>
<span class="sd">            - version (str): The version of the experiment. Defaults to None which uses either a datetime string or</span>
<span class="sd">                lightning&#39;s TensorboardLogger system of using version_{int}.</span>
<span class="sd">            - use_datetime_version (bool): Whether to use a datetime string for version. Defaults to True.</span>
<span class="sd">            - resume_if_exists (bool): Whether this experiment is resuming from a previous run. If True, it sets</span>
<span class="sd">                trainer.checkpoint_connector.resume_from_checkpoint_fit_path so that the trainer should auto-resume.</span>
<span class="sd">                exp_manager will move files under log_dir to log_dir/run_{int}. Defaults to False. From v1.0.0, when</span>
<span class="sd">                resume_if_exists is True, we would not create version folders to make it easier to find the log folder</span>
<span class="sd">                 for next runs.</span>
<span class="sd">            - resume_past_end (bool): exp_manager errors out if resume_if_exists is True and a checkpoint matching</span>
<span class="sd">                *end.ckpt indicating a previous training run fully completed. This behaviour can be disabled, in which</span>
<span class="sd">                case the *end.ckpt will be loaded by setting resume_past_end to True. Defaults to False.</span>
<span class="sd">            - resume_ignore_no_checkpoint (bool): exp_manager errors out if resume_if_exists is True and no checkpoint</span>
<span class="sd">                could be found. This behaviour can be disabled, in which case exp_manager will print a message and</span>
<span class="sd">                continue without restoring, by setting resume_ignore_no_checkpoint to True. Defaults to False.</span>
<span class="sd">            - create_tensorboard_logger (bool): Whether to create a tensorboard logger and attach it to the pytorch</span>
<span class="sd">                lightning trainer. Defaults to True.</span>
<span class="sd">            - summary_writer_kwargs (dict): A dictionary of kwargs that can be passed to lightning&#39;s TensorboardLogger</span>
<span class="sd">                class. Note that log_dir is passed by exp_manager and cannot exist in this dict. Defaults to None.</span>
<span class="sd">            - create_wandb_logger (bool): Whether to create a Weights and Biases logger and attach it to the pytorch</span>
<span class="sd">                lightning trainer. Defaults to False.</span>
<span class="sd">            - wandb_logger_kwargs (dict): A dictionary of kwargs that can be passed to lightning&#39;s WandBLogger</span>
<span class="sd">                class. Note that name and project are required parameters if create_wandb_logger is True.</span>
<span class="sd">                Defaults to None.</span>
<span class="sd">            - create_checkpoint_callback (bool): Whether to create a ModelCheckpoint callback and attach it to the</span>
<span class="sd">                pytorch lightning trainer. The ModelCheckpoint saves the top 3 models with the best &quot;val_loss&quot;, the</span>
<span class="sd">                most recent checkpoint under *last.ckpt, and the final checkpoint after training completes under</span>
<span class="sd">                *end.ckpt. Defaults to True.</span>
<span class="sd">            - files_to_copy (list): A list of files to copy to the experiment logging directory. Defaults to None which</span>
<span class="sd">                copies no files.</span>
<span class="sd">    returns:</span>
<span class="sd">        log_dir (Path): The final logging directory where logging files are saved. Usually the concatenation of</span>
<span class="sd">            exp_dir, name, and version.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add rank information to logger</span>
    <span class="c1"># Note: trainer.global_rank and trainer.is_global_zero are not set until trainer.fit, so have to hack around it</span>
    <span class="n">local_rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">global_rank</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">node_rank</span> <span class="o">*</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">+</span> <span class="n">local_rank</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">rank</span> <span class="o">=</span> <span class="n">global_rank</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">world_size</span>

    <span class="k">if</span> <span class="n">cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;exp_manager did not receive a cfg argument. It will be disabled.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fast_dev_run</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Trainer was called with fast_dev_run. exp_manager will return without any functionality.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span>

    <span class="c1"># Ensure passed cfg is compliant with ExpManagerConfig</span>
    <span class="n">schema</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">structured</span><span class="p">(</span><span class="n">ExpManagerConfig</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cfg was type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span><span class="si">}</span><span class="s2">. Expected either a dict or a DictConfig&quot;</span><span class="p">)</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>

    <span class="n">error_checks</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>  <span class="c1"># Ensures that trainer options are compliant with MRIDC and exp_manager arguments</span>

    <span class="n">log_dir</span><span class="p">,</span> <span class="n">exp_dir</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">version</span> <span class="o">=</span> <span class="n">get_log_dir</span><span class="p">(</span>
        <span class="n">trainer</span><span class="o">=</span><span class="n">trainer</span><span class="p">,</span>
        <span class="n">exp_dir</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">exp_dir</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">version</span><span class="p">,</span>
        <span class="n">explicit_log_dir</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">explicit_log_dir</span><span class="p">,</span>
        <span class="n">use_datetime_version</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">use_datetime_version</span><span class="p">,</span>
        <span class="n">resume_if_exists</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">:</span>
        <span class="n">check_resume</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">log_dir</span><span class="p">),</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_past_end</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_ignore_no_checkpoint</span><span class="p">)</span>

    <span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="c1"># If name returned from get_log_dir is &quot;&quot;, use cfg.name for checkpointing</span>
    <span class="k">if</span> <span class="n">checkpoint_name</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">checkpoint_name</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
        <span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;default&quot;</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>  <span class="c1"># Used for configure_loggers so that the log_dir is properly set even if name is &quot;&quot;</span>
    <span class="n">cfg</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">version</span>

    <span class="c1"># update app_state with log_dir, exp_dir, etc</span>
    <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">log_dir</span> <span class="o">=</span> <span class="n">log_dir</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">exp_dir</span> <span class="o">=</span> <span class="n">exp_dir</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">version</span> <span class="o">=</span> <span class="n">version</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">checkpoint_name</span> <span class="o">=</span> <span class="n">checkpoint_name</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">create_checkpoint_callback</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_checkpoint_callback</span>
    <span class="n">app_state</span><span class="o">.</span><span class="n">checkpoint_callback_params</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_callback_params</span>

    <span class="c1"># Create the logging directory if it does not exist</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Cannot limit creation to global zero as all ranks write to own log file</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Experiments will be logged at </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">_default_root_dir</span> <span class="o">=</span> <span class="n">log_dir</span>

    <span class="c1"># Handle logging to file</span>
    <span class="k">if</span> <span class="n">get_envbool</span><span class="p">(</span><span class="n">MRIDC_ENV_VARNAME_TESTING</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span> <span class="ow">or</span> <span class="n">world_size</span> <span class="o">&lt;=</span> <span class="mi">32</span><span class="p">:</span>
        <span class="c1"># If MRIDC_TESTING is set (debug mode) or if less than 32 ranks save all log files</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">world_size</span> <span class="o">&lt;=</span> <span class="mi">256</span> <span class="ow">and</span> <span class="n">local_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># If less than 256 ranks, try to save 1 log file per &quot;machine&quot;</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">global_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># If running more than 256 ranks, only save 1 log file</span>
        <span class="n">log_file</span> <span class="o">=</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;mridc_log_globalrank-</span><span class="si">{</span><span class="n">global_rank</span><span class="si">}</span><span class="s2">_localrank-</span><span class="si">{</span><span class="n">local_rank</span><span class="si">}</span><span class="s2">.txt&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_file_handler</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>

    <span class="c1"># For some reason, LearningRateLogger requires trainer to have a logger. Safer to create logger on all ranks</span>
    <span class="c1"># not just global rank 0.</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span> <span class="ow">or</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">:</span>
        <span class="n">configure_loggers</span><span class="p">(</span>
            <span class="n">trainer</span><span class="p">,</span>
            <span class="p">[</span><span class="n">Path</span><span class="p">(</span><span class="n">exp_dir</span><span class="p">)],</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">version</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">summary_writer_kwargs</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">wandb_logger_kwargs</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># add loggers timing callbacks</span>
    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">log_step_timing</span><span class="p">:</span>
        <span class="n">timing_callback</span> <span class="o">=</span> <span class="n">TimingCallback</span><span class="p">(</span><span class="n">timer_kwargs</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">step_timing_kwargs</span> <span class="ow">or</span> <span class="p">{})</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">timing_callback</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_checkpoint_callback</span><span class="p">:</span>
        <span class="n">configure_checkpointing</span><span class="p">(</span>
            <span class="n">trainer</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">,</span> <span class="n">checkpoint_name</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">resume_if_exists</span><span class="p">,</span> <span class="n">cfg</span><span class="o">.</span><span class="n">checkpoint_callback_params</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
        <span class="c1"># Move files_to_copy to folder and add git information if present</span>
        <span class="k">if</span> <span class="n">cfg</span><span class="o">.</span><span class="n">files_to_copy</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_file</span> <span class="ow">in</span> <span class="n">cfg</span><span class="o">.</span><span class="n">files_to_copy</span><span class="p">:</span>
                <span class="n">copy</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">_file</span><span class="p">),</span> <span class="n">log_dir</span><span class="p">)</span>

        <span class="c1"># Create files for cmd args and git info</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;cmd-args.log&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">_file</span><span class="p">:</span>
            <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">))</span>

        <span class="c1"># Try to get git hash</span>
        <span class="n">git_repo</span><span class="p">,</span> <span class="n">git_hash</span> <span class="o">=</span> <span class="n">get_git_hash</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">git_repo</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;git-info.log&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">_file</span><span class="p">:</span>
                <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;commit hash: </span><span class="si">{</span><span class="n">git_hash</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">get_git_diff</span><span class="p">())</span>

        <span class="c1"># Add err_file logging to global_rank zero</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">add_err_file_handler</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;mridc_error_log.txt&quot;</span><span class="p">)</span>

        <span class="c1"># Add lightning file logging to global_rank zero</span>
        <span class="n">add_filehandlers_to_pl_logger</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;lightning_logs.txt&quot;</span><span class="p">,</span> <span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;mridc_error_log.txt&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">log_dir</span>
</pre></div>

        </details>

            <div class="docstring"><p>exp_manager is a helper function used to manage folders for experiments. It follows the pytorch lightning paradigm
of exp_dir/model_or_experiment_name/version. If the lightning trainer has a logger, exp_manager will get exp_dir,
name, and version from the logger. Otherwise it will use the exp_dir and name arguments to create the logging
directory. exp_manager also allows for explicit folder creation via explicit_log_dir.
The version can be a datetime string or an integer. Datetime version can be disabled if use_datetime_version is set
 to False. It optionally creates TensorBoardLogger, WandBLogger, ModelCheckpoint objects from pytorch lightning.
It copies sys.argv, and git information if available to the logging directory. It creates a log file for each
process to log their output into.
exp_manager additionally has a resume feature (resume_if_exists) which can be used to continuing training from
the constructed log_dir. When you need to continue the training repeatedly (like on a cluster which you need
multiple consecutive jobs), you need to avoid creating the version folders. Therefore from v1.0.0, when
resume_if_exists is set to True, creating the version folders is ignored.
Args:
    trainer (Trainer): The lightning trainer.
    cfg (DictConfig, dict): Can have the following keys:
        - explicit_log_dir (str, Path): Can be used to override exp_dir/name/version folder creation. Defaults to
            None, which will use exp_dir, name, and version to construct the logging directory.
        - exp_dir (str, Path): The base directory to create the logging directory. Defaults to None, which logs to
            ./mridc_experiments.
        - name (str): The name of the experiment. Defaults to None which turns into "default" via name = name or
            "default".
        - version (str): The version of the experiment. Defaults to None which uses either a datetime string or
            lightning's TensorboardLogger system of using version_{int}.
        - use_datetime_version (bool): Whether to use a datetime string for version. Defaults to True.
        - resume_if_exists (bool): Whether this experiment is resuming from a previous run. If True, it sets
            trainer.checkpoint_connector.resume_from_checkpoint_fit_path so that the trainer should auto-resume.
            exp_manager will move files under log_dir to log_dir/run_{int}. Defaults to False. From v1.0.0, when
            resume_if_exists is True, we would not create version folders to make it easier to find the log folder
             for next runs.
        - resume_past_end (bool): exp_manager errors out if resume_if_exists is True and a checkpoint matching
            *end.ckpt indicating a previous training run fully completed. This behaviour can be disabled, in which
            case the *end.ckpt will be loaded by setting resume_past_end to True. Defaults to False.
        - resume_ignore_no_checkpoint (bool): exp_manager errors out if resume_if_exists is True and no checkpoint
            could be found. This behaviour can be disabled, in which case exp_manager will print a message and
            continue without restoring, by setting resume_ignore_no_checkpoint to True. Defaults to False.
        - create_tensorboard_logger (bool): Whether to create a tensorboard logger and attach it to the pytorch
            lightning trainer. Defaults to True.
        - summary_writer_kwargs (dict): A dictionary of kwargs that can be passed to lightning's TensorboardLogger
            class. Note that log_dir is passed by exp_manager and cannot exist in this dict. Defaults to None.
        - create_wandb_logger (bool): Whether to create a Weights and Biases logger and attach it to the pytorch
            lightning trainer. Defaults to False.
        - wandb_logger_kwargs (dict): A dictionary of kwargs that can be passed to lightning's WandBLogger
            class. Note that name and project are required parameters if create_wandb_logger is True.
            Defaults to None.
        - create_checkpoint_callback (bool): Whether to create a ModelCheckpoint callback and attach it to the
            pytorch lightning trainer. The ModelCheckpoint saves the top 3 models with the best "val_loss", the
            most recent checkpoint under *last.ckpt, and the final checkpoint after training completes under
            *end.ckpt. Defaults to True.
        - files_to_copy (list): A list of files to copy to the experiment logging directory. Defaults to None which
            copies no files.
returns:
    log_dir (Path): The final logging directory where logging files are saved. Usually the concatenation of
        exp_dir, name, and version.</p>
</div>


                </section>
                <section id="error_checks">
                            <div class="attr function"><a class="headerlink" href="#error_checks">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">error_checks</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    cfg: Union[omegaconf.dictconfig.DictConfig, Dict, NoneType] = None
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">error_checks</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks that the passed trainer is compliant with MRIDC and exp_manager&#39;s passed configuration. Checks that:</span>
<span class="sd">        - Throws error when hydra has changed the working directory. This causes issues with lightning&#39;s DDP</span>
<span class="sd">        - Throws error when trainer has loggers defined but create_tensorboard_logger or create_WandB_logger is True</span>
<span class="sd">        - Prints error messages when 1) run on multi-node and not Slurm, and 2) run on multi-gpu without DDP</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">HydraConfig</span><span class="o">.</span><span class="n">initialized</span><span class="p">()</span> <span class="ow">and</span> <span class="n">get_original_cwd</span><span class="p">()</span> <span class="o">!=</span> <span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Hydra changed the working directory. This interferes with ExpManger&#39;s functionality. Please pass &quot;</span>
            <span class="s2">&quot;hydra.run.dir=. to your python script.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">create_tensorboard_logger</span> <span class="ow">or</span> <span class="n">cfg</span><span class="o">.</span><span class="n">create_wandb_logger</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, and either &quot;</span>
            <span class="s2">&quot;create_tensorboard_logger or create_wandb_logger was set to True. These can only be used if trainer does &quot;</span>
            <span class="s2">&quot;not already have a logger.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_nodes</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">check_slurm</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="s2">&quot;You are running multi-node training without SLURM handling the processes.&quot;</span>
            <span class="s2">&quot; Please note that this is not tested in MRIDC and could result in errors.&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="p">,</span> <span class="n">DDPPlugin</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="s2">&quot;You are running multi-gpu without ddp.Please note that this is not tested in MRIDC and could result in &quot;</span>
            <span class="s2">&quot;errors.&quot;</span>
        <span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Checks that the passed trainer is compliant with MRIDC and exp_manager's passed configuration. Checks that:
    - Throws error when hydra has changed the working directory. This causes issues with lightning's DDP
    - Throws error when trainer has loggers defined but create_tensorboard_logger or create_WandB_logger is True
    - Prints error messages when 1) run on multi-node and not Slurm, and 2) run on multi-gpu without DDP</p>
</div>


                </section>
                <section id="check_resume">
                            <div class="attr function"><a class="headerlink" href="#check_resume">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">check_resume</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    log_dir: str,
    resume_past_end: bool = False,
    resume_ignore_no_checkpoint: bool = False
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">check_resume</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">log_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">resume_past_end</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Checks that resume=True was used correctly with the arguments pass to exp_manager. Sets</span>
<span class="sd">    trainer.checkpoint_connector.resume_from_checkpoint_fit_path as necessary.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">    Raises:</span>
<span class="sd">        NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.</span>
<span class="sd">        ValueError: If resume is True, and there were more than 1 checkpoint could found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">log_dir</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming requires the log_dir </span><span class="si">{</span><span class="n">log_dir</span><span class="si">}</span><span class="s2"> to be passed to exp_manager&quot;</span><span class="p">)</span>

    <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>

    <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">end_checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*end.ckpt&quot;</span><span class="p">))</span>
    <span class="n">last_checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*last.ckpt&quot;</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">checkpoint_dir</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;There was no checkpoint folder at checkpoint_dir :</span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Training from scratch.&quot;</span>
            <span class="p">)</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="n">NotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There was no checkpoint folder at checkpoint_dir :</span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Cannot resume.&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">end_checkpoints</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">resume_past_end</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> indicating that the last training run has already completed.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">end_checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;mp_rank&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiple checkpoints </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="si">}</span><span class="s2"> that matches *end.ckpt.&quot;</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming from </span><span class="si">{</span><span class="n">end_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">last_checkpoints</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">resume_ignore_no_checkpoint</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were no checkpoints found in </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Training from scratch.&quot;</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">raise</span> <span class="n">NotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There were no checkpoints found in </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">. Cannot resume.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">last_checkpoints</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;mp_rank&quot;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Multiple checkpoints </span><span class="si">{</span><span class="n">last_checkpoints</span><span class="si">}</span><span class="s2"> that matches *last.ckpt.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Resuming from </span><span class="si">{</span><span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">last_checkpoints</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_from_checkpoint_fit_path</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">files_to_move</span> <span class="o">:=</span> <span class="p">[</span><span class="n">child</span> <span class="k">for</span> <span class="n">child</span> <span class="ow">in</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span><span class="o">.</span><span class="n">iterdir</span><span class="p">()</span> <span class="k">if</span> <span class="n">child</span><span class="o">.</span><span class="n">is_file</span><span class="p">()]:</span>
            <span class="c1"># Move old files to a new folder</span>
            <span class="n">other_run_dirs</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s2">&quot;run_*&quot;</span><span class="p">)</span>
            <span class="n">run_count</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">bool</span><span class="p">(</span><span class="n">fold</span><span class="o">.</span><span class="n">is_dir</span><span class="p">())</span> <span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="n">other_run_dirs</span><span class="p">)</span>
            <span class="n">new_run_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span> <span class="o">/</span> <span class="sa">f</span><span class="s2">&quot;run_</span><span class="si">{</span><span class="n">run_count</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">new_run_dir</span><span class="o">.</span><span class="n">mkdir</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">_file</span> <span class="ow">in</span> <span class="n">files_to_move</span><span class="p">:</span>
                <span class="n">move</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">_file</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">new_run_dir</span><span class="p">))</span>
</pre></div>

        </details>

            <div class="docstring"><p>Checks that resume=True was used correctly with the arguments pass to exp_manager. Sets
trainer.checkpoint_connector.resume_from_checkpoint_fit_path as necessary.
Returns:
    log_dir (Path): the log_dir
    exp_dir (str): the base exp_dir without name nor version
    name (str): The name of the experiment
    version (str): The version of the experiment
Raises:
    NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.
    ValueError: If resume is True, and there were more than 1 checkpoint could found.</p>
</div>


                </section>
                <section id="check_explicit_log_dir">
                            <div class="attr function"><a class="headerlink" href="#check_explicit_log_dir">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">check_explicit_log_dir</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    explicit_log_dir: List[Union[pathlib.Path, str]],
    exp_dir: str,
    name: str,
    version: str
) -&gt; Tuple[pathlib.Path, str, str, str]</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">check_explicit_log_dir</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span> <span class="n">exp_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">version</span><span class="p">:</span> <span class="nb">str</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Checks that the passed arguments are compatible with explicit_log_dir.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">    Raise:</span>
<span class="sd">        LoggerMisconfigurationError</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger and explicit_log_dir: &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2"> was pass to exp_manager. Please remove the logger from the lightning trainer.&quot;</span>
        <span class="p">)</span>
    <span class="c1"># Checking only (explicit_log_dir) vs (exp_dir and version).</span>
    <span class="c1"># The `name` will be used as the actual name of checkpoint/archive.</span>
    <span class="k">if</span> <span class="n">exp_dir</span> <span class="ow">or</span> <span class="n">version</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;exp_manager received explicit_log_dir: </span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2"> and at least one of exp_dir: </span><span class="si">{</span><span class="n">exp_dir</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;or version: </span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s2">. Please note that exp_dir, name, and version will be ignored.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">()</span> <span class="ow">and</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">))</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exp_manager is logging to </span><span class="si">{</span><span class="n">explicit_log_dir</span><span class="si">}</span><span class="s2">, but it already exists.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">)),</span> <span class="nb">str</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">),</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Checks that the passed arguments are compatible with explicit_log_dir.
Returns:
    log_dir (Path): the log_dir
    exp_dir (str): the base exp_dir without name nor version
    name (str): The name of the experiment
    version (str): The version of the experiment
Raise:
    LoggerMisconfigurationError</p>
</div>


                </section>
                <section id="get_log_dir">
                            <div class="attr function"><a class="headerlink" href="#get_log_dir">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_log_dir</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    exp_dir: str = None,
    name: str = None,
    version: str = None,
    explicit_log_dir: str = None,
    use_datetime_version: bool = True,
    resume_if_exists: bool = False
) -&gt; Tuple[pathlib.Path, str, str, str]</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_log_dir</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">explicit_log_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_datetime_version</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">resume_if_exists</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Obtains the log_dir used for exp_manager.</span>
<span class="sd">    Returns:</span>
<span class="sd">        log_dir (Path): the log_dir</span>
<span class="sd">        exp_dir (str): the base exp_dir without name nor version</span>
<span class="sd">        name (str): The name of the experiment</span>
<span class="sd">        version (str): The version of the experiment</span>
<span class="sd">        explicit_log_dir (str): The explicit path to the log folder. Defaults to False.</span>
<span class="sd">        use_datetime_version (bool): Uses date and time as the version of the log folder. Defaults to True.</span>
<span class="sd">        resume_if_exists (bool): if resume_if_exists of the exp_manager&#39;s config is enabled or not. When enabled, the</span>
<span class="sd">            version folders would not get created.</span>
<span class="sd">    Raise:</span>
<span class="sd">        LoggerMisconfigurationError: If trainer is incompatible with arguments</span>
<span class="sd">        NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.</span>
<span class="sd">        ValueError: If resume is True, and there were more than 1 checkpoint could found.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">explicit_log_dir</span><span class="p">:</span>  <span class="c1"># If explicit log_dir was passed, short circuit</span>
        <span class="k">return</span> <span class="n">check_explicit_log_dir</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="p">[</span><span class="n">Path</span><span class="p">(</span><span class="n">explicit_log_dir</span><span class="p">)],</span> <span class="nb">str</span><span class="p">(</span><span class="n">exp_dir</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">))</span>

    <span class="c1"># Default exp_dir to ./mridc_experiments if None was passed</span>
    <span class="n">_exp_dir</span> <span class="o">=</span> <span class="n">exp_dir</span>
    <span class="k">if</span> <span class="n">exp_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_exp_dir</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">()</span> <span class="o">/</span> <span class="s2">&quot;mridc_experiments&quot;</span><span class="p">)</span>

    <span class="c1"># If the user has already defined a logger for the trainer, use the logger defaults for logging directory</span>
    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">exp_dir</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
                    <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, the logger&#39;s &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;save_dir was not None, and exp_dir (</span><span class="si">{</span><span class="n">exp_dir</span><span class="si">}</span><span class="s2">) was not None. If trainer.logger.save_dir &quot;</span>
                    <span class="s2">&quot;exists, exp_manager will use trainer.logger.save_dir as the logging directory and exp_dir &quot;</span>
                    <span class="s2">&quot;must be None.&quot;</span>
                <span class="p">)</span>
            <span class="n">_exp_dir</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">save_dir</span>
        <span class="k">if</span> <span class="n">name</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">LoggerMisconfigurationError</span><span class="p">(</span>
                <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a logger, and name: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2"> was also passed to exp_manager. If the trainer contains a &quot;</span>
                <span class="s2">&quot;logger, exp_manager will use trainer.logger.name, and name passed to exp_manager must be None.&quot;</span>
            <span class="p">)</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">name</span>
        <span class="n">version</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;version_</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="c1"># Use user-defined exp_dir, project_name, exp_name, and versioning options</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">name</span> <span class="ow">or</span> <span class="s2">&quot;default&quot;</span>
        <span class="n">version</span> <span class="o">=</span> <span class="n">version</span> <span class="ow">or</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">MRIDC_ENV_VARNAME_VERSION</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">version</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">resume_if_exists</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;No version folders would be created under the log folder as &#39;resume_if_exists&#39; is enabled.&quot;</span>
                <span class="p">)</span>
                <span class="n">version</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">elif</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">use_datetime_version</span><span class="p">:</span>
                    <span class="n">version</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="n">_exp_dir</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
                    <span class="n">version</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;version_</span><span class="si">{</span><span class="n">tensorboard_logger</span><span class="o">.</span><span class="n">version</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">MRIDC_ENV_VARNAME_VERSION</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">version</span>

    <span class="n">log_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">_exp_dir</span><span class="p">))</span> <span class="o">/</span> <span class="n">Path</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">))</span> <span class="o">/</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;&quot;</span> <span class="k">if</span> <span class="n">version</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">log_dir</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">_exp_dir</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">version</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Obtains the log_dir used for exp_manager.
Returns:
    log_dir (Path): the log_dir
    exp_dir (str): the base exp_dir without name nor version
    name (str): The name of the experiment
    version (str): The version of the experiment
    explicit_log_dir (str): The explicit path to the log folder. Defaults to False.
    use_datetime_version (bool): Uses date and time as the version of the log folder. Defaults to True.
    resume_if_exists (bool): if resume_if_exists of the exp_manager's config is enabled or not. When enabled, the
        version folders would not get created.
Raise:
    LoggerMisconfigurationError: If trainer is incompatible with arguments
    NotFoundError: If resume is True, resume_ignore_no_checkpoint is False, and checkpoints could not be found.
    ValueError: If resume is True, and there were more than 1 checkpoint could found.</p>
</div>


                </section>
                <section id="get_git_hash">
                            <div class="attr function"><a class="headerlink" href="#get_git_hash">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_git_hash</span><span class="signature">()</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_git_hash</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function that tries to get the commit hash if running inside a git folder</span>
<span class="sd">    returns:</span>
<span class="sd">        Bool: Whether the git subprocess ran without error</span>
<span class="sd">        str: git subprocess output or error message</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;git&quot;</span><span class="p">,</span> <span class="s2">&quot;rev-parse&quot;</span><span class="p">,</span> <span class="s2">&quot;HEAD&quot;</span><span class="p">],</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
</pre></div>

        </details>

            <div class="docstring"><p>Helper function that tries to get the commit hash if running inside a git folder
returns:
    Bool: Whether the git subprocess ran without error
    str: git subprocess output or error message</p>
</div>


                </section>
                <section id="get_git_diff">
                            <div class="attr function"><a class="headerlink" href="#get_git_diff">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">get_git_diff</span><span class="signature">()</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_git_diff</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function that tries to get the git diff if running inside a git folder</span>
<span class="sd">    returns:</span>
<span class="sd">        Bool: Whether the git subprocess ran without error</span>
<span class="sd">        str: git subprocess output or error message</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">check_output</span><span class="p">([</span><span class="s2">&quot;git&quot;</span><span class="p">,</span> <span class="s2">&quot;diff&quot;</span><span class="p">],</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">CalledProcessError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">))</span>
</pre></div>

        </details>

            <div class="docstring"><p>Helper function that tries to get the git diff if running inside a git folder
returns:
    Bool: Whether the git subprocess ran without error
    str: git subprocess output or error message</p>
</div>


                </section>
                <section id="LoggerList">
                                <div class="attr class">
        <a class="headerlink" href="#LoggerList">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">LoggerList</span><wbr>(<span class="base">pytorch_lightning.loggers.base.LoggerCollection</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">LoggerList</span><span class="p">(</span><span class="n">_LoggerCollection</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A thin wrapper on Lightning&#39;s LoggerCollection such that name and version are better aligned with exp_manager&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_logger_iterable</span><span class="p">,</span> <span class="n">mridc_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mridc_version</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_logger_iterable</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_name</span> <span class="o">=</span> <span class="n">mridc_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_version</span> <span class="o">=</span> <span class="n">mridc_version</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">name</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The name of the experiment.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_name</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">version</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;The version of the experiment. If the logger was created with a version, this will be the version.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_version</span>
</pre></div>

        </details>

            <div class="docstring"><p>A thin wrapper on Lightning's LoggerCollection such that name and version are better aligned with exp_manager</p>
</div>


                            <div id="LoggerList.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#LoggerList.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">LoggerList</span><span class="signature">(_logger_iterable, mridc_name=None, mridc_version=&#39;&#39;)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">_logger_iterable</span><span class="p">,</span> <span class="n">mridc_name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mridc_version</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_logger_iterable</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_name</span> <span class="o">=</span> <span class="n">mridc_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_mridc_version</span> <span class="o">=</span> <span class="n">mridc_version</span>
</pre></div>

        </details>

    

                            </div>
                            <div id="LoggerList.name" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#LoggerList.name">#&nbsp;&nbsp</a>

        <span class="name">name</span><span class="annotation">: str</span>
    </div>

    
            <div class="docstring"><p>The name of the experiment.</p>
</div>


                            </div>
                            <div id="LoggerList.version" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#LoggerList.version">#&nbsp;&nbsp</a>

        <span class="name">version</span><span class="annotation">: str</span>
    </div>

    
            <div class="docstring"><p>The version of the experiment. If the logger was created with a version, this will be the version.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>pytorch_lightning.loggers.base.LoggerCollection</dt>
                                <dd id="LoggerList.after_save_checkpoint" class="function">after_save_checkpoint</dd>
                <dd id="LoggerList.update_agg_funcs" class="function">update_agg_funcs</dd>
                <dd id="LoggerList.experiment" class="variable">experiment</dd>
                <dd id="LoggerList.agg_and_log_metrics" class="function">agg_and_log_metrics</dd>
                <dd id="LoggerList.log_metrics" class="function">log_metrics</dd>
                <dd id="LoggerList.log_hyperparams" class="function">log_hyperparams</dd>
                <dd id="LoggerList.log_graph" class="function">log_graph</dd>
                <dd id="LoggerList.log_text" class="function">log_text</dd>
                <dd id="LoggerList.log_image" class="function">log_image</dd>
                <dd id="LoggerList.save" class="function">save</dd>
                <dd id="LoggerList.finalize" class="function">finalize</dd>
                <dd id="LoggerList.close" class="function">close</dd>
                <dd id="LoggerList.save_dir" class="variable">save_dir</dd>

            </div>
            <div><dt>pytorch_lightning.loggers.base.LightningLoggerBase</dt>
                                <dd id="LoggerList.group_separator" class="variable">group_separator</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="configure_loggers">
                            <div class="attr function"><a class="headerlink" href="#configure_loggers">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">configure_loggers</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    exp_dir: List[Union[pathlib.Path, str]],
    name: str,
    version: str,
    create_tensorboard_logger: bool,
    summary_writer_kwargs: dict,
    create_wandb_logger: bool,
    wandb_kwargs: dict
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">configure_loggers</span><span class="p">(</span>
    <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span>
    <span class="n">exp_dir</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Path</span><span class="p">,</span> <span class="nb">str</span><span class="p">]],</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">version</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">create_tensorboard_logger</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">summary_writer_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
    <span class="n">create_wandb_logger</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">wandb_kwargs</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates TensorboardLogger and/or WandBLogger and attach them to trainer. Raises ValueError if</span>
<span class="sd">    summary_writer_kwargs or wandb_kwargs are miss configured.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Potentially create tensorboard logger and/or WandBLogger</span>
    <span class="n">logger_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">create_tensorboard_logger</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">summary_writer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">summary_writer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">elif</span> <span class="s2">&quot;log_dir&quot;</span> <span class="ow">in</span> <span class="n">summary_writer_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;You cannot pass `log_dir` as part of `summary_writer_kwargs`. `log_dir` is handled by lightning&#39;s &quot;</span>
                <span class="s2">&quot;TensorBoardLogger logger.&quot;</span>
            <span class="p">)</span>
        <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span>
            <span class="n">save_dir</span><span class="o">=</span><span class="n">exp_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="o">**</span><span class="n">summary_writer_kwargs</span>
        <span class="p">)</span>
        <span class="n">logger_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tensorboard_logger</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;TensorboardLogger has been set up&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">create_wandb_logger</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">wandb_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">wandb_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;name&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wandb_kwargs</span> <span class="ow">and</span> <span class="s2">&quot;project&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">wandb_kwargs</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;name and project are required for wandb_logger&quot;</span><span class="p">)</span>
        <span class="n">wandb_logger</span> <span class="o">=</span> <span class="n">WandbLogger</span><span class="p">(</span><span class="n">save_dir</span><span class="o">=</span><span class="n">exp_dir</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="o">**</span><span class="n">wandb_kwargs</span><span class="p">)</span>

        <span class="n">logger_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">wandb_logger</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;WandBLogger has been set up&quot;</span><span class="p">)</span>

    <span class="n">logger_list</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">LoggerList</span><span class="p">(</span><span class="n">logger_list</span><span class="p">,</span> <span class="n">mridc_name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">mridc_version</span><span class="o">=</span><span class="n">version</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">logger_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">logger_list</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">logger_connector</span><span class="o">.</span><span class="n">configure_logger</span><span class="p">(</span><span class="n">logger_list</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Creates TensorboardLogger and/or WandBLogger and attach them to trainer. Raises ValueError if
summary_writer_kwargs or wandb_kwargs are miss configured.</p>
</div>


                </section>
                <section id="MRIDCModelCheckpoint">
                                <div class="attr class">
        <a class="headerlink" href="#MRIDCModelCheckpoint">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">MRIDCModelCheckpoint</span><wbr>(<span class="base">pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">MRIDCModelCheckpoint</span><span class="p">(</span><span class="n">ModelCheckpoint</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Light wrapper around Lightning&#39;s ModelCheckpoint to force a saved checkpoint on train_end&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">always_save_mridc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">save_mridc_on_train_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_best_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">postfix</span><span class="o">=</span><span class="s2">&quot;.mridc&quot;</span><span class="p">,</span>
        <span class="n">n_resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">model_parallel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            always_save_mridc (): (Default value = False)</span>
<span class="sd">            save_mridc_on_train_end (): (Default value = True)</span>
<span class="sd">            save_best_model (): (Default value = False)</span>
<span class="sd">            postfix (): (Default value = &quot;.mridc&quot;)</span>
<span class="sd">            n_resume (): (Default value = False)</span>
<span class="sd">            model_parallel_size (): (Default value = None)</span>
<span class="sd">            **kwargs (): (Default value = {})</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Parse and store &quot;extended&quot; parameters: save_best model and postfix.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span> <span class="o">=</span> <span class="n">always_save_mridc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span> <span class="o">=</span> <span class="n">save_mridc_on_train_end</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="o">=</span> <span class="n">save_best_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Found save_best_model is True and save_mridc_on_train_end is False. &quot;</span>
                    <span class="s2">&quot;Set save_mridc_on_train_end to True to automatically save the best model.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span> <span class="o">=</span> <span class="n">postfix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">=</span> <span class="n">model_parallel_size</span>

        <span class="c1"># `prefix` is deprecated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;prefix&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;prefix&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="c1"># Call the parent class constructor with the remaining kwargs.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">n_resume</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Checking previous runs&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mridc_topk_check_previous_run</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">mridc_topk_check_previous_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if there are previous runs with the same topk value.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="n">checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">)</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*.ckpt&quot;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uninject_mp_rank</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">checkpoint</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">==</span> <span class="s2">&quot;-last.ckpt&quot;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Find monitor in str + 1 for &#39;=&#39;</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">match</span> <span class="o">:=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;[A-z]&quot;</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span><span class="p">:]):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># -1 due to separator hypen</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># No saved checkpoints yet</span>

        <span class="n">_reverse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;min&quot;</span>

        <span class="n">best_k_models</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">_reverse</span><span class="p">)</span>

        <span class="c1"># This section should be ok as rank zero will delete all excess checkpoints, since all other ranks are</span>
        <span class="c1"># instantiated after rank zero. models_to_delete should be 0 for all other ranks.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of models to delete: </span><span class="si">{</span><span class="n">models_to_delete</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">models_to_delete</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_del_model_without_trainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed checkpoint: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_uninject_mp_rank</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Injects the rank of the current process into the checkpoint filepath.</span>

<span class="sd">        Args:</span>
<span class="sd">            filepath (): Path to the checkpoint file.</span>

<span class="sd">        Returns:</span>
<span class="sd">            str: Path to the checkpoint file with the rank of the current process injected.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">))</span>
        <span class="n">basename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
        <span class="n">filepath</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dirname</span><span class="p">,</span> <span class="n">basename</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">filepath</span>

    <span class="c1"># TODO: remove _save_last_checkpoint after fix https://github.com/PyTorchLightning/pytorch-lightning/issues/11451</span>
    <span class="k">def</span> <span class="nf">_save_last_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">monitor_candidates</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Save the last checkpoint.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_last</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">filepath</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">format_checkpoint_name</span><span class="p">(</span><span class="n">monitor_candidates</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">CHECKPOINT_NAME_LAST</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">!=</span> <span class="n">filepath</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="o">.</span><span class="n">remove_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">filepath</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_weights_only</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the default on_save_checkpoint to save the best model if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): The trainer object.</span>
<span class="sd">            pl_module (): The LightningModule object.</span>
<span class="sd">            checkpoint (): The checkpoint object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_save_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="c1"># Load the best model and then re-save it</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;always_save_nemo is not implemented for model parallel models.&quot;</span><span class="p">)</span>

        <span class="c1"># since we are creating tarfile artifacts we need to update .nemo path</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span>
            <span class="n">old_state_dict</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pl_module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;state_dict&quot;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span>

            <span class="c1"># get a new instance of the model</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">old_state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is called at the end of training.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): the trainer object</span>
<span class="sd">            pl_module (): the pl_module object</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fast_dev_run</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Call parent on_train_end() to save the -last checkpoint</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

        <span class="c1"># Load the best model and then re-save it</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> was told to save the best checkpoint at the end of training, but no saved checkpoints &quot;</span>
                    <span class="s2">&quot;were found. Saving latest model instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_del_model_without_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete a model without a trainer.</span>

<span class="sd">        Args:</span>
<span class="sd">            filepath (): path to the model to delete</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># filepath needs to be updated to include mp_rank</span>
            <span class="n">dirname</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
            <span class="n">basename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
            <span class="n">filepath</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dirname</span><span class="si">}</span><span class="s2">/mp_rank_</span><span class="si">{</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_rank</span><span class="si">:</span><span class="s2">02d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">basename</span><span class="si">}</span><span class="s2">&quot;</span>

        <span class="c1"># each model parallel rank needs to remove its model</span>
        <span class="k">if</span> <span class="n">is_global_rank_zero</span><span class="p">()</span> <span class="ow">or</span> <span class="p">(</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">data_parallel_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_fs</span><span class="o">.</span><span class="n">rm</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed checkpoint: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tried to remove checkpoint: </span><span class="si">{</span><span class="n">filepath</span><span class="si">}</span><span class="s2"> but failed.&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Light wrapper around Lightning's ModelCheckpoint to force a saved checkpoint on train_end</p>
</div>


                            <div id="MRIDCModelCheckpoint.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MRIDCModelCheckpoint.__init__">#&nbsp;&nbsp</a>

        
            <span class="name">MRIDCModelCheckpoint</span><span class="signature">(
    always_save_mridc=False,
    save_mridc_on_train_end=True,
    save_best_model=False,
    postfix=&#39;.mridc&#39;,
    n_resume=False,
    model_parallel_size=None,
    **kwargs
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">always_save_mridc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">save_mridc_on_train_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">save_best_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">postfix</span><span class="o">=</span><span class="s2">&quot;.mridc&quot;</span><span class="p">,</span>
        <span class="n">n_resume</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">model_parallel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            always_save_mridc (): (Default value = False)</span>
<span class="sd">            save_mridc_on_train_end (): (Default value = True)</span>
<span class="sd">            save_best_model (): (Default value = False)</span>
<span class="sd">            postfix (): (Default value = &quot;.mridc&quot;)</span>
<span class="sd">            n_resume (): (Default value = False)</span>
<span class="sd">            model_parallel_size (): (Default value = None)</span>
<span class="sd">            **kwargs (): (Default value = {})</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Parse and store &quot;extended&quot; parameters: save_best model and postfix.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span> <span class="o">=</span> <span class="n">always_save_mridc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span> <span class="o">=</span> <span class="n">save_mridc_on_train_end</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="o">=</span> <span class="n">save_best_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_model_path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="s2">&quot;Found save_best_model is True and save_mridc_on_train_end is False. &quot;</span>
                    <span class="s2">&quot;Set save_mridc_on_train_end to True to automatically save the best model.&quot;</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span> <span class="o">=</span> <span class="n">postfix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">=</span> <span class="n">model_parallel_size</span>

        <span class="c1"># `prefix` is deprecated</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;prefix&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="s2">&quot;prefix&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="c1"># Call the parent class constructor with the remaining kwargs.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">n_resume</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s2">&quot;Checking previous runs&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mridc_topk_check_previous_run</span><span class="p">()</span>
</pre></div>

        </details>

            <div class="docstring"><p>Args:
    always_save_mridc (): (Default value = False)
    save_mridc_on_train_end (): (Default value = True)
    save_best_model (): (Default value = False)
    postfix (): (Default value = ".mridc")
    n_resume (): (Default value = False)
    model_parallel_size (): (Default value = None)
    **kwargs (): (Default value = {})</p>
</div>


                            </div>
                            <div id="MRIDCModelCheckpoint.mridc_topk_check_previous_run" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MRIDCModelCheckpoint.mridc_topk_check_previous_run">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">mridc_topk_check_previous_run</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">mridc_topk_check_previous_run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Check if there are previous runs with the same topk value.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

        <span class="n">checkpoints</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">)</span><span class="o">.</span><span class="n">rglob</span><span class="p">(</span><span class="s2">&quot;*.ckpt&quot;</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">checkpoint</span> <span class="ow">in</span> <span class="n">checkpoints</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uninject_mp_rank</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">checkpoint</span><span class="p">[</span><span class="o">-</span><span class="mi">10</span><span class="p">:]</span> <span class="o">==</span> <span class="s2">&quot;-last.ckpt&quot;</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">index</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="o">.</span><span class="n">find</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">monitor</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># Find monitor in str + 1 for &#39;=&#39;</span>
            <span class="k">if</span> <span class="n">index</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">match</span> <span class="o">:=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;[A-z]&quot;</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span><span class="p">:]):</span>
                    <span class="n">value</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="n">index</span> <span class="p">:</span> <span class="n">index</span> <span class="o">+</span> <span class="n">match</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># -1 due to separator hypen</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="n">checkpoint</span><span class="p">]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">:</span>
            <span class="k">return</span>  <span class="c1"># No saved checkpoints yet</span>

        <span class="n">_reverse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">!=</span> <span class="s2">&quot;min&quot;</span>

        <span class="n">best_k_models</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="n">_reverse</span><span class="p">)</span>

        <span class="c1"># This section should be ok as rank zero will delete all excess checkpoints, since all other ranks are</span>
        <span class="c1"># instantiated after rank zero. models_to_delete should be 0 for all other ranks.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">models_to_delete</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">best_k_models</span><span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_top_k</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of models to delete: </span><span class="si">{</span><span class="n">models_to_delete</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">models_to_delete</span><span class="p">):</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_del_model_without_trainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Removed checkpoint: </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kth_best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">=</span> <span class="n">best_k_models</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_model_score</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_k_models</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">]</span>
</pre></div>

        </details>

            <div class="docstring"><p>Check if there are previous runs with the same topk value.</p>
</div>


                            </div>
                            <div id="MRIDCModelCheckpoint.on_save_checkpoint" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MRIDCModelCheckpoint.on_save_checkpoint">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_save_checkpoint</span><span class="signature">(self, trainer, pl_module, checkpoint)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Override the default on_save_checkpoint to save the best model if needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): The trainer object.</span>
<span class="sd">            pl_module (): The LightningModule object.</span>
<span class="sd">            checkpoint (): The checkpoint object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_save_checkpoint</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">always_save_mridc</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="c1"># Load the best model and then re-save it</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;always_save_nemo is not implemented for model parallel models.&quot;</span><span class="p">)</span>

        <span class="c1"># since we are creating tarfile artifacts we need to update .nemo path</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">previous_best_path</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">output</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">previous_model_path</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span>
            <span class="n">old_state_dict</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">pl_module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;state_dict&quot;</span> <span class="ow">in</span> <span class="n">checkpoint</span><span class="p">:</span>
                <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">]</span>

            <span class="c1"># get a new instance of the model</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">old_state_dict</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>
</pre></div>

        </details>

            <div class="docstring"><p>Override the default on_save_checkpoint to save the best model if needed.</p>

<p>Args:
    trainer (): The trainer object.
    pl_module (): The LightningModule object.
    checkpoint (): The checkpoint object.</p>

<p>Returns:
    None</p>
</div>


                            </div>
                            <div id="MRIDCModelCheckpoint.on_train_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#MRIDCModelCheckpoint.on_train_end">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_train_end</span><span class="signature">(self, trainer, pl_module)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This is called at the end of training.</span>

<span class="sd">        Args:</span>
<span class="sd">            trainer (): the trainer object</span>
<span class="sd">            pl_module (): the pl_module object</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">fast_dev_run</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Call parent on_train_end() to save the -last checkpoint</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">on_train_end</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">)</span>

        <span class="c1"># Load the best model and then re-save it</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_best_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span> <span class="o">==</span> <span class="s2">&quot;&quot;</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> was told to save the best checkpoint at the end of training, but no saved checkpoints &quot;</span>
                    <span class="s2">&quot;were found. Saving latest model instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">best_model_path</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_mridc_on_train_end</span><span class="p">:</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="n">save_path</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dirpath</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefix</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">postfix</span><span class="p">))</span>
</pre></div>

        </details>

            <div class="docstring"><p>This is called at the end of training.</p>

<p>Args:
    trainer (): the trainer object
    pl_module (): the pl_module object</p>

<p>Returns:
    None</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint</dt>
                                <dd id="MRIDCModelCheckpoint.CHECKPOINT_JOIN_CHAR" class="variable">CHECKPOINT_JOIN_CHAR</dd>
                <dd id="MRIDCModelCheckpoint.CHECKPOINT_NAME_LAST" class="variable">CHECKPOINT_NAME_LAST</dd>
                <dd id="MRIDCModelCheckpoint.FILE_EXTENSION" class="variable">FILE_EXTENSION</dd>
                <dd id="MRIDCModelCheckpoint.STARTING_VERSION" class="variable">STARTING_VERSION</dd>
                <dd id="MRIDCModelCheckpoint.state_key" class="variable">state_key</dd>
                <dd id="MRIDCModelCheckpoint.on_init_end" class="function">on_init_end</dd>
                <dd id="MRIDCModelCheckpoint.on_pretrain_routine_start" class="function">on_pretrain_routine_start</dd>
                <dd id="MRIDCModelCheckpoint.on_train_start" class="function">on_train_start</dd>
                <dd id="MRIDCModelCheckpoint.on_train_batch_end" class="function">on_train_batch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_train_epoch_end" class="function">on_train_epoch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_end" class="function">on_validation_end</dd>
                <dd id="MRIDCModelCheckpoint.on_load_checkpoint" class="function">on_load_checkpoint</dd>
                <dd id="MRIDCModelCheckpoint.save_checkpoint" class="function">save_checkpoint</dd>
                <dd id="MRIDCModelCheckpoint.every_n_epochs" class="variable">every_n_epochs</dd>
                <dd id="MRIDCModelCheckpoint.check_monitor_top_k" class="function">check_monitor_top_k</dd>
                <dd id="MRIDCModelCheckpoint.format_checkpoint_name" class="function">format_checkpoint_name</dd>
                <dd id="MRIDCModelCheckpoint.to_yaml" class="function">to_yaml</dd>
                <dd id="MRIDCModelCheckpoint.file_exists" class="function">file_exists</dd>

            </div>
            <div><dt>pytorch_lightning.callbacks.base.Callback</dt>
                                <dd id="MRIDCModelCheckpoint.on_configure_sharded_model" class="function">on_configure_sharded_model</dd>
                <dd id="MRIDCModelCheckpoint.on_before_accelerator_backend_setup" class="function">on_before_accelerator_backend_setup</dd>
                <dd id="MRIDCModelCheckpoint.setup" class="function">setup</dd>
                <dd id="MRIDCModelCheckpoint.teardown" class="function">teardown</dd>
                <dd id="MRIDCModelCheckpoint.on_init_start" class="function">on_init_start</dd>
                <dd id="MRIDCModelCheckpoint.on_fit_start" class="function">on_fit_start</dd>
                <dd id="MRIDCModelCheckpoint.on_fit_end" class="function">on_fit_end</dd>
                <dd id="MRIDCModelCheckpoint.on_sanity_check_start" class="function">on_sanity_check_start</dd>
                <dd id="MRIDCModelCheckpoint.on_sanity_check_end" class="function">on_sanity_check_end</dd>
                <dd id="MRIDCModelCheckpoint.on_train_batch_start" class="function">on_train_batch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_train_epoch_start" class="function">on_train_epoch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_epoch_start" class="function">on_validation_epoch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_epoch_end" class="function">on_validation_epoch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_test_epoch_start" class="function">on_test_epoch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_test_epoch_end" class="function">on_test_epoch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_epoch_start" class="function">on_predict_epoch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_epoch_end" class="function">on_predict_epoch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_epoch_start" class="function">on_epoch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_epoch_end" class="function">on_epoch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_batch_start" class="function">on_batch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_batch_start" class="function">on_validation_batch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_batch_end" class="function">on_validation_batch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_test_batch_start" class="function">on_test_batch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_test_batch_end" class="function">on_test_batch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_batch_start" class="function">on_predict_batch_start</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_batch_end" class="function">on_predict_batch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_batch_end" class="function">on_batch_end</dd>
                <dd id="MRIDCModelCheckpoint.on_pretrain_routine_end" class="function">on_pretrain_routine_end</dd>
                <dd id="MRIDCModelCheckpoint.on_validation_start" class="function">on_validation_start</dd>
                <dd id="MRIDCModelCheckpoint.on_test_start" class="function">on_test_start</dd>
                <dd id="MRIDCModelCheckpoint.on_test_end" class="function">on_test_end</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_start" class="function">on_predict_start</dd>
                <dd id="MRIDCModelCheckpoint.on_predict_end" class="function">on_predict_end</dd>
                <dd id="MRIDCModelCheckpoint.on_keyboard_interrupt" class="function">on_keyboard_interrupt</dd>
                <dd id="MRIDCModelCheckpoint.on_exception" class="function">on_exception</dd>
                <dd id="MRIDCModelCheckpoint.on_before_backward" class="function">on_before_backward</dd>
                <dd id="MRIDCModelCheckpoint.on_after_backward" class="function">on_after_backward</dd>
                <dd id="MRIDCModelCheckpoint.on_before_optimizer_step" class="function">on_before_optimizer_step</dd>
                <dd id="MRIDCModelCheckpoint.on_before_zero_grad" class="function">on_before_zero_grad</dd>

            </div>
                                </dl>
                            </div>
                </section>
                <section id="configure_checkpointing">
                            <div class="attr function"><a class="headerlink" href="#configure_checkpointing">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">configure_checkpointing</span><span class="signature">(
    trainer: pytorch_lightning.trainer.trainer.Trainer,
    log_dir: pathlib.Path,
    name: str,
    resume: bool,
    params: omegaconf.dictconfig.DictConfig
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">configure_checkpointing</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">log_dir</span><span class="p">:</span> <span class="n">Path</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">resume</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="s2">&quot;DictConfig&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adds ModelCheckpoint to trainer. Raises CheckpointMisconfigurationError if trainer already has a ModelCheckpoint</span>
<span class="sd">    callback or if trainer.weights_save_path was passed to Trainer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">callback</span><span class="p">,</span> <span class="n">ModelCheckpoint</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">CheckpointMisconfigurationError</span><span class="p">(</span>
                <span class="s2">&quot;The pytorch lightning trainer that was passed to exp_manager contained a ModelCheckpoint &quot;</span>
                <span class="s2">&quot;and create_checkpoint_callback was set to True. Please either set create_checkpoint_callback &quot;</span>
                <span class="s2">&quot;to False, or remove ModelCheckpoint from the lightning trainer&quot;</span>
            <span class="p">)</span>
    <span class="k">if</span> <span class="n">Path</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">weights_save_path</span><span class="p">)</span> <span class="o">!=</span> <span class="n">Path</span><span class="o">.</span><span class="n">cwd</span><span class="p">():</span>
        <span class="k">raise</span> <span class="n">CheckpointMisconfigurationError</span><span class="p">(</span>
            <span class="s2">&quot;The pytorch lightning was passed weights_save_path. This variable is ignored by exp_manager&quot;</span>
        <span class="p">)</span>

    <span class="c1"># Create the callback and attach it to trainer</span>
    <span class="k">if</span> <span class="s2">&quot;filepath&quot;</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filepath</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;filepath is deprecated. Please switch to dirpath and filename instead&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span><span class="o">.</span><span class="n">parent</span>
            <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
        <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="k">del</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;filepath&quot;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">dirpath</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">log_dir</span> <span class="o">/</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">--</span><span class="se">{{</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="si">}</span><span class="s2">:.4f</span><span class="se">}}</span><span class="s2">-</span><span class="se">{{</span><span class="s2">epoch</span><span class="se">}}</span><span class="s2">&quot;</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">prefix</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">params</span><span class="o">.</span><span class="n">prefix</span> <span class="o">=</span> <span class="n">name</span>
    <span class="n">MRIDCModelCheckpoint</span><span class="o">.</span><span class="n">CHECKPOINT_NAME_LAST</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">filename</span><span class="si">}</span><span class="s2">-last&quot;</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">dirpath</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">filename</span><span class="p">)</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">params</span><span class="o">.</span><span class="n">prefix</span><span class="p">)</span>

    <span class="k">if</span> <span class="s2">&quot;val&quot;</span> <span class="ow">in</span> <span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span>
            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span> <span class="o">&lt;</span> <span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span>
        <span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                <span class="s2">&quot;The checkpoint callback was told to monitor a validation value but trainer.max_epochs(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_epochs</span><span class="si">}</span><span class="s2">) was less than trainer.check_val_every_n_epoch(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span><span class="si">}</span><span class="s2">). It is very likely this run will fail with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;ModelCheckpoint(monitor=&#39;</span><span class="si">{</span><span class="n">params</span><span class="o">.</span><span class="n">monitor</span><span class="si">}</span><span class="s2">&#39;) not found in the returned metrics. Please ensure that &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;validation is run within trainer.max_epochs.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The checkpoint callback was told to monitor a validation value and trainer&#39;s max_steps was set to &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span><span class="si">}</span><span class="s2">. Please ensure that max_steps will run for at least &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">trainer</span><span class="o">.</span><span class="n">check_val_every_n_epoch</span><span class="si">}</span><span class="s2"> epochs to ensure that checkpointing will not error out.&quot;</span>
            <span class="p">)</span>

    <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">MRIDCModelCheckpoint</span><span class="p">(</span><span class="n">n_resume</span><span class="o">=</span><span class="n">resume</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">)</span>
    <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_from_checkpoint_fit_path</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">params</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span> <span class="o">=</span> <span class="n">MRIDCModelCheckpoint</span><span class="o">.</span><span class="n">_uninject_mp_rank</span><span class="p">(</span>
            <span class="n">checkpoint_callback</span><span class="o">.</span><span class="n">last_model_path</span>
        <span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">checkpoint_callback</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Adds ModelCheckpoint to trainer. Raises CheckpointMisconfigurationError if trainer already has a ModelCheckpoint
callback or if trainer.weights_save_path was passed to Trainer.</p>
</div>


                </section>
                <section id="check_slurm">
                            <div class="attr function"><a class="headerlink" href="#check_slurm">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">check_slurm</span><span class="signature">(trainer)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">def</span> <span class="nf">check_slurm</span><span class="p">(</span><span class="n">trainer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if the trainer is running on a slurm cluster. If so, it will check if the trainer is running on the master</span>
<span class="sd">    node. If it is not, it will exit.</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer (): The trainer to check.</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if the trainer is running on the master node, False otherwise.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">trainer</span><span class="o">.</span><span class="n">accelerator_connector</span><span class="o">.</span><span class="n">is_slurm_managing_tasks</span>
    <span class="k">except</span> <span class="ne">AttributeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
</pre></div>

        </details>

            <div class="docstring"><p>Checks if the trainer is running on a slurm cluster. If so, it will check if the trainer is running on the master
node. If it is not, it will exit.</p>

<p>Args:
    trainer (): The trainer to check.</p>

<p>Returns:
    bool: True if the trainer is running on the master node, False otherwise.</p>
</div>


                </section>
                <section id="StatelessTimer">
                                <div class="attr class">
        <a class="headerlink" href="#StatelessTimer">#&nbsp;&nbsp</a>

        
        <span class="def">class</span>
        <span class="name">StatelessTimer</span><wbr>(<span class="base">pytorch_lightning.callbacks.timer.Timer</span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">StatelessTimer</span><span class="p">(</span><span class="n">Timer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extension of PTL timers to be per run.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not save the state of the timer.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">callback_state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not load the state of the timer.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_check_time_remaining</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not check the time remaining.&quot;&quot;&quot;</span>
        <span class="c1"># Default timer only checks for train time exceeding max_time, this includes time for all stages.</span>
        <span class="n">train_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">TRAINING</span><span class="p">)</span>
        <span class="n">validation_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">VALIDATING</span><span class="p">)</span>
        <span class="n">test_duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_elapsed</span><span class="p">(</span><span class="n">RunningStage</span><span class="o">.</span><span class="n">TESTING</span><span class="p">)</span>
        <span class="n">total_duration</span> <span class="o">=</span> <span class="n">train_duration</span> <span class="o">+</span> <span class="n">validation_duration</span> <span class="o">+</span> <span class="n">test_duration</span>
        <span class="n">should_stop</span> <span class="o">=</span> <span class="n">total_duration</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_duration</span>
        <span class="n">should_stop</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">training_type_plugin</span><span class="o">.</span><span class="n">reduce_boolean_decision</span><span class="p">(</span><span class="n">should_stop</span><span class="p">)</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">should_stop</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">should_stop</span> <span class="ow">or</span> <span class="n">should_stop</span>
        <span class="k">if</span> <span class="n">should_stop</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_verbose</span><span class="p">:</span>
            <span class="n">rank_zero_info</span><span class="p">(</span><span class="s2">&quot;Time limit reached. Signaling Trainer to stop.&quot;</span><span class="p">)</span>
            <span class="n">rank_zero_info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Spent </span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">train_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on training, &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">validation_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on validation and &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span><span class="o">=</span><span class="n">test_duration</span><span class="p">)</span><span class="si">}</span><span class="s2"> seconds on testing&quot;</span>
            <span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Extension of PTL timers to be per run.</p>
</div>


                            <div id="StatelessTimer.on_save_checkpoint" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#StatelessTimer.on_save_checkpoint">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_save_checkpoint</span><span class="signature">(self, trainer, pl_module, checkpoint) -&gt; None</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not save the state of the timer.&quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Override to not save the state of the timer.</p>
</div>


                            </div>
                            <div id="StatelessTimer.on_load_checkpoint" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#StatelessTimer.on_load_checkpoint">#&nbsp;&nbsp</a>

        
            <span class="def">def</span>
            <span class="name">on_load_checkpoint</span><span class="signature">(self, trainer, pl_module, callback_state) -&gt; None</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">,</span> <span class="n">callback_state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Override to not load the state of the timer.&quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Override to not load the state of the timer.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>pytorch_lightning.callbacks.timer.Timer</dt>
                                <dd id="StatelessTimer.__init__" class="function">Timer</dd>
                <dd id="StatelessTimer.start_time" class="function">start_time</dd>
                <dd id="StatelessTimer.end_time" class="function">end_time</dd>
                <dd id="StatelessTimer.time_elapsed" class="function">time_elapsed</dd>
                <dd id="StatelessTimer.time_remaining" class="function">time_remaining</dd>
                <dd id="StatelessTimer.on_train_start" class="function">on_train_start</dd>
                <dd id="StatelessTimer.on_train_end" class="function">on_train_end</dd>
                <dd id="StatelessTimer.on_validation_start" class="function">on_validation_start</dd>
                <dd id="StatelessTimer.on_validation_end" class="function">on_validation_end</dd>
                <dd id="StatelessTimer.on_test_start" class="function">on_test_start</dd>
                <dd id="StatelessTimer.on_test_end" class="function">on_test_end</dd>
                <dd id="StatelessTimer.on_train_batch_end" class="function">on_train_batch_end</dd>
                <dd id="StatelessTimer.on_train_epoch_end" class="function">on_train_epoch_end</dd>

            </div>
            <div><dt>pytorch_lightning.callbacks.base.Callback</dt>
                                <dd id="StatelessTimer.state_key" class="variable">state_key</dd>
                <dd id="StatelessTimer.on_configure_sharded_model" class="function">on_configure_sharded_model</dd>
                <dd id="StatelessTimer.on_before_accelerator_backend_setup" class="function">on_before_accelerator_backend_setup</dd>
                <dd id="StatelessTimer.setup" class="function">setup</dd>
                <dd id="StatelessTimer.teardown" class="function">teardown</dd>
                <dd id="StatelessTimer.on_init_start" class="function">on_init_start</dd>
                <dd id="StatelessTimer.on_init_end" class="function">on_init_end</dd>
                <dd id="StatelessTimer.on_fit_start" class="function">on_fit_start</dd>
                <dd id="StatelessTimer.on_fit_end" class="function">on_fit_end</dd>
                <dd id="StatelessTimer.on_sanity_check_start" class="function">on_sanity_check_start</dd>
                <dd id="StatelessTimer.on_sanity_check_end" class="function">on_sanity_check_end</dd>
                <dd id="StatelessTimer.on_train_batch_start" class="function">on_train_batch_start</dd>
                <dd id="StatelessTimer.on_train_epoch_start" class="function">on_train_epoch_start</dd>
                <dd id="StatelessTimer.on_validation_epoch_start" class="function">on_validation_epoch_start</dd>
                <dd id="StatelessTimer.on_validation_epoch_end" class="function">on_validation_epoch_end</dd>
                <dd id="StatelessTimer.on_test_epoch_start" class="function">on_test_epoch_start</dd>
                <dd id="StatelessTimer.on_test_epoch_end" class="function">on_test_epoch_end</dd>
                <dd id="StatelessTimer.on_predict_epoch_start" class="function">on_predict_epoch_start</dd>
                <dd id="StatelessTimer.on_predict_epoch_end" class="function">on_predict_epoch_end</dd>
                <dd id="StatelessTimer.on_epoch_start" class="function">on_epoch_start</dd>
                <dd id="StatelessTimer.on_epoch_end" class="function">on_epoch_end</dd>
                <dd id="StatelessTimer.on_batch_start" class="function">on_batch_start</dd>
                <dd id="StatelessTimer.on_validation_batch_start" class="function">on_validation_batch_start</dd>
                <dd id="StatelessTimer.on_validation_batch_end" class="function">on_validation_batch_end</dd>
                <dd id="StatelessTimer.on_test_batch_start" class="function">on_test_batch_start</dd>
                <dd id="StatelessTimer.on_test_batch_end" class="function">on_test_batch_end</dd>
                <dd id="StatelessTimer.on_predict_batch_start" class="function">on_predict_batch_start</dd>
                <dd id="StatelessTimer.on_predict_batch_end" class="function">on_predict_batch_end</dd>
                <dd id="StatelessTimer.on_batch_end" class="function">on_batch_end</dd>
                <dd id="StatelessTimer.on_pretrain_routine_start" class="function">on_pretrain_routine_start</dd>
                <dd id="StatelessTimer.on_pretrain_routine_end" class="function">on_pretrain_routine_end</dd>
                <dd id="StatelessTimer.on_predict_start" class="function">on_predict_start</dd>
                <dd id="StatelessTimer.on_predict_end" class="function">on_predict_end</dd>
                <dd id="StatelessTimer.on_keyboard_interrupt" class="function">on_keyboard_interrupt</dd>
                <dd id="StatelessTimer.on_exception" class="function">on_exception</dd>
                <dd id="StatelessTimer.on_before_backward" class="function">on_before_backward</dd>
                <dd id="StatelessTimer.on_after_backward" class="function">on_after_backward</dd>
                <dd id="StatelessTimer.on_before_optimizer_step" class="function">on_before_optimizer_step</dd>
                <dd id="StatelessTimer.on_before_zero_grad" class="function">on_before_zero_grad</dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.type) {
                    case "function":
                        heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span><span class="signature">${doc.signature}:</span>`;
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value">${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.type}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>