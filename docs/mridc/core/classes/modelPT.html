<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="pdoc 10.0.1"/>
    <title>mridc.core.classes.modelPT API documentation</title>

    <style>/*! * Bootstrap Reboot v5.0.0 (https://getbootstrap.com/) * Copyright 2011-2021 The Bootstrap Authors * Copyright 2011-2021 Twitter, Inc. * Licensed under MIT (https://github.com/twbs/bootstrap/blob/main/LICENSE) * Forked from Normalize.css, licensed MIT (https://github.com/necolas/normalize.css/blob/master/LICENSE.md) */*,::after,::before{box-sizing:border-box}@media (prefers-reduced-motion:no-preference){:root{scroll-behavior:smooth}}body{margin:0;font-family:system-ui,-apple-system,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans","Liberation Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";font-size:1rem;font-weight:400;line-height:1.5;color:#212529;background-color:#fff;-webkit-text-size-adjust:100%;-webkit-tap-highlight-color:transparent}hr{margin:1rem 0;color:inherit;background-color:currentColor;border:0;opacity:.25}hr:not([size]){height:1px}h1,h2,h3,h4,h5,h6{margin-top:0;margin-bottom:.5rem;font-weight:500;line-height:1.2}h1{font-size:calc(1.375rem + 1.5vw)}@media (min-width:1200px){h1{font-size:2.5rem}}h2{font-size:calc(1.325rem + .9vw)}@media (min-width:1200px){h2{font-size:2rem}}h3{font-size:calc(1.3rem + .6vw)}@media (min-width:1200px){h3{font-size:1.75rem}}h4{font-size:calc(1.275rem + .3vw)}@media (min-width:1200px){h4{font-size:1.5rem}}h5{font-size:1.25rem}h6{font-size:1rem}p{margin-top:0;margin-bottom:1rem}abbr[data-bs-original-title],abbr[title]{-webkit-text-decoration:underline dotted;text-decoration:underline dotted;cursor:help;-webkit-text-decoration-skip-ink:none;text-decoration-skip-ink:none}address{margin-bottom:1rem;font-style:normal;line-height:inherit}ol,ul{padding-left:2rem}dl,ol,ul{margin-top:0;margin-bottom:1rem}ol ol,ol ul,ul ol,ul ul{margin-bottom:0}dt{font-weight:700}dd{margin-bottom:.5rem;margin-left:0}blockquote{margin:0 0 1rem}b,strong{font-weight:bolder}small{font-size:.875em}mark{padding:.2em;background-color:#fcf8e3}sub,sup{position:relative;font-size:.75em;line-height:0;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}a{color:#0d6efd;text-decoration:underline}a:hover{color:#0a58ca}a:not([href]):not([class]),a:not([href]):not([class]):hover{color:inherit;text-decoration:none}code,kbd,pre,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-size:1em;direction:ltr;unicode-bidi:bidi-override}pre{display:block;margin-top:0;margin-bottom:1rem;overflow:auto;font-size:.875em}pre code{font-size:inherit;color:inherit;word-break:normal}code{font-size:.875em;color:#d63384;word-wrap:break-word}a>code{color:inherit}kbd{padding:.2rem .4rem;font-size:.875em;color:#fff;background-color:#212529;border-radius:.2rem}kbd kbd{padding:0;font-size:1em;font-weight:700}figure{margin:0 0 1rem}img,svg{vertical-align:middle}table{caption-side:bottom;border-collapse:collapse}caption{padding-top:.5rem;padding-bottom:.5rem;color:#6c757d;text-align:left}th{text-align:inherit;text-align:-webkit-match-parent}tbody,td,tfoot,th,thead,tr{border-color:inherit;border-style:solid;border-width:0}label{display:inline-block}button{border-radius:0}button:focus:not(:focus-visible){outline:0}button,input,optgroup,select,textarea{margin:0;font-family:inherit;font-size:inherit;line-height:inherit}button,select{text-transform:none}[role=button]{cursor:pointer}select{word-wrap:normal}select:disabled{opacity:1}[list]::-webkit-calendar-picker-indicator{display:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]:not(:disabled),[type=reset]:not(:disabled),[type=submit]:not(:disabled),button:not(:disabled){cursor:pointer}::-moz-focus-inner{padding:0;border-style:none}textarea{resize:vertical}fieldset{min-width:0;padding:0;margin:0;border:0}legend{float:left;width:100%;padding:0;margin-bottom:.5rem;font-size:calc(1.275rem + .3vw);line-height:inherit}@media (min-width:1200px){legend{font-size:1.5rem}}legend+*{clear:left}::-webkit-datetime-edit-day-field,::-webkit-datetime-edit-fields-wrapper,::-webkit-datetime-edit-hour-field,::-webkit-datetime-edit-minute,::-webkit-datetime-edit-month-field,::-webkit-datetime-edit-text,::-webkit-datetime-edit-year-field{padding:0}::-webkit-inner-spin-button{height:auto}[type=search]{outline-offset:-2px;-webkit-appearance:textfield}::-webkit-search-decoration{-webkit-appearance:none}::-webkit-color-swatch-wrapper{padding:0}::file-selector-button{font:inherit}::-webkit-file-upload-button{font:inherit;-webkit-appearance:button}output{display:inline-block}iframe{border:0}summary{display:list-item;cursor:pointer}progress{vertical-align:baseline}[hidden]{display:none!important}</style>
    <style>/*! syntax-highlighting.css */pre{line-height:125%;}span.linenos{color:inherit; background-color:transparent; padding-left:5px; padding-right:20px;}.pdoc-code .hll{background-color:#ffffcc}.pdoc-code{background:#f8f8f8;}.pdoc-code .c{color:#3D7B7B; font-style:italic}.pdoc-code .err{border:1px solid #FF0000}.pdoc-code .k{color:#008000; font-weight:bold}.pdoc-code .o{color:#666666}.pdoc-code .ch{color:#3D7B7B; font-style:italic}.pdoc-code .cm{color:#3D7B7B; font-style:italic}.pdoc-code .cp{color:#9C6500}.pdoc-code .cpf{color:#3D7B7B; font-style:italic}.pdoc-code .c1{color:#3D7B7B; font-style:italic}.pdoc-code .cs{color:#3D7B7B; font-style:italic}.pdoc-code .gd{color:#A00000}.pdoc-code .ge{font-style:italic}.pdoc-code .gr{color:#E40000}.pdoc-code .gh{color:#000080; font-weight:bold}.pdoc-code .gi{color:#008400}.pdoc-code .go{color:#717171}.pdoc-code .gp{color:#000080; font-weight:bold}.pdoc-code .gs{font-weight:bold}.pdoc-code .gu{color:#800080; font-weight:bold}.pdoc-code .gt{color:#0044DD}.pdoc-code .kc{color:#008000; font-weight:bold}.pdoc-code .kd{color:#008000; font-weight:bold}.pdoc-code .kn{color:#008000; font-weight:bold}.pdoc-code .kp{color:#008000}.pdoc-code .kr{color:#008000; font-weight:bold}.pdoc-code .kt{color:#B00040}.pdoc-code .m{color:#666666}.pdoc-code .s{color:#BA2121}.pdoc-code .na{color:#687822}.pdoc-code .nb{color:#008000}.pdoc-code .nc{color:#0000FF; font-weight:bold}.pdoc-code .no{color:#880000}.pdoc-code .nd{color:#AA22FF}.pdoc-code .ni{color:#717171; font-weight:bold}.pdoc-code .ne{color:#CB3F38; font-weight:bold}.pdoc-code .nf{color:#0000FF}.pdoc-code .nl{color:#767600}.pdoc-code .nn{color:#0000FF; font-weight:bold}.pdoc-code .nt{color:#008000; font-weight:bold}.pdoc-code .nv{color:#19177C}.pdoc-code .ow{color:#AA22FF; font-weight:bold}.pdoc-code .w{color:#bbbbbb}.pdoc-code .mb{color:#666666}.pdoc-code .mf{color:#666666}.pdoc-code .mh{color:#666666}.pdoc-code .mi{color:#666666}.pdoc-code .mo{color:#666666}.pdoc-code .sa{color:#BA2121}.pdoc-code .sb{color:#BA2121}.pdoc-code .sc{color:#BA2121}.pdoc-code .dl{color:#BA2121}.pdoc-code .sd{color:#BA2121; font-style:italic}.pdoc-code .s2{color:#BA2121}.pdoc-code .se{color:#AA5D1F; font-weight:bold}.pdoc-code .sh{color:#BA2121}.pdoc-code .si{color:#A45A77; font-weight:bold}.pdoc-code .sx{color:#008000}.pdoc-code .sr{color:#A45A77}.pdoc-code .s1{color:#BA2121}.pdoc-code .ss{color:#19177C}.pdoc-code .bp{color:#008000}.pdoc-code .fm{color:#0000FF}.pdoc-code .vc{color:#19177C}.pdoc-code .vg{color:#19177C}.pdoc-code .vi{color:#19177C}.pdoc-code .vm{color:#19177C}.pdoc-code .il{color:#666666}</style>
    <style>/*! theme.css */:root{--pdoc-background:#fff;}.pdoc{--text:#212529;--muted:#6c757d;--link:#3660a5;--link-hover:#1659c5;--code:#f8f8f8;--active:#fff598;--accent:#eee;--accent2:#c1c1c1;--nav-hover:rgba(255, 255, 255, 0.5);--name:#0066BB;--def:#008800;--annotation:#007020;}</style>
    <style>/*! layout.css */html, body{width:100%;height:100%;}html, main{scroll-behavior:smooth;}body{background-color:var(--pdoc-background);}@media (max-width:769px){#navtoggle{cursor:pointer;position:absolute;width:50px;height:40px;top:1rem;right:1rem;border-color:var(--text);color:var(--text);display:flex;opacity:0.8;}#navtoggle:hover{opacity:1;}#togglestate + div{display:none;}#togglestate:checked + div{display:inherit;}main, header{padding:2rem 3vw;}header + main{margin-top:-3rem;}.git-button{display:none !important;}nav input[type="search"]{max-width:77%;}nav input[type="search"]:first-child{margin-top:-6px;}nav input[type="search"]:valid ~ *{display:none !important;}}@media (min-width:770px){:root{--sidebar-width:clamp(12.5rem, 28vw, 22rem);}nav{position:fixed;overflow:auto;height:100vh;width:var(--sidebar-width);}main, header{padding:3rem 2rem 3rem calc(var(--sidebar-width) + 3rem);width:calc(54rem + var(--sidebar-width));max-width:100%;}header + main{margin-top:-4rem;}#navtoggle{display:none;}}#togglestate{position:absolute;height:0;}nav.pdoc{--pad:1.75rem;--indent:1.5rem;background-color:var(--accent);border-right:1px solid var(--accent2);box-shadow:0 0 20px rgba(50, 50, 50, .2) inset;padding:0 0 0 var(--pad);overflow-wrap:anywhere;scrollbar-width:thin; scrollbar-color:var(--accent2) transparent }nav.pdoc::-webkit-scrollbar{width:.4rem; }nav.pdoc::-webkit-scrollbar-thumb{background-color:var(--accent2); }nav.pdoc > div{padding:var(--pad) 0;}nav.pdoc .module-list-button{display:inline-flex;align-items:center;color:var(--text);border-color:var(--muted);margin-bottom:1rem;}nav.pdoc .module-list-button:hover{border-color:var(--text);}nav.pdoc input[type=search]{display:block;outline-offset:0;width:calc(100% - var(--pad));}nav.pdoc .logo{max-width:calc(100% - var(--pad));max-height:35vh;display:block;margin:0 auto 1rem;transform:translate(calc(-.5 * var(--pad)), 0);}nav.pdoc ul{list-style:none;padding-left:0;}nav.pdoc li{display:block;margin:0;padding:.2rem 0 .2rem var(--indent);transition:all 100ms;}nav.pdoc > div > ul > li{padding-left:0;}nav.pdoc li:hover{background-color:var(--nav-hover);}nav.pdoc a, nav.pdoc a:hover{color:var(--text);}nav.pdoc a{display:block;}nav.pdoc > h2:first-of-type{margin-top:1.5rem;}nav.pdoc .class:before{content:"class ";color:var(--muted);}nav.pdoc .function:after{content:"()";color:var(--muted);}nav.pdoc footer:before{content:"";display:block;width:calc(100% - var(--pad));border-top:solid var(--accent2) 1px;margin-top:1.5rem;padding-top:.5rem;}nav.pdoc footer{font-size:small;}</style>
    <style>/*! content.css */.pdoc{color:var(--text);box-sizing:border-box;line-height:1.5;background:none;}.pdoc .pdoc-button{display:inline-block;border:solid black 1px;border-radius:2px;font-size:.75rem;padding:calc(0.5em - 1px) 1em;transition:100ms all;}.pdoc .visually-hidden{position:absolute !important;width:1px !important;height:1px !important;padding:0 !important;margin:-1px !important;overflow:hidden !important;clip:rect(0, 0, 0, 0) !important;white-space:nowrap !important;border:0 !important;}.pdoc h1, .pdoc h2, .pdoc h3{font-weight:300;margin:.3em 0;padding:.2em 0;}.pdoc a{text-decoration:none;color:var(--link);}.pdoc a:hover{color:var(--link-hover);}.pdoc blockquote{margin-left:2rem;}.pdoc pre{border-top:1px solid var(--accent2);border-bottom:1px solid var(--accent2);margin-top:0;margin-bottom:1em;padding:.5rem 0 .5rem .5rem;overflow-x:auto;background-color:var(--code);}.pdoc code{color:var(--text);padding:.2em .4em;margin:0;font-size:85%;background-color:var(--code);border-radius:6px;}.pdoc a > code{color:inherit;}.pdoc pre > code{display:inline-block;font-size:inherit;background:none;border:none;padding:0;}.pdoc .modulename{margin-top:0;font-weight:bold;}.pdoc .modulename a{color:var(--link);transition:100ms all;}.pdoc .git-button{float:right;border:solid var(--link) 1px;}.pdoc .git-button:hover{background-color:var(--link);color:var(--pdoc-background);}.pdoc details{filter:opacity(1);}.pdoc details:not([open]){height:0;}.pdoc details > summary{position:absolute;top:-35px;right:0;font-size:.75rem;color:var(--muted);padding:0 .7em;user-select:none;cursor:pointer;}.pdoc details > summary:focus{outline:0;}.pdoc .docstring{margin-bottom:1.5rem;}.pdoc > section:first-of-type > .docstring{margin-bottom:2.5rem;}.pdoc .docstring .pdoc-code{margin-left:1em;margin-right:1em;}.pdoc h1:target,.pdoc h2:target,.pdoc h3:target,.pdoc h4:target,.pdoc h5:target,.pdoc h6:target{background-color:var(--active);box-shadow:-1rem 0 0 0 var(--active);}.pdoc div:target > .attr,.pdoc section:target > .attr,.pdoc dd:target > a{background-color:var(--active);}.pdoc .attr:hover{filter:contrast(0.95);}.pdoc .headerlink{position:absolute;width:0;margin-left:-1.5rem;line-height:1.4rem;font-size:1.5rem;font-weight:normal;transition:all 100ms ease-in-out;opacity:0;user-select:none;}.pdoc .attr > .headerlink{margin-left:-2.5rem;}.pdoc *:hover > .headerlink,.pdoc *:target > .attr > .headerlink{opacity:1;}.pdoc .attr{display:block;color:var(--text);margin:.5rem 0 .5rem;padding:.4rem 5rem .4rem 1rem;background-color:var(--accent);}.pdoc .classattr{margin-left:2rem;}.pdoc .name{color:var(--name);font-weight:bold;}.pdoc .def{color:var(--def);font-weight:bold;}.pdoc .signature{white-space:pre-wrap;}.pdoc .annotation{color:var(--annotation);}.pdoc .inherited{margin-left:2rem;}.pdoc .inherited dt{font-weight:700;}.pdoc .inherited dt, .pdoc .inherited dd{display:inline;margin-left:0;margin-bottom:.5rem;}.pdoc .inherited dd:not(:last-child):after{content:", ";}.pdoc .inherited .class:before{content:"class ";}.pdoc .inherited .function a:after{content:"()";}.pdoc .search-result .docstring{overflow:auto;max-height:25vh;}.pdoc .search-result.focused > .attr{background-color:var(--active);}.pdoc .attribution{margin-top:2rem;display:block;opacity:0.5;transition:all 200ms;filter:grayscale(100%);}.pdoc .attribution:hover{opacity:1;filter:grayscale(0%);}.pdoc .attribution img{margin-left:5px;height:35px;vertical-align:middle;width:70px;transition:all 200ms;}.pdoc table{display:block;width:max-content;max-width:100%;overflow:auto;margin-bottom:1rem;}.pdoc table th{font-weight:600;}.pdoc table th, .pdoc table td{padding:6px 13px;border:1px solid var(--accent2);}</style>
    <style>/*! custom.css */</style></head>
<body>
    <nav class="pdoc">
        <label id="navtoggle" for="togglestate" class="pdoc-button"><svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 30 30'><path stroke-linecap='round' stroke="currentColor" stroke-miterlimit='10' stroke-width='2' d='M4 7h22M4 15h22M4 23h22'/></svg></label>
        <input id="togglestate" type="checkbox" aria-hidden="true" tabindex="-1">
        <div>            <a class="pdoc-button module-list-button" href="../classes.html">
<svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-box-arrow-in-left" viewBox="0 0 16 16">
  <path fill-rule="evenodd" d="M10 3.5a.5.5 0 0 0-.5-.5h-8a.5.5 0 0 0-.5.5v9a.5.5 0 0 0 .5.5h8a.5.5 0 0 0 .5-.5v-2a.5.5 0 0 1 1 0v2A1.5 1.5 0 0 1 9.5 14h-8A1.5 1.5 0 0 1 0 12.5v-9A1.5 1.5 0 0 1 1.5 2h8A1.5 1.5 0 0 1 11 3.5v2a.5.5 0 0 1-1 0v-2z"/>
  <path fill-rule="evenodd" d="M4.146 8.354a.5.5 0 0 1 0-.708l3-3a.5.5 0 1 1 .708.708L5.707 7.5H14.5a.5.5 0 0 1 0 1H5.707l2.147 2.146a.5.5 0 0 1-.708.708l-3-3z"/>
</svg>                &nbsp;mridc.core.classes</a>


            <input type="search" placeholder="Search..." role="searchbox" aria-label="search"
                   pattern=".+" required>



        <h2>API Documentation</h2>
            <ul class="memberlist">
            <li>
                    <a class="class" href="#ModelPT">ModelPT</a>
                            <ul class="memberlist">
                        <li>
                                <a class="function" href="#ModelPT.__init__">ModelPT</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.register_artifact">register_artifact</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.save_to">save_to</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.restore_from">restore_from</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.load_from_checkpoint">load_from_checkpoint</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_training_data">setup_training_data</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_validation_data">setup_validation_data</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_test_data">setup_test_data</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_multiple_validation_data">setup_multiple_validation_data</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_multiple_test_data">setup_multiple_test_data</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.setup_optimization">setup_optimization</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.configure_optimizers">configure_optimizers</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.train_dataloader">train_dataloader</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.val_dataloader">val_dataloader</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.test_dataloader">test_dataloader</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.validation_epoch_end">validation_epoch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.test_epoch_end">test_epoch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.multi_validation_epoch_end">multi_validation_epoch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.multi_test_epoch_end">multi_test_epoch_end</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.get_validation_dataloader_prefix">get_validation_dataloader_prefix</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.get_test_dataloader_prefix">get_test_dataloader_prefix</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.load_part_of_state_dict">load_part_of_state_dict</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.maybe_init_from_pretrained_checkpoint">maybe_init_from_pretrained_checkpoint</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.teardown">teardown</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.extract_state_dict_from">extract_state_dict_from</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.prepare_test">prepare_test</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.set_trainer">set_trainer</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.set_world_size">set_world_size</a>
                        </li>
                        <li>
                                <a class="variable" href="#ModelPT.num_weights">num_weights</a>
                        </li>
                        <li>
                                <a class="variable" href="#ModelPT.cfg">cfg</a>
                        </li>
                        <li>
                                <a class="function" href="#ModelPT.update_save_restore_connector">update_save_restore_connector</a>
                        </li>
                </ul>

            </li>
    </ul>



        <a class="attribution" title="pdoc: Python API documentation generator" href="https://pdoc.dev">
            built with <span class="visually-hidden">pdoc</span><img
                alt="pdoc logo"
                src="data:image/svg+xml,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20role%3D%22img%22%20aria-label%3D%22pdoc%20logo%22%20width%3D%22300%22%20height%3D%22150%22%20viewBox%3D%22-1%200%2060%2030%22%3E%3Ctitle%3Epdoc%3C/title%3E%3Cpath%20d%3D%22M29.621%2021.293c-.011-.273-.214-.475-.511-.481a.5.5%200%200%200-.489.503l-.044%201.393c-.097.551-.695%201.215-1.566%201.704-.577.428-1.306.486-2.193.182-1.426-.617-2.467-1.654-3.304-2.487l-.173-.172a3.43%203.43%200%200%200-.365-.306.49.49%200%200%200-.286-.196c-1.718-1.06-4.931-1.47-7.353.191l-.219.15c-1.707%201.187-3.413%202.131-4.328%201.03-.02-.027-.49-.685-.141-1.763.233-.721.546-2.408.772-4.076.042-.09.067-.187.046-.288.166-1.347.277-2.625.241-3.351%201.378-1.008%202.271-2.586%202.271-4.362%200-.976-.272-1.935-.788-2.774-.057-.094-.122-.18-.184-.268.033-.167.052-.339.052-.516%200-1.477-1.202-2.679-2.679-2.679-.791%200-1.496.352-1.987.9a6.3%206.3%200%200%200-1.001.029c-.492-.564-1.207-.929-2.012-.929-1.477%200-2.679%201.202-2.679%202.679A2.65%202.65%200%200%200%20.97%206.554c-.383.747-.595%201.572-.595%202.41%200%202.311%201.507%204.29%203.635%205.107-.037.699-.147%202.27-.423%203.294l-.137.461c-.622%202.042-2.515%208.257%201.727%2010.643%201.614.908%203.06%201.248%204.317%201.248%202.665%200%204.492-1.524%205.322-2.401%201.476-1.559%202.886-1.854%206.491.82%201.877%201.393%203.514%201.753%204.861%201.068%202.223-1.713%202.811-3.867%203.399-6.374.077-.846.056-1.469.054-1.537zm-4.835%204.313c-.054.305-.156.586-.242.629-.034-.007-.131-.022-.307-.157-.145-.111-.314-.478-.456-.908.221.121.432.25.675.355.115.039.219.051.33.081zm-2.251-1.238c-.05.33-.158.648-.252.694-.022.001-.125-.018-.307-.157-.217-.166-.488-.906-.639-1.573.358.344.754.693%201.198%201.036zm-3.887-2.337c-.006-.116-.018-.231-.041-.342.635.145%201.189.368%201.599.625.097.231.166.481.174.642-.03.049-.055.101-.067.158-.046.013-.128.026-.298.004-.278-.037-.901-.57-1.367-1.087zm-1.127-.497c.116.306.176.625.12.71-.019.014-.117.045-.345.016-.206-.027-.604-.332-.986-.695.41-.051.816-.056%201.211-.031zm-4.535%201.535c.209.22.379.47.358.598-.006.041-.088.138-.351.234-.144.055-.539-.063-.979-.259a11.66%2011.66%200%200%200%20.972-.573zm.983-.664c.359-.237.738-.418%201.126-.554.25.237.479.548.457.694-.006.042-.087.138-.351.235-.174.064-.694-.105-1.232-.375zm-3.381%201.794c-.022.145-.061.29-.149.401-.133.166-.358.248-.69.251h-.002c-.133%200-.306-.26-.45-.621.417.091.854.07%201.291-.031zm-2.066-8.077a4.78%204.78%200%200%201-.775-.584c.172-.115.505-.254.88-.378l-.105.962zm-.331%202.302a10.32%2010.32%200%200%201-.828-.502c.202-.143.576-.328.984-.49l-.156.992zm-.45%202.157l-.701-.403c.214-.115.536-.249.891-.376a11.57%2011.57%200%200%201-.19.779zm-.181%201.716c.064.398.194.702.298.893-.194-.051-.435-.162-.736-.398.061-.119.224-.3.438-.495zM8.87%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zm-.735-.389a1.15%201.15%200%200%200-.314.783%201.16%201.16%200%200%200%201.162%201.162c.457%200%20.842-.27%201.032-.653.026.117.042.238.042.362a1.68%201.68%200%200%201-1.679%201.679%201.68%201.68%200%200%201-1.679-1.679c0-.843.626-1.535%201.436-1.654zM5.059%205.406A1.68%201.68%200%200%201%203.38%207.085a1.68%201.68%200%200%201-1.679-1.679c0-.037.009-.072.011-.109.21.3.541.508.935.508a1.16%201.16%200%200%200%201.162-1.162%201.14%201.14%200%200%200-.474-.912c.015%200%20.03-.005.045-.005.926.001%201.679.754%201.679%201.68zM3.198%204.141c0%20.152-.123.276-.276.276s-.275-.124-.275-.276.123-.276.276-.276.275.124.275.276zM1.375%208.964c0-.52.103-1.035.288-1.52.466.394%201.06.64%201.717.64%201.144%200%202.116-.725%202.499-1.738.383%201.012%201.355%201.738%202.499%201.738.867%200%201.631-.421%202.121-1.062.307.605.478%201.267.478%201.942%200%202.486-2.153%204.51-4.801%204.51s-4.801-2.023-4.801-4.51zm24.342%2019.349c-.985.498-2.267.168-3.813-.979-3.073-2.281-5.453-3.199-7.813-.705-1.315%201.391-4.163%203.365-8.423.97-3.174-1.786-2.239-6.266-1.261-9.479l.146-.492c.276-1.02.395-2.457.444-3.268a6.11%206.11%200%200%200%201.18.115%206.01%206.01%200%200%200%202.536-.562l-.006.175c-.802.215-1.848.612-2.021%201.25-.079.295.021.601.274.837.219.203.415.364.598.501-.667.304-1.243.698-1.311%201.179-.02.144-.022.507.393.787.213.144.395.26.564.365-1.285.521-1.361.96-1.381%201.126-.018.142-.011.496.427.746l.854.489c-.473.389-.971.914-.999%201.429-.018.278.095.532.316.713.675.556%201.231.721%201.653.721.059%200%20.104-.014.158-.02.207.707.641%201.64%201.513%201.64h.013c.8-.008%201.236-.345%201.462-.626.173-.216.268-.457.325-.692.424.195.93.374%201.372.374.151%200%20.294-.021.423-.068.732-.27.944-.704.993-1.021.009-.061.003-.119.002-.179.266.086.538.147.789.147.15%200%20.294-.021.423-.069.542-.2.797-.489.914-.754.237.147.478.258.704.288.106.014.205.021.296.021.356%200%20.595-.101.767-.229.438.435%201.094.992%201.656%201.067.106.014.205.021.296.021a1.56%201.56%200%200%200%20.323-.035c.17.575.453%201.289.866%201.605.358.273.665.362.914.362a.99.99%200%200%200%20.421-.093%201.03%201.03%200%200%200%20.245-.164c.168.428.39.846.68%201.068.358.273.665.362.913.362a.99.99%200%200%200%20.421-.093c.317-.148.512-.448.639-.762.251.157.495.257.726.257.127%200%20.25-.024.37-.071.427-.17.706-.617.841-1.314.022-.015.047-.022.068-.038.067-.051.133-.104.196-.159-.443%201.486-1.107%202.761-2.086%203.257zM8.66%209.925a.5.5%200%201%200-1%200c0%20.653-.818%201.205-1.787%201.205s-1.787-.552-1.787-1.205a.5.5%200%201%200-1%200c0%201.216%201.25%202.205%202.787%202.205s2.787-.989%202.787-2.205zm4.4%2015.965l-.208.097c-2.661%201.258-4.708%201.436-6.086.527-1.542-1.017-1.88-3.19-1.844-4.198a.4.4%200%200%200-.385-.414c-.242-.029-.406.164-.414.385-.046%201.249.367%203.686%202.202%204.896.708.467%201.547.7%202.51.7%201.248%200%202.706-.392%204.362-1.174l.185-.086a.4.4%200%200%200%20.205-.527c-.089-.204-.326-.291-.527-.206zM9.547%202.292c.093.077.205.114.317.114a.5.5%200%200%200%20.318-.886L8.817.397a.5.5%200%200%200-.703.068.5.5%200%200%200%20.069.703l1.364%201.124zm-7.661-.065c.086%200%20.173-.022.253-.068l1.523-.893a.5.5%200%200%200-.506-.863l-1.523.892a.5.5%200%200%200-.179.685c.094.158.261.247.432.247z%22%20transform%3D%22matrix%28-1%200%200%201%2058%200%29%22%20fill%3D%22%233bb300%22/%3E%3Cpath%20d%3D%22M.3%2021.86V10.18q0-.46.02-.68.04-.22.18-.5.28-.54%201.34-.54%201.06%200%201.42.28.38.26.44.78.76-1.04%202.38-1.04%201.64%200%203.1%201.54%201.46%201.54%201.46%203.58%200%202.04-1.46%203.58-1.44%201.54-3.08%201.54-1.64%200-2.38-.92v4.04q0%20.46-.04.68-.02.22-.18.5-.14.3-.5.42-.36.12-.98.12-.62%200-1-.12-.36-.12-.52-.4-.14-.28-.18-.5-.02-.22-.02-.68zm3.96-9.42q-.46.54-.46%201.18%200%20.64.46%201.18.48.52%201.2.52.74%200%201.24-.52.52-.52.52-1.18%200-.66-.48-1.18-.48-.54-1.26-.54-.76%200-1.22.54zm14.741-8.36q.16-.3.54-.42.38-.12%201-.12.64%200%201.02.12.38.12.52.42.16.3.18.54.04.22.04.68v11.94q0%20.46-.04.7-.02.22-.18.5-.3.54-1.7.54-1.38%200-1.54-.98-.84.96-2.34.96-1.8%200-3.28-1.56-1.48-1.58-1.48-3.66%200-2.1%201.48-3.68%201.5-1.58%203.28-1.58%201.48%200%202.3%201v-4.2q0-.46.02-.68.04-.24.18-.52zm-3.24%2010.86q.52.54%201.26.54.74%200%201.22-.54.5-.54.5-1.18%200-.66-.48-1.22-.46-.56-1.26-.56-.8%200-1.28.56-.48.54-.48%201.2%200%20.66.52%201.2zm7.833-1.2q0-2.4%201.68-3.96%201.68-1.56%203.84-1.56%202.16%200%203.82%201.56%201.66%201.54%201.66%203.94%200%201.66-.86%202.96-.86%201.28-2.1%201.9-1.22.6-2.54.6-1.32%200-2.56-.64-1.24-.66-2.1-1.92-.84-1.28-.84-2.88zm4.18%201.44q.64.48%201.3.48.66%200%201.32-.5.66-.5.66-1.48%200-.98-.62-1.46-.62-.48-1.34-.48-.72%200-1.34.5-.62.5-.62%201.48%200%20.96.64%201.46zm11.412-1.44q0%20.84.56%201.32.56.46%201.18.46.64%200%201.18-.36.56-.38.9-.38.6%200%201.46%201.06.46.58.46%201.04%200%20.76-1.1%201.42-1.14.8-2.8.8-1.86%200-3.58-1.34-.82-.64-1.34-1.7-.52-1.08-.52-2.36%200-1.3.52-2.34.52-1.06%201.34-1.7%201.66-1.32%203.54-1.32.76%200%201.48.22.72.2%201.06.4l.32.2q.36.24.56.38.52.4.52.92%200%20.5-.42%201.14-.72%201.1-1.38%201.1-.38%200-1.08-.44-.36-.34-1.04-.34-.66%200-1.24.48-.58.48-.58%201.34z%22%20fill%3D%22green%22/%3E%3C/svg%3E"/>
        </a>
</div>
    </nav>
    <main class="pdoc">
            <section>
                    <h1 class="modulename">
<a href="./../../../mridc.html">mridc</a><wbr>.<a href="./../../core.html">core</a><wbr>.<a href="./../classes.html">classes</a><wbr>.modelPT    </h1>


                        <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="c1"># encoding: utf-8</span>
<span class="n">__author__</span> <span class="o">=</span> <span class="s2">&quot;Dimitrios Karkalousos&quot;</span>

<span class="c1"># Taken and adapted from: https://github.com/NVIDIA/NeMo/blob/main/nemo/core/classes/modelPT.py</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">inspect</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">path</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">hydra</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">omegaconf</span> <span class="kn">import</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">OmegaConf</span><span class="p">,</span> <span class="n">open_dict</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">LightningModule</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities</span> <span class="kn">import</span> <span class="n">rank_zero_only</span>

<span class="kn">from</span> <span class="nn">mridc.core.classes.common</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ModelPT&quot;</span><span class="p">]</span>

<span class="kn">import</span> <span class="nn">mridc.core.optim</span>
<span class="kn">from</span> <span class="nn">mridc.core.connectors.save_restore_connector</span> <span class="kn">import</span> <span class="n">SaveRestoreConnector</span>
<span class="kn">from</span> <span class="nn">mridc.utils</span> <span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">mridc.utils.app_state</span> <span class="kn">import</span> <span class="n">AppState</span>
<span class="kn">from</span> <span class="nn">mridc.utils.get_rank</span> <span class="kn">import</span> <span class="n">is_global_rank_zero</span>
<span class="kn">import</span> <span class="nn">mridc.utils</span>


<span class="k">class</span> <span class="nc">ModelPT</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">,</span> <span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Interface for Pytorch-lightning based mridc models&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Base class from which all mridc models should inherit</span>

<span class="sd">        Internal global flags that determine core functionality of ModelPT.</span>
<span class="sd">        _MODEL_IS_RESTORED:</span>
<span class="sd">            This flag determines the context of the model - whether the model is currently being</span>
<span class="sd">            restored or not.</span>
<span class="sd">            -   When set, it can be assumed that the model&#39;s will disable all automatic methods -</span>
<span class="sd">                setup_training_data(), setup_validation/test_data() and their multi equivalents.</span>
<span class="sd">            -   If a model is being restored from a archive file (tarfile), it can be assumed that</span>
<span class="sd">                under this context, the cwd is *inside* the tarfile itself.</span>
<span class="sd">        _MODEL_RESTORE_PATH:</span>
<span class="sd">            A string path to a a file from which the model is being restored.</span>
<span class="sd">            This file can either be a PyTorch Lightning Checkpoint, or a archive (tarfile) that contains</span>
<span class="sd">            artifact objects.</span>
<span class="sd">            If it is an archive file, during restoration, the cwd will be temporarily moved to inside the</span>
<span class="sd">            archive itself.</span>

<span class="sd">        Args:</span>
<span class="sd">            cfg (DictConfig):  configuration object.</span>
<span class="sd">                The cfg object should have (optionally) the following sub-configs:</span>
<span class="sd">                * train_ds - to instantiate training dataset</span>
<span class="sd">                * validation_ds - to instantiate validation dataset</span>
<span class="sd">                * test_ds - to instantiate testing dataset</span>
<span class="sd">                * optim - to instantiate optimizer with learning rate scheduler</span>
<span class="sd">            trainer (Optional): Pytorch Lightning Trainer instance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;trainer constructor argument must be either None or pytorch_lightning.Trainer. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;But got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># set global vars in AppState</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="c1"># Convert config to a DictConfig</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">convert_model_config_to_dict_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># Convert config to support Hydra 1.0+ instantiation</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">maybe_update_config_version</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Creating model config node is forbidden due to collision problem when loading from checkpoint.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;target&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="c1"># This is for Jarvis service.</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{0}</span><span class="s2">.</span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;cfg&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>  <span class="c1"># reference required for self.*_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>  <span class="c1"># alias for backward compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_guid</span><span class="p">()</span>

        <span class="c1"># Set device_id in AppState</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">app_state</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_model_being_restored</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_test_data</span><span class="p">(</span><span class="n">test_data_config</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;method and provide a valid configuration file to setup the train data loader.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Train config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do validation, please call the ModelPT.setup_validation_data() or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;setup the validation data loader(s). </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Validation config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;and provide a valid configuration file to setup the test data loader(s).</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Test config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>

        <span class="c1"># ModelPT wrappers over subclass implementations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_step</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">wrap_training_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;This method is called when a subclass is created.&quot;&quot;&quot;</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Register model artifacts with this function. These artifacts (files) will be included inside .mridc file</span>
<span class="sd">        when model.save_to(&quot;model.mridc&quot;) is called.</span>
<span class="sd">        How it works:</span>
<span class="sd">        1. It always returns existing absolute path which can be used during Model constructor call</span>
<span class="sd">            EXCEPTION: src is None or &quot;&quot; in which case nothing will be done and src will be returned</span>
<span class="sd">        2. It will add (config_path, model_utils.ArtifactItem()) pair to self.artifacts</span>
<span class="sd">        If &quot;src&quot; is local existing path, then it will be returned in absolute path form.</span>
<span class="sd">        elif &quot;src&quot; starts with &quot;mridc_file:unique_artifact_name&quot;:</span>
<span class="sd">            .mridc will be untarred to a temporary folder location and an actual existing path will be returned</span>
<span class="sd">        else an error will be raised.</span>
<span class="sd">        WARNING: use .register_artifact calls in your models&#39; constructors.</span>
<span class="sd">        The returned path is not guaranteed to exist after you have exited your model&#39;s constructor.</span>
<span class="sd">        Args:</span>
<span class="sd">            config_path (str): Artifact key. Usually corresponds to the model config.</span>
<span class="sd">            src (str): Path to artifact.</span>
<span class="sd">            verify_src_exists (bool): If set to False, then the artifact is optional and register_artifact will</span>
<span class="sd">                                        return None even if src is not found. Defaults to True.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str: If src is not None or empty it always returns absolute path which is guaranteed to exists during</span>
<span class="sd">                model instance life</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">src</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">src</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;artifacts&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">ArtifactItem</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">config_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You tried to register an artifact under config key=</span><span class="si">{</span><span class="n">config_path</span><span class="si">}</span><span class="s2"> but an artifact for &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it has already been registered.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves model instance (weights and configuration) into .mridc file</span>
<span class="sd">         You can use &quot;restore_from&quot; method to fully restore instance from .mridc file.</span>
<span class="sd">        .mridc file is an archive (tar.gz) with the following:</span>
<span class="sd">            model_config.yaml - model configuration in .yaml format. You can deserialize this into cfg argument for</span>
<span class="sd">            model&#39;s constructor</span>
<span class="sd">            model_wights.ckpt - model checkpoint</span>
<span class="sd">        Args:</span>
<span class="sd">            save_path: Path to .mridc file where model instance should be saved</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">maybe_make_save_dir</span><span class="p">(</span><span class="n">_path</span><span class="p">:</span> <span class="s2">&quot;Path&quot;</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Creates directory if it does not exist&quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
                <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="p">)</span> <span class="ow">is</span> <span class="n">SaveRestoreConnector</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Default mridc SaveRestoreConnector will not work in model parallel mode. You should use a &quot;</span>
                    <span class="s2">&quot;connector which supports model parallel mode. You can also use a custom one.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">data_parallel_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="c1"># connector checks for ranks properly, no need to check here</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>
        <span class="k">elif</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
            <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore_from</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">override_config_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">OmegaConf</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Trainer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restores model instance (weights and configuration) from .mridc file.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which model should be instantiated</span>
<span class="sd">            override_config_path: path to a yaml config that will override the internal</span>
<span class="sd">                config file or an OmegaConf / DictConfig object representing the model config.</span>
<span class="sd">            map_location: Optional torch.device() to map the instantiated model to a device.</span>
<span class="sd">                By default (None), it will select a GPU if available, falling back to CPU otherwise.</span>
<span class="sd">            strict: Passed to load_state_dict. By default True.</span>
<span class="sd">            return_config: If set to true, will return just the underlying config of the restored</span>
<span class="sd">                model as an OmegaConf DictConfig object without instantiating the model.</span>
<span class="sd">            trainer: Optional, a pytorch lightning Trainer object that will be forwarded to the</span>
<span class="sd">                instantiated model&#39;s constructor.</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">            Example:</span>
<span class="sd">                ```</span>
<span class="sd">                model = mridc.collections.asr.models.EncDecCTCModel.restore_from(&#39;asr.mridc&#39;)</span>
<span class="sd">                assert isinstance(model, mridc.collections.asr.models.EncDecCTCModel)</span>
<span class="sd">                ```</span>
<span class="sd">        Returns:</span>
<span class="sd">            An instance of type cls or its underlying config (if return_config is set).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="n">restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">restore_path</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">restore_path</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">,</span> <span class="n">restore_path</span><span class="p">,</span> <span class="n">override_config_path</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">return_config</span><span class="p">,</span> <span class="n">trainer</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">ModelPT</span><span class="p">):</span>
            <span class="n">instance</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">return</span> <span class="n">instance</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_from_checkpoint</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hparams_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads ModelPT from checkpoint, with some maintenance of restoration.</span>
<span class="sd">        For documentation, please refer to LightningModule.load_from_checkpoint() documentation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
                <span class="n">checkpoint_path</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span>
                <span class="n">hparams_file</span><span class="o">=</span><span class="n">hparams_file</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">checkpoint</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in training</span>
<span class="sd">        Args:</span>
<span class="sd">            train_data_config: training data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in validation</span>
<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">setup_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config: test data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in validation</span>

<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">val_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_validation_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;val_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">))]</span>

    <span class="k">def</span> <span class="nf">setup_multiple_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test, with support for multiple data loaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config ():  test data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">test_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_test_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">))]</span>

    <span class="k">def</span> <span class="nf">setup_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optim_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares an optimizer from a string name and its optional config parameters.</span>
<span class="sd">        Args:</span>
<span class="sd">            optim_config: A dictionary containing the following keys:</span>
<span class="sd">                * &quot;lr&quot;: mandatory key for learning rate. Will raise ValueError if not provided.</span>
<span class="sd">                * &quot;optimizer&quot;: string name pointing to one of the available optimizers in the registry. \</span>
<span class="sd">                If not provided, defaults to &quot;adam&quot;.</span>
<span class="sd">                * &quot;opt_args&quot;: Optional list of strings, in the format &quot;arg_name=arg_value&quot;. \</span>
<span class="sd">                The list of &quot;arg_value&quot; will be parsed and a dictionary of optimizer kwargs \</span>
<span class="sd">                will be built and supplied to instantiate the optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If config was not explicitly passed to us</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span>

        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No optimizer config provided, therefore no optimizer was created&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Preserve the configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># See if internal config has `optim` namespace before preservation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># Setup optimizer and scheduler</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Trainer wasn&#39;t specified in model constructor. Make sure that you really wanted it.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;We do not currently support gradient accumulation that is not an integer.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Store information needed to calculate max_steps</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_max_epochs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_epochs</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_accumulate_grad_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_limit_train_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">limit_train_batches</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">or</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp_cpu&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The lightning trainer received accelerator: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="si">}</span><span class="s2">. We &quot;</span>
                        <span class="s2">&quot;recommend to use &#39;ddp&#39; instead.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;max_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span>

        <span class="c1"># Force into DictConfig from nested structure</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
        <span class="c1"># Get back nested dict so we its mutable</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Extract scheduler config if inside optimizer config</span>
        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sched&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Check if caller provided optimizer name, default to Adam otherwise</span>
        <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_target_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Try to get optimizer name for dynamic resolution, defaulting to Adam</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;adam&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># resolve the class name (lowercase) from the class path if not provided</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="c1"># We are guaranteed to have lr since it is required by the argparser</span>
        <span class="c1"># But maybe user forgot to pass it to this function</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Check if caller has optimizer kwargs, default to empty dictionary</span>
        <span class="k">if</span> <span class="s2">&quot;args&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;args&quot;</span><span class="p">)</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">parse_optimizer_args</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

            <span class="c1"># Remove extra parameters from optimizer_args nest</span>
            <span class="c1"># Assume all other parameters are to be passed into optimizer constructor</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Adaptive schedulers don&#39;t need `lr`</span>
        <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer_args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Actually instantiate the optimizer</span>
        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">get_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Attempt class path resolution</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="s2">&quot;_target_&quot;</span><span class="p">:</span> <span class="n">optimizer_cls</span><span class="p">})</span>
                <span class="n">optimizer_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">}</span> <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="n">optimizer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">optimizer_args</span><span class="p">)</span>

                <span class="n">optimizer_instance</span> <span class="o">=</span> <span class="n">hydra</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">instantiate</span><span class="p">(</span>
                    <span class="n">optimizer_cls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_config</span>
                <span class="p">)</span>  <span class="c1"># type: DictConfig</span>

                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_instance</span><span class="p">))</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_instance</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="s2">&quot;Could not instantiate class path - </span><span class="si">{}</span><span class="s2"> with kwargs </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_config</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

        <span class="c1"># Try to instantiate scheduler for optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">prepare_lr_scheduler</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">train_dataloader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Return the optimizer with/without scheduler</span>
        <span class="c1"># This return allows multiple optimizers or schedulers to be created</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Configure optimizers and schedulers for training.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_optimization</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>

        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the training dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the validation dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the test dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Validation set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_validation_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_validation_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `val_loss`,</span>
<span class="sd">            only the `val_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `val_dl_idx: int`</span>
<span class="sd">            inside the `validation_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">val_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_validation_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">val_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `val_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;val_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span> <span class="ow">and</span> <span class="s2">&quot;val_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span>
            <span class="p">):</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>

                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the metric, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="c1"># Store log value</span>
                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>

    <span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Test set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_test_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_test_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `test_loss`,</span>
<span class="sd">            only the `test_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `_test_dl_idx: int`</span>
<span class="sd">            inside the `test_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">test_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `test_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;test_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span>
                <span class="ow">and</span> <span class="s2">&quot;test_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span>  <span class="c1"># type: ignore</span>
                <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span>
            <span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the loss, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="p">{})</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_validation_epoch_end</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple validation datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_validation_epoch_end(outputs, dataloader_idx) &quot;</span>
            <span class="s2">&quot;has not been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override `validation_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple test datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_test_epoch_end(outputs, dataloader_idx) has not &quot;</span>
            <span class="s2">&quot;been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override test_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_validation_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">get_test_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">load_part_of_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="n">load_from_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Load part of the state dict.&quot;&quot;&quot;</span>
        <span class="n">excluded_param_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># create dict</span>
        <span class="n">dict_to_load</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">should_add</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">include</span><span class="p">)</span>
            <span class="c1"># except for if any string from exclude is present</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">exclude</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                    <span class="n">excluded_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                    <span class="n">should_add</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">should_add</span><span class="p">:</span>
                <span class="n">dict_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c1"># Restore checkpoint part into current model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">dict_to_load</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint partially restored from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">excluded_param_names</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following parameters were excluded from loading from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2"> : </span><span class="si">{</span><span class="n">excluded_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Make sure that this is what you wanted!&quot;</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span> <span class="nf">maybe_init_from_pretrained_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">OmegaConf</span><span class="p">,</span> <span class="n">map_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a given model with the parameters obtained via specific config arguments.</span>
<span class="sd">        The state dict of the provided model will be updated with `strict=False` setting so as to prevent</span>
<span class="sd">        requirement of exact model parameters matching.</span>
<span class="sd">        Initializations:</span>
<span class="sd">            init_from_mridc_model: Str path to a .mridc model, which will be instantiated in order</span>
<span class="sd">                to extract the state dict.</span>
<span class="sd">            init_from_pretrained_model: Str name of a pretrained model checkpoint (obtained via cloud).</span>
<span class="sd">                The model will be downloaded (or a cached copy will be used), instantiated and then</span>
<span class="sd">                its state dict will be extracted.</span>
<span class="sd">            init_from_ptl_ckpt: Str name of a Pytorch Lightning checkpoint file. It will be loaded and</span>
<span class="sd">                the state dict will extracted.</span>
<span class="sd">        Args:</span>
<span class="sd">            cfg: The config used to instantiate the model. It need only contain one of the above keys.</span>
<span class="sd">            map_location: str or torch.device() which represents where the intermediate state dict</span>
<span class="sd">                (from the pretrained model or checkpoint) will be loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;init_from_mridc_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">]</span>
        <span class="n">arg_matches</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># model weights do not need to be restored</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot pass more than one model initialization arguments to config!</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Found : </span><span class="si">{</span><span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">arg_present</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="k">if</span> <span class="n">arg_present</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_mridc_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore model</span>
                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                        <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>
                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from mridc file with path : `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                            <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;mridc file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_mridc_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Restore model</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_name</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

                    <span class="c1"># Check if model is being resumed or not - only works if `Trainer` is attached to model</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;trainer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="nb">hasattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;resume_from_checkpoint&quot;</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_checkpoint_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="p">):</span>
                            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="s2">&quot;Model training is being resumed via Pytorch Lightning.</span><span class="se">\n</span><span class="s2">&quot;</span>
                                <span class="s2">&quot;Initialization from pretrained model (via cloud) will be skipped.&quot;</span>
                            <span class="p">)</span>
                            <span class="k">return</span>

                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                        <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pretrained checkpoint with name : `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">name</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                            <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                            <span class="n">include</span><span class="p">,</span>
                            <span class="n">exclude</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;pretrained checkpoint with name `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_pretrained_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore checkpoint</span>
                    <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pytorch lightning checkpoint with path : `</span><span class="si">{</span><span class="n">ckpt_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;nemo file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_ptl_ckpt is not a string or a dict!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Called at the end of fit and test.</span>
<span class="sd">        Args:</span>
<span class="sd">            stage: either &#39;fit&#39; or &#39;test&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;fit&quot;</span> <span class="ow">and</span> <span class="s2">&quot;PL_TRAINER_GPUS&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;PL_TRAINER_GPUS&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">teardown</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">extract_state_dict_from</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">split_by_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract the state dict(s) from a provided .mridc tarfile and save it to a directory.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which state dict(s) should be extracted</span>
<span class="sd">            save_dir: directory in which the saved state dict(s) should be stored</span>
<span class="sd">            split_by_module: bool flag, which determines whether the output checkpoint should</span>
<span class="sd">                be for the entire Model, or the individual module&#39;s that comprise the Model</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">        Example:</span>
<span class="sd">            To convert the .mridc tarfile into a single Model level PyTorch checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;)</span>
<span class="sd">            To restore a model from a Model level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            model.load_state_dict(torch.load(&quot;./asr_ckpts/model_weights.ckpt&quot;))</span>
<span class="sd">            To convert the .mridc tarfile into multiple Module level PyTorch checkpoints</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;, split_by_module=True)</span>
<span class="sd">            To restore a module from a Module level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            # load the individual components</span>
<span class="sd">            model.preprocessor.load_state_dict(torch.load(&quot;./asr_ckpts/preprocessor.ckpt&quot;))</span>
<span class="sd">            model.encoder.load_state_dict(torch.load(&quot;./asr_ckpts/encoder.ckpt&quot;))</span>
<span class="sd">            model.decoder.load_state_dict(torch.load(&quot;./asr_ckpts/decoder.ckpt&quot;))</span>
<span class="sd">        Returns:</span>
<span class="sd">            The state dict that was loaded from the original .mridc checkpoint</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileExistsError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">extract_state_dict_from</span><span class="p">(</span><span class="n">restore_path</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">split_by_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;Trainer&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper method to check whether the model can safely be tested</span>
<span class="sd">        on a dataset after training (or loading a checkpoint).</span>
<span class="sd">        ::</span>
<span class="sd">            trainer = Trainer()</span>
<span class="sd">            if model.prepare_test(trainer):</span>
<span class="sd">                trainer.test(model)</span>
<span class="sd">        Returns:</span>
<span class="sd">            bool which declares the model safe to test. Provides warnings if it has to</span>
<span class="sd">            return False to guide the user.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;test_ds&quot;</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No `test_ds` config found within the manifest.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Replace ddp multi-gpu until PTL has a fix</span>
            <span class="n">DDP_WARN</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\n\n</span><span class="s2">During testing, it is currently advisable to construct a new Trainer &quot;</span>
<span class="s2">                    &quot;with single GPU and no DDP to obtain accurate results.</span>
<span class="s2">                    &quot;Following pattern should be used: &quot;</span>
<span class="s2">                    &quot;gpu = 1 if cfg.trainer.gpus != 0 else 0&quot;</span>
<span class="s2">                    &quot;trainer = Trainer(gpus=gpu)&quot;</span>
<span class="s2">                    &quot;if model.prepare_test(trainer):&quot;</span>
<span class="s2">                    &quot;  trainer.test(model)</span><span class="se">\n\n</span><span class="s2">&quot;&quot;&quot;</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">DDP_WARN</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Assign trainer to the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">set_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set an instance of Trainer object.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer: PyTorch Lightning Trainer object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determines the world size from the PyTorch Lightning Trainer.</span>
<span class="sd">        And then updates AppState.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer (Trainer): PyTorch Lightning Trainer object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update AppState with world information from trainer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">app_state</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;World size can only be set by PyTorch Lightning Trainer.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_dataset_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update the config (if not None) of the dataset by given name.</span>
<span class="sd">        Preserves said config after updating.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataset_name: str name of the dataset whose config is being updated.</span>
<span class="sd">                Can be one of `train`, `validation` and `test`.</span>
<span class="sd">            config: Optional DictConfig or dict. If None is passed, this method simply returns.</span>
<span class="sd">                If dict is passed, it is cast into a DictConfig.</span>
<span class="sd">                The internal config is updated with the passed config.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_multi_dataset_mode&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">}:</span>
                <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

                <span class="n">key_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">_ds&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>

                <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

                <span class="c1"># Update hyper parameters by calling property setter</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`dataset_name` when updating config must be one of [train, validation, test]&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Utility property that returns the total number of parameters of the Model.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cfg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property that holds the finalized internal config of the model.</span>
<span class="sd">        Note:</span>
<span class="sd">            Changes to this config are not reflected in the state of the model.</span>
<span class="sd">            Please create a new model using an updated config to properly update the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span>

    <span class="nd">@cfg</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cfg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property that holds the finalized internal config of the model.</span>
<span class="sd">        Note:</span>
<span class="sd">            Changes to this config are not reflected in the state of the model.</span>
<span class="sd">            Please create a new model using an updated config to properly update the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_hparams</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="s2">&quot;cfg&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">}))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_is_model_being_restored</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Checks if the model is being restored from a checkpoint.&quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">app_state</span><span class="o">.</span><span class="n">is_model_being_restored</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the state of the model to be restored.&quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">is_model_being_restored</span> <span class="o">=</span> <span class="n">is_being_restored</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">mridc_file_folder</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_model_guid</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the model guid.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;model_guid&quot;</span><span class="p">):</span>
            <span class="n">appstate</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

            <span class="c1"># Generate a unique uuid for the instance</span>
            <span class="c1"># also determine if the model is being restored or not, and preserve the path</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_guid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_model_being_restored</span><span class="p">():</span>
                <span class="n">restore_path</span> <span class="o">=</span> <span class="n">appstate</span><span class="o">.</span><span class="n">model_restore_path</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">restore_path</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">appstate</span><span class="o">.</span><span class="n">register_model_guid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_guid</span><span class="p">,</span> <span class="n">restoration_path</span><span class="o">=</span><span class="n">restore_path</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">update_save_restore_connector</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the save_restore_connector of the model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">)</span>
</pre></div>

        </details>

            </section>
                <section id="ModelPT">
                                <div class="attr class">
        <a class="headerlink" href="#ModelPT">#&nbsp;&nbsp</a>


        <span class="def">class</span>
        <span class="name">ModelPT</span><wbr>(<span class="base">pytorch_lightning.core.lightning.LightningModule</span>, <span class="base"><a href="common.html#Model">mridc.core.classes.common.Model</a></span>):
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span><span class="k">class</span> <span class="nc">ModelPT</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">,</span> <span class="n">Model</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Interface for Pytorch-lightning based mridc models&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Base class from which all mridc models should inherit</span>

<span class="sd">        Internal global flags that determine core functionality of ModelPT.</span>
<span class="sd">        _MODEL_IS_RESTORED:</span>
<span class="sd">            This flag determines the context of the model - whether the model is currently being</span>
<span class="sd">            restored or not.</span>
<span class="sd">            -   When set, it can be assumed that the model&#39;s will disable all automatic methods -</span>
<span class="sd">                setup_training_data(), setup_validation/test_data() and their multi equivalents.</span>
<span class="sd">            -   If a model is being restored from a archive file (tarfile), it can be assumed that</span>
<span class="sd">                under this context, the cwd is *inside* the tarfile itself.</span>
<span class="sd">        _MODEL_RESTORE_PATH:</span>
<span class="sd">            A string path to a a file from which the model is being restored.</span>
<span class="sd">            This file can either be a PyTorch Lightning Checkpoint, or a archive (tarfile) that contains</span>
<span class="sd">            artifact objects.</span>
<span class="sd">            If it is an archive file, during restoration, the cwd will be temporarily moved to inside the</span>
<span class="sd">            archive itself.</span>

<span class="sd">        Args:</span>
<span class="sd">            cfg (DictConfig):  configuration object.</span>
<span class="sd">                The cfg object should have (optionally) the following sub-configs:</span>
<span class="sd">                * train_ds - to instantiate training dataset</span>
<span class="sd">                * validation_ds - to instantiate validation dataset</span>
<span class="sd">                * test_ds - to instantiate testing dataset</span>
<span class="sd">                * optim - to instantiate optimizer with learning rate scheduler</span>
<span class="sd">            trainer (Optional): Pytorch Lightning Trainer instance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;trainer constructor argument must be either None or pytorch_lightning.Trainer. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;But got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># set global vars in AppState</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="c1"># Convert config to a DictConfig</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">convert_model_config_to_dict_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># Convert config to support Hydra 1.0+ instantiation</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">maybe_update_config_version</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Creating model config node is forbidden due to collision problem when loading from checkpoint.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;target&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="c1"># This is for Jarvis service.</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{0}</span><span class="s2">.</span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;cfg&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>  <span class="c1"># reference required for self.*_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>  <span class="c1"># alias for backward compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_guid</span><span class="p">()</span>

        <span class="c1"># Set device_id in AppState</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">app_state</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_model_being_restored</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_test_data</span><span class="p">(</span><span class="n">test_data_config</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;method and provide a valid configuration file to setup the train data loader.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Train config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do validation, please call the ModelPT.setup_validation_data() or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;setup the validation data loader(s). </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Validation config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;and provide a valid configuration file to setup the test data loader(s).</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Test config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>

        <span class="c1"># ModelPT wrappers over subclass implementations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_step</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">wrap_training_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">__init_subclass__</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;This method is called when a subclass is created.&quot;&quot;&quot;</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Register model artifacts with this function. These artifacts (files) will be included inside .mridc file</span>
<span class="sd">        when model.save_to(&quot;model.mridc&quot;) is called.</span>
<span class="sd">        How it works:</span>
<span class="sd">        1. It always returns existing absolute path which can be used during Model constructor call</span>
<span class="sd">            EXCEPTION: src is None or &quot;&quot; in which case nothing will be done and src will be returned</span>
<span class="sd">        2. It will add (config_path, model_utils.ArtifactItem()) pair to self.artifacts</span>
<span class="sd">        If &quot;src&quot; is local existing path, then it will be returned in absolute path form.</span>
<span class="sd">        elif &quot;src&quot; starts with &quot;mridc_file:unique_artifact_name&quot;:</span>
<span class="sd">            .mridc will be untarred to a temporary folder location and an actual existing path will be returned</span>
<span class="sd">        else an error will be raised.</span>
<span class="sd">        WARNING: use .register_artifact calls in your models&#39; constructors.</span>
<span class="sd">        The returned path is not guaranteed to exist after you have exited your model&#39;s constructor.</span>
<span class="sd">        Args:</span>
<span class="sd">            config_path (str): Artifact key. Usually corresponds to the model config.</span>
<span class="sd">            src (str): Path to artifact.</span>
<span class="sd">            verify_src_exists (bool): If set to False, then the artifact is optional and register_artifact will</span>
<span class="sd">                                        return None even if src is not found. Defaults to True.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str: If src is not None or empty it always returns absolute path which is guaranteed to exists during</span>
<span class="sd">                model instance life</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">src</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">src</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;artifacts&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">ArtifactItem</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">config_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You tried to register an artifact under config key=</span><span class="si">{</span><span class="n">config_path</span><span class="si">}</span><span class="s2"> but an artifact for &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it has already been registered.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves model instance (weights and configuration) into .mridc file</span>
<span class="sd">         You can use &quot;restore_from&quot; method to fully restore instance from .mridc file.</span>
<span class="sd">        .mridc file is an archive (tar.gz) with the following:</span>
<span class="sd">            model_config.yaml - model configuration in .yaml format. You can deserialize this into cfg argument for</span>
<span class="sd">            model&#39;s constructor</span>
<span class="sd">            model_wights.ckpt - model checkpoint</span>
<span class="sd">        Args:</span>
<span class="sd">            save_path: Path to .mridc file where model instance should be saved</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">maybe_make_save_dir</span><span class="p">(</span><span class="n">_path</span><span class="p">:</span> <span class="s2">&quot;Path&quot;</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Creates directory if it does not exist&quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
                <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="p">)</span> <span class="ow">is</span> <span class="n">SaveRestoreConnector</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Default mridc SaveRestoreConnector will not work in model parallel mode. You should use a &quot;</span>
                    <span class="s2">&quot;connector which supports model parallel mode. You can also use a custom one.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">data_parallel_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="c1"># connector checks for ranks properly, no need to check here</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>
        <span class="k">elif</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
            <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore_from</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">override_config_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">OmegaConf</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Trainer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restores model instance (weights and configuration) from .mridc file.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which model should be instantiated</span>
<span class="sd">            override_config_path: path to a yaml config that will override the internal</span>
<span class="sd">                config file or an OmegaConf / DictConfig object representing the model config.</span>
<span class="sd">            map_location: Optional torch.device() to map the instantiated model to a device.</span>
<span class="sd">                By default (None), it will select a GPU if available, falling back to CPU otherwise.</span>
<span class="sd">            strict: Passed to load_state_dict. By default True.</span>
<span class="sd">            return_config: If set to true, will return just the underlying config of the restored</span>
<span class="sd">                model as an OmegaConf DictConfig object without instantiating the model.</span>
<span class="sd">            trainer: Optional, a pytorch lightning Trainer object that will be forwarded to the</span>
<span class="sd">                instantiated model&#39;s constructor.</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">            Example:</span>
<span class="sd">                ```</span>
<span class="sd">                model = mridc.collections.asr.models.EncDecCTCModel.restore_from(&#39;asr.mridc&#39;)</span>
<span class="sd">                assert isinstance(model, mridc.collections.asr.models.EncDecCTCModel)</span>
<span class="sd">                ```</span>
<span class="sd">        Returns:</span>
<span class="sd">            An instance of type cls or its underlying config (if return_config is set).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="n">restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">restore_path</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">restore_path</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">,</span> <span class="n">restore_path</span><span class="p">,</span> <span class="n">override_config_path</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">return_config</span><span class="p">,</span> <span class="n">trainer</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">ModelPT</span><span class="p">):</span>
            <span class="n">instance</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">return</span> <span class="n">instance</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_from_checkpoint</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hparams_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads ModelPT from checkpoint, with some maintenance of restoration.</span>
<span class="sd">        For documentation, please refer to LightningModule.load_from_checkpoint() documentation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
                <span class="n">checkpoint_path</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span>
                <span class="n">hparams_file</span><span class="o">=</span><span class="n">hparams_file</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">checkpoint</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in training</span>
<span class="sd">        Args:</span>
<span class="sd">            train_data_config: training data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in validation</span>
<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">setup_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config: test data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in validation</span>

<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">val_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_validation_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;val_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">))]</span>

    <span class="k">def</span> <span class="nf">setup_multiple_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test, with support for multiple data loaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config ():  test data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">test_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_test_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">))]</span>

    <span class="k">def</span> <span class="nf">setup_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optim_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares an optimizer from a string name and its optional config parameters.</span>
<span class="sd">        Args:</span>
<span class="sd">            optim_config: A dictionary containing the following keys:</span>
<span class="sd">                * &quot;lr&quot;: mandatory key for learning rate. Will raise ValueError if not provided.</span>
<span class="sd">                * &quot;optimizer&quot;: string name pointing to one of the available optimizers in the registry. \</span>
<span class="sd">                If not provided, defaults to &quot;adam&quot;.</span>
<span class="sd">                * &quot;opt_args&quot;: Optional list of strings, in the format &quot;arg_name=arg_value&quot;. \</span>
<span class="sd">                The list of &quot;arg_value&quot; will be parsed and a dictionary of optimizer kwargs \</span>
<span class="sd">                will be built and supplied to instantiate the optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If config was not explicitly passed to us</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span>

        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No optimizer config provided, therefore no optimizer was created&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Preserve the configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># See if internal config has `optim` namespace before preservation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># Setup optimizer and scheduler</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Trainer wasn&#39;t specified in model constructor. Make sure that you really wanted it.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;We do not currently support gradient accumulation that is not an integer.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Store information needed to calculate max_steps</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_max_epochs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_epochs</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_accumulate_grad_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_limit_train_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">limit_train_batches</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">or</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp_cpu&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The lightning trainer received accelerator: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="si">}</span><span class="s2">. We &quot;</span>
                        <span class="s2">&quot;recommend to use &#39;ddp&#39; instead.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;max_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span>

        <span class="c1"># Force into DictConfig from nested structure</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
        <span class="c1"># Get back nested dict so we its mutable</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Extract scheduler config if inside optimizer config</span>
        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sched&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Check if caller provided optimizer name, default to Adam otherwise</span>
        <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_target_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Try to get optimizer name for dynamic resolution, defaulting to Adam</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;adam&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># resolve the class name (lowercase) from the class path if not provided</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="c1"># We are guaranteed to have lr since it is required by the argparser</span>
        <span class="c1"># But maybe user forgot to pass it to this function</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Check if caller has optimizer kwargs, default to empty dictionary</span>
        <span class="k">if</span> <span class="s2">&quot;args&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;args&quot;</span><span class="p">)</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">parse_optimizer_args</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

            <span class="c1"># Remove extra parameters from optimizer_args nest</span>
            <span class="c1"># Assume all other parameters are to be passed into optimizer constructor</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Adaptive schedulers don&#39;t need `lr`</span>
        <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer_args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Actually instantiate the optimizer</span>
        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">get_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Attempt class path resolution</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="s2">&quot;_target_&quot;</span><span class="p">:</span> <span class="n">optimizer_cls</span><span class="p">})</span>
                <span class="n">optimizer_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">}</span> <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="n">optimizer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">optimizer_args</span><span class="p">)</span>

                <span class="n">optimizer_instance</span> <span class="o">=</span> <span class="n">hydra</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">instantiate</span><span class="p">(</span>
                    <span class="n">optimizer_cls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_config</span>
                <span class="p">)</span>  <span class="c1"># type: DictConfig</span>

                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_instance</span><span class="p">))</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_instance</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="s2">&quot;Could not instantiate class path - </span><span class="si">{}</span><span class="s2"> with kwargs </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_config</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

        <span class="c1"># Try to instantiate scheduler for optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">prepare_lr_scheduler</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">train_dataloader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Return the optimizer with/without scheduler</span>
        <span class="c1"># This return allows multiple optimizers or schedulers to be created</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Configure optimizers and schedulers for training.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_optimization</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>

        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the training dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the validation dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the test dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Validation set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_validation_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_validation_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `val_loss`,</span>
<span class="sd">            only the `val_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `val_dl_idx: int`</span>
<span class="sd">            inside the `validation_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">val_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_validation_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">val_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `val_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;val_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span> <span class="ow">and</span> <span class="s2">&quot;val_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span>
            <span class="p">):</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>

                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the metric, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="c1"># Store log value</span>
                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>

    <span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Test set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_test_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_test_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `test_loss`,</span>
<span class="sd">            only the `test_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `_test_dl_idx: int`</span>
<span class="sd">            inside the `test_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">test_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `test_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;test_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span>
                <span class="ow">and</span> <span class="s2">&quot;test_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span>  <span class="c1"># type: ignore</span>
                <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span>
            <span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the loss, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="p">{})</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_validation_epoch_end</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple validation datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_validation_epoch_end(outputs, dataloader_idx) &quot;</span>
            <span class="s2">&quot;has not been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override `validation_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple test datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_test_epoch_end(outputs, dataloader_idx) has not &quot;</span>
            <span class="s2">&quot;been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override test_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_validation_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">get_test_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">load_part_of_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="n">load_from_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Load part of the state dict.&quot;&quot;&quot;</span>
        <span class="n">excluded_param_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># create dict</span>
        <span class="n">dict_to_load</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">should_add</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">include</span><span class="p">)</span>
            <span class="c1"># except for if any string from exclude is present</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">exclude</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                    <span class="n">excluded_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                    <span class="n">should_add</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">should_add</span><span class="p">:</span>
                <span class="n">dict_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c1"># Restore checkpoint part into current model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">dict_to_load</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint partially restored from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">excluded_param_names</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following parameters were excluded from loading from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2"> : </span><span class="si">{</span><span class="n">excluded_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Make sure that this is what you wanted!&quot;</span><span class="p">)</span>

    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span> <span class="nf">maybe_init_from_pretrained_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">OmegaConf</span><span class="p">,</span> <span class="n">map_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a given model with the parameters obtained via specific config arguments.</span>
<span class="sd">        The state dict of the provided model will be updated with `strict=False` setting so as to prevent</span>
<span class="sd">        requirement of exact model parameters matching.</span>
<span class="sd">        Initializations:</span>
<span class="sd">            init_from_mridc_model: Str path to a .mridc model, which will be instantiated in order</span>
<span class="sd">                to extract the state dict.</span>
<span class="sd">            init_from_pretrained_model: Str name of a pretrained model checkpoint (obtained via cloud).</span>
<span class="sd">                The model will be downloaded (or a cached copy will be used), instantiated and then</span>
<span class="sd">                its state dict will be extracted.</span>
<span class="sd">            init_from_ptl_ckpt: Str name of a Pytorch Lightning checkpoint file. It will be loaded and</span>
<span class="sd">                the state dict will extracted.</span>
<span class="sd">        Args:</span>
<span class="sd">            cfg: The config used to instantiate the model. It need only contain one of the above keys.</span>
<span class="sd">            map_location: str or torch.device() which represents where the intermediate state dict</span>
<span class="sd">                (from the pretrained model or checkpoint) will be loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;init_from_mridc_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">]</span>
        <span class="n">arg_matches</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># model weights do not need to be restored</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot pass more than one model initialization arguments to config!</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Found : </span><span class="si">{</span><span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">arg_present</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="k">if</span> <span class="n">arg_present</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_mridc_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore model</span>
                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                        <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>
                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from mridc file with path : `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                            <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;mridc file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_mridc_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Restore model</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_name</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

                    <span class="c1"># Check if model is being resumed or not - only works if `Trainer` is attached to model</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;trainer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="nb">hasattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;resume_from_checkpoint&quot;</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_checkpoint_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="p">):</span>
                            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="s2">&quot;Model training is being resumed via Pytorch Lightning.</span><span class="se">\n</span><span class="s2">&quot;</span>
                                <span class="s2">&quot;Initialization from pretrained model (via cloud) will be skipped.&quot;</span>
                            <span class="p">)</span>
                            <span class="k">return</span>

                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                        <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pretrained checkpoint with name : `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">name</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                            <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                            <span class="n">include</span><span class="p">,</span>
                            <span class="n">exclude</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;pretrained checkpoint with name `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_pretrained_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore checkpoint</span>
                    <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pytorch lightning checkpoint with path : `</span><span class="si">{</span><span class="n">ckpt_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;nemo file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_ptl_ckpt is not a string or a dict!&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Called at the end of fit and test.</span>
<span class="sd">        Args:</span>
<span class="sd">            stage: either &#39;fit&#39; or &#39;test&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;fit&quot;</span> <span class="ow">and</span> <span class="s2">&quot;PL_TRAINER_GPUS&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;PL_TRAINER_GPUS&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">teardown</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">extract_state_dict_from</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">split_by_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract the state dict(s) from a provided .mridc tarfile and save it to a directory.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which state dict(s) should be extracted</span>
<span class="sd">            save_dir: directory in which the saved state dict(s) should be stored</span>
<span class="sd">            split_by_module: bool flag, which determines whether the output checkpoint should</span>
<span class="sd">                be for the entire Model, or the individual module&#39;s that comprise the Model</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">        Example:</span>
<span class="sd">            To convert the .mridc tarfile into a single Model level PyTorch checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;)</span>
<span class="sd">            To restore a model from a Model level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            model.load_state_dict(torch.load(&quot;./asr_ckpts/model_weights.ckpt&quot;))</span>
<span class="sd">            To convert the .mridc tarfile into multiple Module level PyTorch checkpoints</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;, split_by_module=True)</span>
<span class="sd">            To restore a module from a Module level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            # load the individual components</span>
<span class="sd">            model.preprocessor.load_state_dict(torch.load(&quot;./asr_ckpts/preprocessor.ckpt&quot;))</span>
<span class="sd">            model.encoder.load_state_dict(torch.load(&quot;./asr_ckpts/encoder.ckpt&quot;))</span>
<span class="sd">            model.decoder.load_state_dict(torch.load(&quot;./asr_ckpts/decoder.ckpt&quot;))</span>
<span class="sd">        Returns:</span>
<span class="sd">            The state dict that was loaded from the original .mridc checkpoint</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileExistsError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">extract_state_dict_from</span><span class="p">(</span><span class="n">restore_path</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">split_by_module</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">prepare_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;Trainer&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper method to check whether the model can safely be tested</span>
<span class="sd">        on a dataset after training (or loading a checkpoint).</span>
<span class="sd">        ::</span>
<span class="sd">            trainer = Trainer()</span>
<span class="sd">            if model.prepare_test(trainer):</span>
<span class="sd">                trainer.test(model)</span>
<span class="sd">        Returns:</span>
<span class="sd">            bool which declares the model safe to test. Provides warnings if it has to</span>
<span class="sd">            return False to guide the user.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;test_ds&quot;</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No `test_ds` config found within the manifest.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Replace ddp multi-gpu until PTL has a fix</span>
            <span class="n">DDP_WARN</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\n\n</span><span class="s2">During testing, it is currently advisable to construct a new Trainer &quot;</span>
<span class="s2">                    &quot;with single GPU and no DDP to obtain accurate results.</span>
<span class="s2">                    &quot;Following pattern should be used: &quot;</span>
<span class="s2">                    &quot;gpu = 1 if cfg.trainer.gpus != 0 else 0&quot;</span>
<span class="s2">                    &quot;trainer = Trainer(gpus=gpu)&quot;</span>
<span class="s2">                    &quot;if model.prepare_test(trainer):&quot;</span>
<span class="s2">                    &quot;  trainer.test(model)</span><span class="se">\n\n</span><span class="s2">&quot;&quot;&quot;</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">DDP_WARN</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Assign trainer to the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">set_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set an instance of Trainer object.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer: PyTorch Lightning Trainer object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determines the world size from the PyTorch Lightning Trainer.</span>
<span class="sd">        And then updates AppState.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer (Trainer): PyTorch Lightning Trainer object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update AppState with world information from trainer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">app_state</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;World size can only be set by PyTorch Lightning Trainer.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_dataset_config</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update the config (if not None) of the dataset by given name.</span>
<span class="sd">        Preserves said config after updating.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataset_name: str name of the dataset whose config is being updated.</span>
<span class="sd">                Can be one of `train`, `validation` and `test`.</span>
<span class="sd">            config: Optional DictConfig or dict. If None is passed, this method simply returns.</span>
<span class="sd">                If dict is passed, it is cast into a DictConfig.</span>
<span class="sd">                The internal config is updated with the passed config.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;_multi_dataset_mode&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="n">config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">dataset_name</span> <span class="ow">in</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="s2">&quot;test&quot;</span><span class="p">}:</span>
                <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

                <span class="n">key_name</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dataset_name</span><span class="si">}</span><span class="s2">_ds&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">[</span><span class="n">key_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">config</span>

                <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

                <span class="c1"># Update hyper parameters by calling property setter</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`dataset_name` when updating config must be one of [train, validation, test]&quot;</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">num_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Utility property that returns the total number of parameters of the Model.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span> <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">cfg</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property that holds the finalized internal config of the model.</span>
<span class="sd">        Note:</span>
<span class="sd">            Changes to this config are not reflected in the state of the model.</span>
<span class="sd">            Please create a new model using an updated config to properly update the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span>

    <span class="nd">@cfg</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">cfg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Property that holds the finalized internal config of the model.</span>
<span class="sd">        Note:</span>
<span class="sd">            Changes to this config are not reflected in the state of the model.</span>
<span class="sd">            Please create a new model using an updated config to properly update the model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_hparams</span><span class="p">(</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="s2">&quot;cfg&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">}))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_is_model_being_restored</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Checks if the model is being restored from a checkpoint.&quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">app_state</span><span class="o">.</span><span class="n">is_model_being_restored</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the state of the model to be restored.&quot;&quot;&quot;</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">is_model_being_restored</span> <span class="o">=</span> <span class="n">is_being_restored</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">mridc_file_folder</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">folder</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_set_model_guid</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the model guid.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;model_guid&quot;</span><span class="p">):</span>
            <span class="n">appstate</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

            <span class="c1"># Generate a unique uuid for the instance</span>
            <span class="c1"># also determine if the model is being restored or not, and preserve the path</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_guid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_model_being_restored</span><span class="p">():</span>
                <span class="n">restore_path</span> <span class="o">=</span> <span class="n">appstate</span><span class="o">.</span><span class="n">model_restore_path</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">restore_path</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">appstate</span><span class="o">.</span><span class="n">register_model_guid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_guid</span><span class="p">,</span> <span class="n">restoration_path</span><span class="o">=</span><span class="n">restore_path</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">update_save_restore_connector</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the save_restore_connector of the model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Interface for Pytorch-lightning based mridc models</p>
</div>


                            <div id="ModelPT.__init__" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.__init__">#&nbsp;&nbsp</a>


            <span class="name">ModelPT</span><span class="signature">(
    cfg: omegaconf.dictconfig.DictConfig,
    trainer: pytorch_lightning.trainer.trainer.Trainer = None
)</span>
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Base class from which all mridc models should inherit</span>

<span class="sd">        Internal global flags that determine core functionality of ModelPT.</span>
<span class="sd">        _MODEL_IS_RESTORED:</span>
<span class="sd">            This flag determines the context of the model - whether the model is currently being</span>
<span class="sd">            restored or not.</span>
<span class="sd">            -   When set, it can be assumed that the model&#39;s will disable all automatic methods -</span>
<span class="sd">                setup_training_data(), setup_validation/test_data() and their multi equivalents.</span>
<span class="sd">            -   If a model is being restored from a archive file (tarfile), it can be assumed that</span>
<span class="sd">                under this context, the cwd is *inside* the tarfile itself.</span>
<span class="sd">        _MODEL_RESTORE_PATH:</span>
<span class="sd">            A string path to a a file from which the model is being restored.</span>
<span class="sd">            This file can either be a PyTorch Lightning Checkpoint, or a archive (tarfile) that contains</span>
<span class="sd">            artifact objects.</span>
<span class="sd">            If it is an archive file, during restoration, the cwd will be temporarily moved to inside the</span>
<span class="sd">            archive itself.</span>

<span class="sd">        Args:</span>
<span class="sd">            cfg (DictConfig):  configuration object.</span>
<span class="sd">                The cfg object should have (optionally) the following sub-configs:</span>
<span class="sd">                * train_ds - to instantiate training dataset</span>
<span class="sd">                * validation_ds - to instantiate validation dataset</span>
<span class="sd">                * test_ds - to instantiate testing dataset</span>
<span class="sd">                * optim - to instantiate optimizer with learning rate scheduler</span>
<span class="sd">            trainer (Optional): Pytorch Lightning Trainer instance</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;trainer constructor argument must be either None or pytorch_lightning.Trainer. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;But got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead.&quot;</span>
            <span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># set global vars in AppState</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>

        <span class="c1"># Convert config to a DictConfig</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">convert_model_config_to_dict_config</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># Convert config to support Hydra 1.0+ instantiation</span>
        <span class="n">cfg</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">maybe_update_config_version</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Creating model config node is forbidden due to collision problem when loading from checkpoint.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;target&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">cfg</span><span class="p">:</span>
            <span class="c1"># This is for Jarvis service.</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
            <span class="n">cfg</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{0}</span><span class="s2">.</span><span class="si">{1}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__module__</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
            <span class="n">OmegaConf</span><span class="o">.</span><span class="n">set_struct</span><span class="p">(</span><span class="n">cfg</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="o">=</span> <span class="n">cfg</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s2">&quot;cfg&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>  <span class="c1"># reference required for self.*_rank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>  <span class="c1"># alias for backward compatibility</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_set_model_guid</span><span class="p">()</span>

        <span class="c1"># Set device_id in AppState</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">app_state</span><span class="o">.</span><span class="n">device_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_is_model_being_restored</span><span class="p">():</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">setup_multiple_test_data</span><span class="p">(</span><span class="n">test_data_config</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;train_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;method and provide a valid configuration file to setup the train data loader.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Train config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">train_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;validation_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;If you intend to do validation, please call the ModelPT.setup_validation_data() or &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;setup the validation data loader(s). </span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Validation config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">validation_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="s2">&quot;test_ds&quot;</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;and provide a valid configuration file to setup the test data loader(s).</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;Test config : </span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">test_ds</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>  <span class="c1"># type: ignore</span>
                <span class="p">)</span>

        <span class="c1"># ModelPT wrappers over subclass implementations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_training_step</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">wrap_training_step</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</pre></div>

        </details>

            <div class="docstring"><p>Base class from which all mridc models should inherit</p>

<p>Internal global flags that determine core functionality of ModelPT.
_MODEL_IS_RESTORED:
    This flag determines the context of the model - whether the model is currently being
    restored or not.
    -   When set, it can be assumed that the model's will disable all automatic methods -
        setup_training_data(), setup_validation/test_data() and their multi equivalents.
    -   If a model is being restored from a archive file (tarfile), it can be assumed that
        under this context, the cwd is <em>inside</em> the tarfile itself.
_MODEL_RESTORE_PATH:
    A string path to a a file from which the model is being restored.
    This file can either be a PyTorch Lightning Checkpoint, or a archive (tarfile) that contains
    artifact objects.
    If it is an archive file, during restoration, the cwd will be temporarily moved to inside the
    archive itself.</p>

<p>Args:
    cfg (DictConfig):  configuration object.
        The cfg object should have (optionally) the following sub-configs:
        * train_ds - to instantiate training dataset
        * validation_ds - to instantiate validation dataset
        * test_ds - to instantiate testing dataset
        * optim - to instantiate optimizer with learning rate scheduler
    trainer (Optional): Pytorch Lightning Trainer instance</p>
</div>


                            </div>
                            <div id="ModelPT.register_artifact" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.register_artifact">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">register_artifact</span><span class="signature">(self, config_path: str, src: str, verify_src_exists: bool = True)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Register model artifacts with this function. These artifacts (files) will be included inside .mridc file</span>
<span class="sd">        when model.save_to(&quot;model.mridc&quot;) is called.</span>
<span class="sd">        How it works:</span>
<span class="sd">        1. It always returns existing absolute path which can be used during Model constructor call</span>
<span class="sd">            EXCEPTION: src is None or &quot;&quot; in which case nothing will be done and src will be returned</span>
<span class="sd">        2. It will add (config_path, model_utils.ArtifactItem()) pair to self.artifacts</span>
<span class="sd">        If &quot;src&quot; is local existing path, then it will be returned in absolute path form.</span>
<span class="sd">        elif &quot;src&quot; starts with &quot;mridc_file:unique_artifact_name&quot;:</span>
<span class="sd">            .mridc will be untarred to a temporary folder location and an actual existing path will be returned</span>
<span class="sd">        else an error will be raised.</span>
<span class="sd">        WARNING: use .register_artifact calls in your models&#39; constructors.</span>
<span class="sd">        The returned path is not guaranteed to exist after you have exited your model&#39;s constructor.</span>
<span class="sd">        Args:</span>
<span class="sd">            config_path (str): Artifact key. Usually corresponds to the model config.</span>
<span class="sd">            src (str): Path to artifact.</span>
<span class="sd">            verify_src_exists (bool): If set to False, then the artifact is optional and register_artifact will</span>
<span class="sd">                                        return None even if src is not found. Defaults to True.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str: If src is not None or empty it always returns absolute path which is guaranteed to exists during</span>
<span class="sd">                model instance life</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">src</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">src</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">src</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;artifacts&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">ArtifactItem</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">config_path</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">artifacts</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;You tried to register an artifact under config key=</span><span class="si">{</span><span class="n">config_path</span><span class="si">}</span><span class="s2"> but an artifact for &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;it has already been registered.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">register_artifact</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">verify_src_exists</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Register model artifacts with this function. These artifacts (files) will be included inside .mridc file
when model.save_to("model.mridc") is called.
How it works:</p>

<ol>
<li>It always returns existing absolute path which can be used during Model constructor call
EXCEPTION: src is None or "" in which case nothing will be done and src will be returned</li>
<li>It will add (config_path, model_utils.ArtifactItem()) pair to self.artifacts
If "src" is local existing path, then it will be returned in absolute path form.
elif "src" starts with "mridc_file:unique_artifact_name":
.mridc will be untarred to a temporary folder location and an actual existing path will be returned
else an error will be raised.
WARNING: use .register_artifact calls in your models' constructors.
The returned path is not guaranteed to exist after you have exited your model's constructor.
Args:
config_path (str): Artifact key. Usually corresponds to the model config.
src (str): Path to artifact.
verify_src_exists (bool): If set to False, then the artifact is optional and register_artifact will
                            return None even if src is not found. Defaults to True.
Returns:
str: If src is not None or empty it always returns absolute path which is guaranteed to exists during
    model instance life</li>
</ol>
</div>


                            </div>
                            <div id="ModelPT.save_to" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.save_to">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">save_to</span><span class="signature">(self, save_path: str)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">save_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves model instance (weights and configuration) into .mridc file</span>
<span class="sd">         You can use &quot;restore_from&quot; method to fully restore instance from .mridc file.</span>
<span class="sd">        .mridc file is an archive (tar.gz) with the following:</span>
<span class="sd">            model_config.yaml - model configuration in .yaml format. You can deserialize this into cfg argument for</span>
<span class="sd">            model&#39;s constructor</span>
<span class="sd">            model_wights.ckpt - model checkpoint</span>
<span class="sd">        Args:</span>
<span class="sd">            save_path: Path to .mridc file where model instance should be saved</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">def</span> <span class="nf">maybe_make_save_dir</span><span class="p">(</span><span class="n">_path</span><span class="p">:</span> <span class="s2">&quot;Path&quot;</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Creates directory if it does not exist&quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
                <span class="n">_path</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">save_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span><span class="o">.</span><span class="n">expanduser</span><span class="p">()</span><span class="o">.</span><span class="n">resolve</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">model_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="p">)</span> <span class="ow">is</span> <span class="n">SaveRestoreConnector</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Default mridc SaveRestoreConnector will not work in model parallel mode. You should use a &quot;</span>
                    <span class="s2">&quot;connector which supports model parallel mode. You can also use a custom one.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">app_state</span><span class="o">.</span><span class="n">data_parallel_rank</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="c1"># connector checks for ranks properly, no need to check here</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>
        <span class="k">elif</span> <span class="n">is_global_rank_zero</span><span class="p">():</span>
            <span class="n">maybe_make_save_dir</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">save_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">save_path</span><span class="p">))</span>  <span class="c1"># downstream tasks expect str, not Path</span>
</pre></div>

        </details>

            <div class="docstring"><p>Saves model instance (weights and configuration) into .mridc file
 You can use "restore_from" method to fully restore instance from .mridc file.
.mridc file is an archive (tar.gz) with the following:
    model_config.yaml - model configuration in .yaml format. You can deserialize this into cfg argument for
    model's constructor
    model_wights.ckpt - model checkpoint
Args:
    save_path: Path to .mridc file where model instance should be saved</p>
</div>


                            </div>
                            <div id="ModelPT.restore_from" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.restore_from">#&nbsp;&nbsp</a>

                <div class="decorator">@classmethod</div>

            <span class="def">def</span>
            <span class="name">restore_from</span><span class="signature">(
    cls,
    restore_path: str,
    override_config_path: Union[omegaconf.omegaconf.OmegaConf, str, NoneType] = None,
    map_location: Optional[torch.device] = None,
    strict: bool = True,
    return_config: bool = False,
    save_restore_connector: <a href="../connectors/save_restore_connector.html#SaveRestoreConnector">mridc.core.connectors.save_restore_connector.SaveRestoreConnector</a> = None,
    trainer: Optional[pytorch_lightning.trainer.trainer.Trainer] = None
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">restore_from</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">override_config_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">OmegaConf</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">return_config</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">trainer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Trainer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Restores model instance (weights and configuration) from .mridc file.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which model should be instantiated</span>
<span class="sd">            override_config_path: path to a yaml config that will override the internal</span>
<span class="sd">                config file or an OmegaConf / DictConfig object representing the model config.</span>
<span class="sd">            map_location: Optional torch.device() to map the instantiated model to a device.</span>
<span class="sd">                By default (None), it will select a GPU if available, falling back to CPU otherwise.</span>
<span class="sd">            strict: Passed to load_state_dict. By default True.</span>
<span class="sd">            return_config: If set to true, will return just the underlying config of the restored</span>
<span class="sd">                model as an OmegaConf DictConfig object without instantiating the model.</span>
<span class="sd">            trainer: Optional, a pytorch lightning Trainer object that will be forwarded to the</span>
<span class="sd">                instantiated model&#39;s constructor.</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">            Example:</span>
<span class="sd">                ```</span>
<span class="sd">                model = mridc.collections.asr.models.EncDecCTCModel.restore_from(&#39;asr.mridc&#39;)</span>
<span class="sd">                assert isinstance(model, mridc.collections.asr.models.EncDecCTCModel)</span>
<span class="sd">                ```</span>
<span class="sd">        Returns:</span>
<span class="sd">            An instance of type cls or its underlying config (if return_config is set).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="n">restore_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">restore_path</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
        <span class="n">app_state</span><span class="o">.</span><span class="n">model_restore_path</span> <span class="o">=</span> <span class="n">restore_path</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="n">instance</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
            <span class="bp">cls</span><span class="p">,</span> <span class="n">restore_path</span><span class="p">,</span> <span class="n">override_config_path</span><span class="p">,</span> <span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span> <span class="n">return_config</span><span class="p">,</span> <span class="n">trainer</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">ModelPT</span><span class="p">):</span>
            <span class="n">instance</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">return</span> <span class="n">instance</span>
</pre></div>

        </details>

            <div class="docstring"><p>Restores model instance (weights and configuration) from .mridc file.
Args:
    restore_path: path to .mridc file from which model should be instantiated
    override_config_path: path to a yaml config that will override the internal
        config file or an OmegaConf / DictConfig object representing the model config.
    map_location: Optional torch.device() to map the instantiated model to a device.
        By default (None), it will select a GPU if available, falling back to CPU otherwise.
    strict: Passed to load_state_dict. By default True.
    return_config: If set to true, will return just the underlying config of the restored
        model as an OmegaConf DictConfig object without instantiating the model.
    trainer: Optional, a pytorch lightning Trainer object that will be forwarded to the
        instantiated model's constructor.
    save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.
    Example:
        <code>
        model = <a href="../../collections.html#asr.models.EncDecCTCModel.restore_from">mridc.collections.asr.models.EncDecCTCModel.restore_from</a>('asr.mridc')
        assert isinstance(model, <a href="../../collections.html#asr.models.EncDecCTCModel">mridc.collections.asr.models.EncDecCTCModel</a>)
</code>
Returns:
    An instance of type cls or its underlying config (if return_config is set).</p>
</div>


                            </div>
                            <div id="ModelPT.load_from_checkpoint" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.load_from_checkpoint">#&nbsp;&nbsp</a>

                <div class="decorator">@classmethod</div>

            <span class="def">def</span>
            <span class="name">load_from_checkpoint</span><span class="signature">(
    cls,
    checkpoint_path: str,
    *args,
    map_location: Union[Dict[str, str], str, torch.device, int, Callable, NoneType] = None,
    hparams_file: Optional[str] = None,
    strict: bool = True,
    **kwargs
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_from_checkpoint</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">checkpoint_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="o">*</span><span class="n">args</span><span class="p">,</span>
        <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hparams_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">strict</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Loads ModelPT from checkpoint, with some maintenance of restoration.</span>
<span class="sd">        For documentation, please refer to LightningModule.load_from_checkpoint() documentation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">checkpoint</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
                <span class="n">checkpoint_path</span><span class="o">=</span><span class="n">checkpoint_path</span><span class="p">,</span>
                <span class="o">*</span><span class="n">args</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
                <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span>
                <span class="n">hparams_file</span><span class="o">=</span><span class="n">hparams_file</span><span class="p">,</span>
                <span class="n">strict</span><span class="o">=</span><span class="n">strict</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_set_model_restore_state</span><span class="p">(</span><span class="n">is_being_restored</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">checkpoint</span>
</pre></div>

        </details>

            <div class="docstring"><p>Loads ModelPT from checkpoint, with some maintenance of restoration.
For documentation, please refer to LightningModule.load_from_checkpoint() documentation.</p>
</div>


                            </div>
                            <div id="ModelPT.setup_training_data" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_training_data">#&nbsp;&nbsp</a>

                <div class="decorator">@abstractmethod</div>

            <span class="def">def</span>
            <span class="name">setup_training_data</span><span class="signature">(
    self,
    train_data_config: Union[omegaconf.dictconfig.DictConfig, Dict]
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_training_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in training</span>
<span class="sd">        Args:</span>
<span class="sd">            train_data_config: training data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Setups data loader to be used in training
Args:
    train_data_config: training data layer parameters.
Returns:</p>
</div>


                            </div>
                            <div id="ModelPT.setup_validation_data" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_validation_data">#&nbsp;&nbsp</a>

                <div class="decorator">@abstractmethod</div>

            <span class="def">def</span>
            <span class="name">setup_validation_data</span><span class="signature">(self, val_data_config: Union[omegaconf.dictconfig.DictConfig, Dict])</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">setup_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Setups data loader to be used in validation</span>
<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
</pre></div>

        </details>

            <div class="docstring"><p>Setups data loader to be used in validation
Args:
    val_data_config: validation data layer parameters.
Returns:</p>
</div>


                            </div>
                            <div id="ModelPT.setup_test_data" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_test_data">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">setup_test_data</span><span class="signature">(self, test_data_config: Union[omegaconf.dictconfig.DictConfig, Dict])</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">setup_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config: test data layer parameters.</span>
<span class="sd">        Returns:</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">()</span>
</pre></div>

        </details>

            <div class="docstring"><p>(Optionally) Setups data loader to be used in test
Args:
    test_data_config: test data layer parameters.
Returns:</p>
</div>


                            </div>
                            <div id="ModelPT.setup_multiple_validation_data" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_multiple_validation_data">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">setup_multiple_validation_data</span><span class="signature">(self, val_data_config: Union[omegaconf.dictconfig.DictConfig, Dict])</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">setup_multiple_validation_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">val_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in validation</span>

<span class="sd">        Args:</span>
<span class="sd">            val_data_config: validation data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">val_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_validation_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;val_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span><span class="p">))]</span>
</pre></div>

        </details>

            <div class="docstring"><p>(Optionally) Setups data loader to be used in validation</p>

<p>Args:
    val_data_config: validation data layer parameters.</p>
</div>


                            </div>
                            <div id="ModelPT.setup_multiple_test_data" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_multiple_test_data">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">setup_multiple_test_data</span><span class="signature">(self, test_data_config: Union[omegaconf.dictconfig.DictConfig, Dict])</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">setup_multiple_test_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">test_data_config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        (Optionally) Setups data loader to be used in test, with support for multiple data loaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            test_data_config ():  test data layer parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Set some placeholder overridden by helper method</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># preserve config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_update_dataset_config</span><span class="p">(</span><span class="n">dataset_name</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">test_data_config</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mridc</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">model_utils</span><span class="o">.</span><span class="n">resolve_test_dataloaders</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_multi_dataset_mode</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;test_</span><span class="si">{}</span><span class="s2">_&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span><span class="p">))]</span>
</pre></div>

        </details>

            <div class="docstring"><p>(Optionally) Setups data loader to be used in test, with support for multiple data loaders.
Args:
    test_data_config ():  test data layer parameters.</p>
</div>


                            </div>
                            <div id="ModelPT.setup_optimization" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.setup_optimization">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">setup_optimization</span><span class="signature">(
    self,
    optim_config: Union[omegaconf.dictconfig.DictConfig, Dict, NoneType] = None
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">setup_optimization</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optim_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">Dict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepares an optimizer from a string name and its optional config parameters.</span>
<span class="sd">        Args:</span>
<span class="sd">            optim_config: A dictionary containing the following keys:</span>
<span class="sd">                * &quot;lr&quot;: mandatory key for learning rate. Will raise ValueError if not provided.</span>
<span class="sd">                * &quot;optimizer&quot;: string name pointing to one of the available optimizers in the registry. \</span>
<span class="sd">                If not provided, defaults to &quot;adam&quot;.</span>
<span class="sd">                * &quot;opt_args&quot;: Optional list of strings, in the format &quot;arg_name=arg_value&quot;. \</span>
<span class="sd">                The list of &quot;arg_value&quot; will be parsed and a dictionary of optimizer kwargs \</span>
<span class="sd">                will be built and supplied to instantiate the optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If config was not explicitly passed to us</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span>

        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No optimizer config provided, therefore no optimizer was created&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="c1"># Preserve the configuration</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># See if internal config has `optim` namespace before preservation</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;optim&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

        <span class="c1"># Setup optimizer and scheduler</span>
        <span class="k">if</span> <span class="n">optim_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Trainer wasn&#39;t specified in model constructor. Make sure that you really wanted it.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;We do not currently support gradient accumulation that is not an integer.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">max_steps</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Store information needed to calculate max_steps</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_max_epochs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_epochs</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_accumulate_grad_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accumulate_grad_batches</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_limit_train_batches&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">limit_train_batches</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">or</span> <span class="mi">1</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp_cpu&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_processes</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span> <span class="o">==</span> <span class="s2">&quot;ddp&quot;</span><span class="p">:</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;The lightning trainer received accelerator: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">accelerator</span><span class="si">}</span><span class="s2">. We &quot;</span>
                        <span class="s2">&quot;recommend to use &#39;ddp&#39; instead.&quot;</span>
                    <span class="p">)</span>
                    <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;t_num_workers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">optim_config</span><span class="p">[</span><span class="s2">&quot;sched&quot;</span><span class="p">][</span><span class="s2">&quot;max_steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">max_steps</span>

        <span class="c1"># Force into DictConfig from nested structure</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>
        <span class="c1"># Get back nested dict so we its mutable</span>
        <span class="n">optim_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">optim_config</span><span class="p">,</span> <span class="n">resolve</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Extract scheduler config if inside optimizer config</span>
        <span class="k">if</span> <span class="s2">&quot;sched&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sched&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Check if caller provided optimizer name, default to Adam otherwise</span>
        <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;_target_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Try to get optimizer name for dynamic resolution, defaulting to Adam</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="s2">&quot;adam&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="vm">__name__</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># resolve the class name (lowercase) from the class path if not provided</span>
            <span class="n">optimizer_name</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

        <span class="c1"># We are guaranteed to have lr since it is required by the argparser</span>
        <span class="c1"># But maybe user forgot to pass it to this function</span>
        <span class="n">lr</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Check if caller has optimizer kwargs, default to empty dictionary</span>
        <span class="k">if</span> <span class="s2">&quot;args&quot;</span> <span class="ow">in</span> <span class="n">optim_config</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">optim_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;args&quot;</span><span class="p">)</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">parse_optimizer_args</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer_args</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">optim_config</span><span class="p">)</span>

            <span class="c1"># Remove extra parameters from optimizer_args nest</span>
            <span class="c1"># Assume all other parameters are to be passed into optimizer constructor</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;name&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;cls&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">optimizer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Adaptive schedulers don&#39;t need `lr`</span>
        <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer_args</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="c1"># Actually instantiate the optimizer</span>
        <span class="k">if</span> <span class="n">optimizer_cls</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">get_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">)</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">elif</span> <span class="n">inspect</span><span class="o">.</span><span class="n">isclass</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">):</span>
            <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer_cls</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_args</span><span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer</span><span class="p">))</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>  <span class="c1"># type: ignore</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Attempt class path resolution</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">optimizer_cls</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({</span><span class="s2">&quot;_target_&quot;</span><span class="p">:</span> <span class="n">optimizer_cls</span><span class="p">})</span>
                <span class="n">optimizer_config</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">lr</span><span class="p">}</span> <span class="k">if</span> <span class="n">lr</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="n">optimizer_config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">optimizer_args</span><span class="p">)</span>

                <span class="n">optimizer_instance</span> <span class="o">=</span> <span class="n">hydra</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">instantiate</span><span class="p">(</span>
                    <span class="n">optimizer_cls</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">optimizer_config</span>
                <span class="p">)</span>  <span class="c1"># type: DictConfig</span>

                <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Optimizer config = </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_instance</span><span class="p">))</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span> <span class="o">=</span> <span class="n">optimizer_instance</span>

            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">logging</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="s2">&quot;Could not instantiate class path - </span><span class="si">{}</span><span class="s2"> with kwargs </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">optimizer_cls</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">optimizer_config</span><span class="p">))</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="n">e</span>

        <span class="c1"># Try to instantiate scheduler for optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="o">=</span> <span class="n">mridc</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">prepare_lr_scheduler</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">optimizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">train_dataloader</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Return the optimizer with/without scheduler</span>
        <span class="c1"># This return allows multiple optimizers or schedulers to be created</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span>
</pre></div>

        </details>

            <div class="docstring"><p>Prepares an optimizer from a string name and its optional config parameters.
Args:
    optim_config: A dictionary containing the following keys:
        * "lr": mandatory key for learning rate. Will raise ValueError if not provided.
        * "optimizer": string name pointing to one of the available optimizers in the registry.                 If not provided, defaults to "adam".
        * "opt_args": Optional list of strings, in the format "arg_name=arg_value".                 The list of "arg_value" will be parsed and a dictionary of optimizer kwargs                 will be built and supplied to instantiate the optimizer.</p>
</div>


                            </div>
                            <div id="ModelPT.configure_optimizers" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.configure_optimizers">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">configure_optimizers</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Configure optimizers and schedulers for training.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_optimization</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span>

        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_optimizer</span><span class="p">],</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_scheduler</span><span class="p">]</span>
</pre></div>

        </details>

            <div class="docstring"><p>Configure optimizers and schedulers for training.</p>
</div>


                            </div>
                            <div id="ModelPT.train_dataloader" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.train_dataloader">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">train_dataloader</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the training dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_train_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</pre></div>

        </details>

            <div class="docstring"><p>Return the training dataloader.</p>
</div>


                            </div>
                            <div id="ModelPT.val_dataloader" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.val_dataloader">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">val_dataloader</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the validation dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validation_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</pre></div>

        </details>

            <div class="docstring"><p>Return the validation dataloader.</p>
</div>


                            </div>
                            <div id="ModelPT.test_dataloader" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.test_dataloader">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">test_dataloader</span><span class="signature">(self)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the test dataloader.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span>
</pre></div>

        </details>

            <div class="docstring"><p>Return the test dataloader.</p>
</div>


                            </div>
                            <div id="ModelPT.validation_epoch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.validation_epoch_end">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">validation_epoch_end</span><span class="signature">(
    self,
    outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]
) -&gt; Optional[Dict[str, Dict[str, torch.Tensor]]]</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Validation set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_validation_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_validation_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `val_loss`,</span>
<span class="sd">            only the `val_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `val_dl_idx: int`</span>
<span class="sd">            inside the `validation_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">val_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_validation_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_validation_epoch_end</span><span class="p">(</span><span class="n">val_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `val_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;val_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span> <span class="ow">and</span> <span class="s2">&quot;val_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span>
            <span class="p">):</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>

                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the metric, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_val_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="c1"># Store log value</span>
                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>
</pre></div>

        </details>

            <div class="docstring"><p>Default DataLoader for Validation set which automatically supports multiple data loaders
via <code><a href="#ModelPT.multi_validation_epoch_end">multi_validation_epoch_end</a></code>.
If multi dataset support is not required, override this method entirely in base class.
In such a case, there is no need to implement <code><a href="#ModelPT.multi_validation_epoch_end">multi_validation_epoch_end</a></code> either.
.. note::
    If more than one data loader exists, and they all provide <code>val_loss</code>,
    only the <code>val_loss</code> of the first data loader will be used by default.
    This default can be changed by passing the special key <code>val_dl_idx: int</code>
    inside the <code>validation_ds</code> config.
Args:
    outputs: Single or nested list of tensor outputs from one or more data loaders.
Returns:
    A dictionary containing the union of all items from individual data_loaders,
    along with merged logs from all data loaders.</p>
</div>


                            </div>
                            <div id="ModelPT.test_epoch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.test_epoch_end">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">test_epoch_end</span><span class="signature">(
    self,
    outputs: Union[List[Dict[str, torch.Tensor]], List[List[Dict[str, torch.Tensor]]]]
) -&gt; Optional[Dict[str, Dict[str, torch.Tensor]]]</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]]:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Default DataLoader for Test set which automatically supports multiple data loaders</span>
<span class="sd">        via `multi_test_epoch_end`.</span>
<span class="sd">        If multi dataset support is not required, override this method entirely in base class.</span>
<span class="sd">        In such a case, there is no need to implement `multi_test_epoch_end` either.</span>
<span class="sd">        .. note::</span>
<span class="sd">            If more than one data loader exists, and they all provide `test_loss`,</span>
<span class="sd">            only the `test_loss` of the first data loader will be used by default.</span>
<span class="sd">            This default can be changed by passing the special key `_test_dl_idx: int`</span>
<span class="sd">            inside the `test_ds` config.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Single or nested list of tensor outputs from one or more data loaders.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary containing the union of all items from individual data_loaders,</span>
<span class="sd">            along with merged logs from all data loaders.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Case where we dont provide data loaders</span>
        <span class="k">if</span> <span class="n">outputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">{}</span>

        <span class="c1"># Case where we provide exactly 1 data loader</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">output_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">if</span> <span class="n">output_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

            <span class="k">return</span> <span class="n">output_dict</span>

        <span class="n">output_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">{}}</span>

        <span class="c1"># The output is a list of list of dicts, outer list corresponds to dataloader idx</span>
        <span class="k">for</span> <span class="n">dataloader_idx</span><span class="p">,</span> <span class="n">test_outputs</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">outputs</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
            <span class="c1"># Get prefix and dispatch call to multi epoch end</span>
            <span class="n">dataloader_prefix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_test_dataloader_prefix</span><span class="p">(</span><span class="n">dataloader_idx</span><span class="p">)</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">multi_test_epoch_end</span><span class="p">(</span><span class="n">test_outputs</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="n">dataloader_idx</span><span class="p">)</span>

            <span class="c1"># If result was not provided, generate empty dict</span>
            <span class="n">dataloader_logs</span> <span class="o">=</span> <span class="n">dataloader_logs</span> <span class="ow">or</span> <span class="p">{}</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># Perform `test_loss` resolution first (if provided outside logs)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="s2">&quot;test_loss&quot;</span> <span class="ow">in</span> <span class="n">dataloader_logs</span>
                <span class="ow">and</span> <span class="s2">&quot;test_loss&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span>  <span class="c1"># type: ignore</span>
                <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span>
            <span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataloader_logs</span><span class="p">[</span><span class="s2">&quot;test_loss&quot;</span><span class="p">]</span>  <span class="c1"># type: ignore</span>

            <span class="c1"># For every item in the result dictionary</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">dataloader_logs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># If the key is `log`</span>
                <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
                    <span class="c1"># Parse every element of the log, and attach the prefix name of the data loader</span>
                    <span class="n">log_dict</span> <span class="o">=</span> <span class="p">{}</span>
                    <span class="k">for</span> <span class="n">k_log</span><span class="p">,</span> <span class="n">v_log</span> <span class="ow">in</span> <span class="n">v</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                        <span class="c1"># If we are logging the loss, but dont provide it at result level,</span>
                        <span class="c1"># store it twice - once in log and once in result level.</span>
                        <span class="c1"># Also mark log with prefix name to avoid log level clash with other data loaders</span>
                        <span class="k">if</span> <span class="n">k_log</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">_test_dl_idx</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">k_log</span>

                            <span class="c1"># Also insert duplicate key with prefix for ease of comparison / avoid name clash</span>
                            <span class="n">log_dict</span><span class="p">[</span><span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                        <span class="k">else</span><span class="p">:</span>
                            <span class="c1"># Simply prepend prefix to key and save</span>
                            <span class="n">new_k_log</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k_log</span>

                        <span class="n">log_dict</span><span class="p">[</span><span class="n">new_k_log</span><span class="p">]</span> <span class="o">=</span> <span class="n">v_log</span>

                    <span class="c1"># Update log storage of individual data loader</span>
                    <span class="n">output_logs</span> <span class="o">=</span> <span class="n">output_dict</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="p">{})</span>  <span class="c1"># type: ignore</span>
                    <span class="n">output_logs</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">log_dict</span><span class="p">)</span>

                    <span class="c1"># Update global log storage</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="s2">&quot;log&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_logs</span>  <span class="c1"># type: ignore</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># If any values are stored outside &#39;log&#39;, simply prefix name and store</span>
                    <span class="n">new_k</span> <span class="o">=</span> <span class="n">dataloader_prefix</span> <span class="o">+</span> <span class="n">k</span>
                    <span class="n">output_dict</span><span class="p">[</span><span class="n">new_k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>  <span class="c1"># type: ignore</span>

        <span class="k">if</span> <span class="s2">&quot;log&quot;</span> <span class="ow">in</span> <span class="n">output_dict</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">output_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">),</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

        <span class="c1"># return everything else</span>
        <span class="k">return</span> <span class="n">output_dict</span>
</pre></div>

        </details>

            <div class="docstring"><p>Default DataLoader for Test set which automatically supports multiple data loaders
via <code><a href="#ModelPT.multi_test_epoch_end">multi_test_epoch_end</a></code>.
If multi dataset support is not required, override this method entirely in base class.
In such a case, there is no need to implement <code><a href="#ModelPT.multi_test_epoch_end">multi_test_epoch_end</a></code> either.
.. note::
    If more than one data loader exists, and they all provide <code>test_loss</code>,
    only the <code>test_loss</code> of the first data loader will be used by default.
    This default can be changed by passing the special key <code>_test_dl_idx: int</code>
    inside the <code>test_ds</code> config.
Args:
    outputs: Single or nested list of tensor outputs from one or more data loaders.
Returns:
    A dictionary containing the union of all items from individual data_loaders,
    along with merged logs from all data loaders.</p>
</div>


                            </div>
                            <div id="ModelPT.multi_validation_epoch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.multi_validation_epoch_end">#&nbsp;&nbsp</a>

                <div class="decorator">@staticmethod</div>

            <span class="def">def</span>
            <span class="name">multi_validation_epoch_end</span><span class="signature">(
    outputs: Union[object, List[Dict[str, torch.Tensor]], NoneType],
    dataloader_idx: int = 0
) -&gt; None</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_validation_epoch_end</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span> <span class="kc">None</span><span class="p">],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple validation datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_validation_epoch_end(outputs, dataloader_idx) &quot;</span>
            <span class="s2">&quot;has not been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override `validation_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Adds support for multiple validation datasets. Should be overridden by subclass,
so as to obtain appropriate logs for each of the dataloaders.
Args:
    outputs: Same as that provided by LightningModule.validation_epoch_end()
        for a single dataloader.
    dataloader_idx: int representing the index of the dataloader.
Returns:
    A dictionary of values, optionally containing a sub-dict <code><a href="#ModelPT.log">log</a></code>,
    such that the values in the log will be pre-pended by the dataloader prefix.</p>
</div>


                            </div>
                            <div id="ModelPT.multi_test_epoch_end" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.multi_test_epoch_end">#&nbsp;&nbsp</a>

                <div class="decorator">@staticmethod</div>

            <span class="def">def</span>
            <span class="name">multi_test_epoch_end</span><span class="signature">(
    outputs: Union[object, List[Dict[str, torch.Tensor]]],
    dataloader_idx: int = 0
) -&gt; None</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">multi_test_epoch_end</span><span class="p">(</span><span class="n">outputs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">object</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]],</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Adds support for multiple test datasets. Should be overridden by subclass,</span>
<span class="sd">        so as to obtain appropriate logs for each of the dataloaders.</span>
<span class="sd">        Args:</span>
<span class="sd">            outputs: Same as that provided by LightningModule.validation_epoch_end()</span>
<span class="sd">                for a single dataloader.</span>
<span class="sd">            dataloader_idx: int representing the index of the dataloader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            A dictionary of values, optionally containing a sub-dict `log`,</span>
<span class="sd">            such that the values in the log will be pre-pended by the dataloader prefix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Multi data loader support has been enabled, but `multi_test_epoch_end(outputs, dataloader_idx) has not &quot;</span>
            <span class="s2">&quot;been implemented.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you require multi data loader support for validation sets, please override this method.</span><span class="se">\n</span><span class="s2">&quot;</span>
            <span class="s2">&quot;If you do not require multi data loader support, please instead override test_epoch_end(outputs).&quot;</span>
        <span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Adds support for multiple test datasets. Should be overridden by subclass,
so as to obtain appropriate logs for each of the dataloaders.
Args:
    outputs: Same as that provided by LightningModule.validation_epoch_end()
        for a single dataloader.
    dataloader_idx: int representing the index of the dataloader.
Returns:
    A dictionary of values, optionally containing a sub-dict <code><a href="#ModelPT.log">log</a></code>,
    such that the values in the log will be pre-pended by the dataloader prefix.</p>
</div>


                            </div>
                            <div id="ModelPT.get_validation_dataloader_prefix" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.get_validation_dataloader_prefix">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">get_validation_dataloader_prefix</span><span class="signature">(self, dataloader_idx: int = 0) -&gt; str</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">get_validation_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
</pre></div>

        </details>

            <div class="docstring"><p>Get the name of one or more data loaders, which will be prepended to all logs.
Args:
    dataloader_idx: Index of the data loader.
Returns:
    str name of the data loader at index provided.</p>
</div>


                            </div>
                            <div id="ModelPT.get_test_dataloader_prefix" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.get_test_dataloader_prefix">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">get_test_dataloader_prefix</span><span class="signature">(self, dataloader_idx: int = 0) -&gt; str</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">get_test_dataloader_prefix</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the name of one or more data loaders, which will be prepended to all logs.</span>
<span class="sd">        Args:</span>
<span class="sd">            dataloader_idx: Index of the data loader.</span>
<span class="sd">        Returns:</span>
<span class="sd">            str name of the data loader at index provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">test_names</span><span class="p">[</span><span class="n">dataloader_idx</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
</pre></div>

        </details>

            <div class="docstring"><p>Get the name of one or more data loaders, which will be prepended to all logs.
Args:
    dataloader_idx: Index of the data loader.
Returns:
    str name of the data loader at index provided.</p>
</div>


                            </div>
                            <div id="ModelPT.load_part_of_state_dict" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.load_part_of_state_dict">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">load_part_of_state_dict</span><span class="signature">(self, state_dict, include, exclude, load_from_string)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">load_part_of_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="n">load_from_string</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Load part of the state dict.&quot;&quot;&quot;</span>
        <span class="n">excluded_param_names</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># create dict</span>
        <span class="n">dict_to_load</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">should_add</span> <span class="o">=</span> <span class="nb">any</span><span class="p">(</span><span class="n">p</span> <span class="ow">in</span> <span class="n">k</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">include</span><span class="p">)</span>
            <span class="c1"># except for if any string from exclude is present</span>
            <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">exclude</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">k</span><span class="p">:</span>
                    <span class="n">excluded_param_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
                    <span class="n">should_add</span> <span class="o">=</span> <span class="kc">False</span>
                    <span class="k">break</span>
            <span class="k">if</span> <span class="n">should_add</span><span class="p">:</span>
                <span class="n">dict_to_load</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="c1"># Restore checkpoint part into current model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">dict_to_load</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint partially restored from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">excluded_param_names</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;The following parameters were excluded from loading from </span><span class="si">{</span><span class="n">load_from_string</span><span class="si">}</span><span class="s2"> : </span><span class="si">{</span><span class="n">excluded_param_names</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Make sure that this is what you wanted!&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Load part of the state dict.</p>
</div>


                            </div>
                            <div id="ModelPT.maybe_init_from_pretrained_checkpoint" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.maybe_init_from_pretrained_checkpoint">#&nbsp;&nbsp</a>

                <div class="decorator">@rank_zero_only</div>

            <span class="def">def</span>
            <span class="name">maybe_init_from_pretrained_checkpoint</span><span class="signature">(self, cfg: omegaconf.omegaconf.OmegaConf, map_location: str = &#39;cpu&#39;)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@rank_zero_only</span>
    <span class="k">def</span> <span class="nf">maybe_init_from_pretrained_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">OmegaConf</span><span class="p">,</span> <span class="n">map_location</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes a given model with the parameters obtained via specific config arguments.</span>
<span class="sd">        The state dict of the provided model will be updated with `strict=False` setting so as to prevent</span>
<span class="sd">        requirement of exact model parameters matching.</span>
<span class="sd">        Initializations:</span>
<span class="sd">            init_from_mridc_model: Str path to a .mridc model, which will be instantiated in order</span>
<span class="sd">                to extract the state dict.</span>
<span class="sd">            init_from_pretrained_model: Str name of a pretrained model checkpoint (obtained via cloud).</span>
<span class="sd">                The model will be downloaded (or a cached copy will be used), instantiated and then</span>
<span class="sd">                its state dict will be extracted.</span>
<span class="sd">            init_from_ptl_ckpt: Str name of a Pytorch Lightning checkpoint file. It will be loaded and</span>
<span class="sd">                the state dict will extracted.</span>
<span class="sd">        Args:</span>
<span class="sd">            cfg: The config used to instantiate the model. It need only contain one of the above keys.</span>
<span class="sd">            map_location: str or torch.device() which represents where the intermediate state dict</span>
<span class="sd">                (from the pretrained model or checkpoint) will be loaded.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;init_from_mridc_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">,</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">]</span>
        <span class="n">arg_matches</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span> <span class="k">if</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">arg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># model weights do not need to be restored</span>
            <span class="k">return</span>

        <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Cannot pass more than one model initialization arguments to config!</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Found : </span><span class="si">{</span><span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">arg_present</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">arg_matches</span><span class="p">)</span> <span class="k">if</span> <span class="n">arg_present</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_mridc_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore model</span>
                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                        <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>
                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from mridc file with path : `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_mridc_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span>
                            <span class="n">model_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;mridc file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_mridc_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_pretrained_model&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="c1"># Restore model</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_name</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_pretrained_model&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

                    <span class="c1"># Check if model is being resumed or not - only works if `Trainer` is attached to model</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;trainer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">trainer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="nb">hasattr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="s2">&quot;resume_from_checkpoint&quot;</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">checkpoint_connector</span><span class="o">.</span><span class="n">resume_checkpoint_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                        <span class="p">):</span>
                            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="s2">&quot;Model training is being resumed via Pytorch Lightning.</span><span class="se">\n</span><span class="s2">&quot;</span>
                                <span class="s2">&quot;Initialization from pretrained model (via cloud) will be skipped.&quot;</span>
                            <span class="p">)</span>
                            <span class="k">return</span>

                    <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                        <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pretrained checkpoint with name : `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_pretrained_model</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">model_name</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">name</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">restored_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
                            <span class="n">model_name</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;init_strict&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                        <span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">restored_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                            <span class="n">include</span><span class="p">,</span>
                            <span class="n">exclude</span><span class="p">,</span>
                            <span class="sa">f</span><span class="s2">&quot;pretrained checkpoint with name `</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">`&quot;</span><span class="p">,</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_pretrained_model is not a string or a dict!&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;init_from_ptl_ckpt&quot;</span> <span class="ow">in</span> <span class="n">cfg</span> <span class="ow">and</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">with</span> <span class="n">open_dict</span><span class="p">(</span><span class="n">cfg</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>  <span class="c1"># type: ignore</span>
                    <span class="c1"># Restore checkpoint</span>
                    <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;init_from_ptl_ckpt&quot;</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
                    <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                    <span class="c1"># Restore checkpoint into current model</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Model checkpoint restored from pytorch lightning checkpoint with path : `</span><span class="si">{</span><span class="n">ckpt_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                    <span class="p">)</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span><span class="p">,</span> <span class="p">(</span><span class="n">DictConfig</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>  <span class="c1"># type: ignore</span>
                    <span class="n">model_load_dict</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">init_from_ptl_ckpt</span>  <span class="c1"># type: ignore</span>
                    <span class="k">for</span> <span class="n">model_load_cfg</span> <span class="ow">in</span> <span class="n">model_load_dict</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                        <span class="n">ckpt_path</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">path</span>
                        <span class="c1"># Restore model</span>
                        <span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">ckpt_path</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

                        <span class="n">include</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;include&quot;</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;&quot;</span><span class="p">])</span>
                        <span class="n">exclude</span> <span class="o">=</span> <span class="n">model_load_cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;exclude&quot;</span><span class="p">,</span> <span class="p">[])</span>

                        <span class="bp">self</span><span class="o">.</span><span class="n">load_part_of_state_dict</span><span class="p">(</span>
                            <span class="n">ckpt</span><span class="p">[</span><span class="s2">&quot;state_dict&quot;</span><span class="p">],</span> <span class="n">include</span><span class="p">,</span> <span class="n">exclude</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;nemo file with path `</span><span class="si">{</span><span class="n">model_path</span><span class="si">}</span><span class="s2">`&quot;</span>
                        <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;Invalid type: init_from_ptl_ckpt is not a string or a dict!&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Initializes a given model with the parameters obtained via specific config arguments.
The state dict of the provided model will be updated with <code>strict=False</code> setting so as to prevent
requirement of exact model parameters matching.
Initializations:
    init_from_mridc_model: Str path to a .mridc model, which will be instantiated in order
        to extract the state dict.
    init_from_pretrained_model: Str name of a pretrained model checkpoint (obtained via cloud).
        The model will be downloaded (or a cached copy will be used), instantiated and then
        its state dict will be extracted.
    init_from_ptl_ckpt: Str name of a Pytorch Lightning checkpoint file. It will be loaded and
        the state dict will extracted.
Args:
    cfg: The config used to instantiate the model. It need only contain one of the above keys.
    map_location: str or torch.device() which represents where the intermediate state dict
        (from the pretrained model or checkpoint) will be loaded.</p>
</div>


                            </div>
                            <div id="ModelPT.teardown" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.teardown">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">teardown</span><span class="signature">(self, stage: str)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">teardown</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Called at the end of fit and test.</span>
<span class="sd">        Args:</span>
<span class="sd">            stage: either &#39;fit&#39; or &#39;test&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">stage</span> <span class="o">==</span> <span class="s2">&quot;fit&quot;</span> <span class="ow">and</span> <span class="s2">&quot;PL_TRAINER_GPUS&quot;</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;PL_TRAINER_GPUS&quot;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">teardown</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Called at the end of fit and test.
Args:
    stage: either 'fit' or 'test'</p>
</div>


                            </div>
                            <div id="ModelPT.extract_state_dict_from" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.extract_state_dict_from">#&nbsp;&nbsp</a>

                <div class="decorator">@classmethod</div>

            <span class="def">def</span>
            <span class="name">extract_state_dict_from</span><span class="signature">(
    cls,
    restore_path: str,
    save_dir: str,
    split_by_module: bool = False,
    save_restore_connector: <a href="../connectors/save_restore_connector.html#SaveRestoreConnector">mridc.core.connectors.save_restore_connector.SaveRestoreConnector</a> = None
)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">extract_state_dict_from</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">restore_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">split_by_module</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">save_restore_connector</span><span class="p">:</span> <span class="n">SaveRestoreConnector</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extract the state dict(s) from a provided .mridc tarfile and save it to a directory.</span>
<span class="sd">        Args:</span>
<span class="sd">            restore_path: path to .mridc file from which state dict(s) should be extracted</span>
<span class="sd">            save_dir: directory in which the saved state dict(s) should be stored</span>
<span class="sd">            split_by_module: bool flag, which determines whether the output checkpoint should</span>
<span class="sd">                be for the entire Model, or the individual module&#39;s that comprise the Model</span>
<span class="sd">            save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.</span>
<span class="sd">        Example:</span>
<span class="sd">            To convert the .mridc tarfile into a single Model level PyTorch checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;)</span>
<span class="sd">            To restore a model from a Model level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            model.load_state_dict(torch.load(&quot;./asr_ckpts/model_weights.ckpt&quot;))</span>
<span class="sd">            To convert the .mridc tarfile into multiple Module level PyTorch checkpoints</span>
<span class="sd">            ::</span>
<span class="sd">            state_dict = mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from(&#39;asr.mridc&#39;,</span>
<span class="sd">                        &#39;./asr_ckpts&#39;, split_by_module=True)</span>
<span class="sd">            To restore a module from a Module level checkpoint</span>
<span class="sd">            ::</span>
<span class="sd">            model = mridc.collections.asr.models.EncDecCTCModel(cfg)  # or any other method of restoration</span>
<span class="sd">            # load the individual components</span>
<span class="sd">            model.preprocessor.load_state_dict(torch.load(&quot;./asr_ckpts/preprocessor.ckpt&quot;))</span>
<span class="sd">            model.encoder.load_state_dict(torch.load(&quot;./asr_ckpts/encoder.ckpt&quot;))</span>
<span class="sd">            model.decoder.load_state_dict(torch.load(&quot;./asr_ckpts/decoder.ckpt&quot;))</span>
<span class="sd">        Returns:</span>
<span class="sd">            The state dict that was loaded from the original .mridc checkpoint</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">save_restore_connector</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">save_restore_connector</span> <span class="o">=</span> <span class="n">SaveRestoreConnector</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">restore_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">FileExistsError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Can&#39;t find </span><span class="si">{</span><span class="n">restore_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="bp">cls</span><span class="o">.</span><span class="n">update_save_restore_connector</span><span class="p">(</span><span class="n">save_restore_connector</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span><span class="o">.</span><span class="n">extract_state_dict_from</span><span class="p">(</span><span class="n">restore_path</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">,</span> <span class="n">split_by_module</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Extract the state dict(s) from a provided .mridc tarfile and save it to a directory.
Args:
    restore_path: path to .mridc file from which state dict(s) should be extracted
    save_dir: directory in which the saved state dict(s) should be stored
    split_by_module: bool flag, which determines whether the output checkpoint should
        be for the entire Model, or the individual module's that comprise the Model
    save_restore_connector (SaveRestoreConnector): Can be overridden to add custom save and restore logic.
Example:
    To convert the .mridc tarfile into a single Model level PyTorch checkpoint
    ::
    state_dict = <a href="../../collections.html#asr.models.EncDecCTCModel.extract_state_dict_from">mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from</a>('asr.mridc',
                './asr_ckpts')
    To restore a model from a Model level checkpoint
    ::
    model = <a href="../../collections.html#asr.models.EncDecCTCModel">mridc.collections.asr.models.EncDecCTCModel</a>(cfg)  # or any other method of restoration
    model.load_state_dict(torch.load("./asr_ckpts/model_weights.ckpt"))
    To convert the .mridc tarfile into multiple Module level PyTorch checkpoints
    ::
    state_dict = <a href="../../collections.html#asr.models.EncDecCTCModel.extract_state_dict_from">mridc.collections.asr.models.EncDecCTCModel.extract_state_dict_from</a>('asr.mridc',
                './asr_ckpts', split_by_module=True)
    To restore a module from a Module level checkpoint
    ::
    model = <a href="../../collections.html#asr.models.EncDecCTCModel">mridc.collections.asr.models.EncDecCTCModel</a>(cfg)  # or any other method of restoration
    # load the individual components
    model.preprocessor.load_state_dict(torch.load("./asr_ckpts/preprocessor.ckpt"))
    model.encoder.load_state_dict(torch.load("./asr_ckpts/encoder.ckpt"))
    model.decoder.load_state_dict(torch.load("./asr_ckpts/decoder.ckpt"))
Returns:
    The state dict that was loaded from the original .mridc checkpoint</p>
</div>


                            </div>
                            <div id="ModelPT.prepare_test" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.prepare_test">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">prepare_test</span><span class="signature">(self, trainer: pytorch_lightning.trainer.trainer.Trainer) -&gt; bool</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">prepare_test</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="s2">&quot;Trainer&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper method to check whether the model can safely be tested</span>
<span class="sd">        on a dataset after training (or loading a checkpoint).</span>
<span class="sd">        ::</span>
<span class="sd">            trainer = Trainer()</span>
<span class="sd">            if model.prepare_test(trainer):</span>
<span class="sd">                trainer.test(model)</span>
<span class="sd">        Returns:</span>
<span class="sd">            bool which declares the model safe to test. Provides warnings if it has to</span>
<span class="sd">            return False to guide the user.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cfg</span><span class="p">,</span> <span class="s2">&quot;test_ds&quot;</span><span class="p">):</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;No `test_ds` config found within the manifest.&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="n">trainer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Replace ddp multi-gpu until PTL has a fix</span>
            <span class="n">DDP_WARN</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span><span class="se">\n\n</span><span class="s2">During testing, it is currently advisable to construct a new Trainer &quot;</span>
<span class="s2">                    &quot;with single GPU and no DDP to obtain accurate results.</span>
<span class="s2">                    &quot;Following pattern should be used: &quot;</span>
<span class="s2">                    &quot;gpu = 1 if cfg.trainer.gpus != 0 else 0&quot;</span>
<span class="s2">                    &quot;trainer = Trainer(gpus=gpu)&quot;</span>
<span class="s2">                    &quot;if model.prepare_test(trainer):&quot;</span>
<span class="s2">                    &quot;  trainer.test(model)</span><span class="se">\n\n</span><span class="s2">&quot;&quot;&quot;</span>

            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">DDP_WARN</span><span class="p">)</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Assign trainer to the model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_trainer</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
</pre></div>

        </details>

            <div class="docstring"><p>Helper method to check whether the model can safely be tested
on a dataset after training (or loading a checkpoint).
::
    trainer = Trainer()
    if model.prepare_test(trainer):
        trainer.test(model)
Returns:
    bool which declares the model safe to test. Provides warnings if it has to
    return False to guide the user.</p>
</div>


                            </div>
                            <div id="ModelPT.set_trainer" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.set_trainer">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">set_trainer</span><span class="signature">(self, trainer: pytorch_lightning.trainer.trainer.Trainer)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">set_trainer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Set an instance of Trainer object.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer: PyTorch Lightning Trainer object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Set an instance of Trainer object.
Args:
    trainer: PyTorch Lightning Trainer object.</p>
</div>


                            </div>
                            <div id="ModelPT.set_world_size" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.set_world_size">#&nbsp;&nbsp</a>


            <span class="def">def</span>
            <span class="name">set_world_size</span><span class="signature">(self, trainer: pytorch_lightning.trainer.trainer.Trainer)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="k">def</span> <span class="nf">set_world_size</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">:</span> <span class="n">Trainer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Determines the world size from the PyTorch Lightning Trainer.</span>
<span class="sd">        And then updates AppState.</span>
<span class="sd">        Args:</span>
<span class="sd">            trainer (Trainer): PyTorch Lightning Trainer object</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Update AppState with world information from trainer</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">Trainer</span><span class="p">):</span>
            <span class="n">app_state</span> <span class="o">=</span> <span class="n">AppState</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
                <span class="n">app_state</span><span class="o">.</span><span class="n">world_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trainer</span><span class="o">.</span><span class="n">num_nodes</span>  <span class="c1"># type: ignore</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;World size can only be set by PyTorch Lightning Trainer.&quot;</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Determines the world size from the PyTorch Lightning Trainer.
And then updates AppState.
Args:
    trainer (Trainer): PyTorch Lightning Trainer object</p>
</div>


                            </div>
                            <div id="ModelPT.num_weights" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ModelPT.num_weights">#&nbsp;&nbsp</a>

        <span class="name">num_weights</span>
    </div>


            <div class="docstring"><p>Utility property that returns the total number of parameters of the Model.</p>
</div>


                            </div>
                            <div id="ModelPT.cfg" class="classattr">
                                            <div class="attr variable"><a class="headerlink" href="#ModelPT.cfg">#&nbsp;&nbsp</a>

        <span class="name">cfg</span>
    </div>


            <div class="docstring"><p>Property that holds the finalized internal config of the model.
Note:
    Changes to this config are not reflected in the state of the model.
    Please create a new model using an updated config to properly update the model.</p>
</div>


                            </div>
                            <div id="ModelPT.update_save_restore_connector" class="classattr">
                                        <div class="attr function"><a class="headerlink" href="#ModelPT.update_save_restore_connector">#&nbsp;&nbsp</a>

                <div class="decorator">@classmethod</div>

            <span class="def">def</span>
            <span class="name">update_save_restore_connector</span><span class="signature">(cls, save_restore_connector)</span>:
    </div>

            <details>
            <summary>View Source</summary>
            <div class="pdoc-code codehilite"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">update_save_restore_connector</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update the save_restore_connector of the model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">):</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_save_restore_connector</span> <span class="o">=</span> <span class="n">save_restore_connector</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="s2">&quot;_save_restore_connector&quot;</span><span class="p">,</span> <span class="n">save_restore_connector</span><span class="p">)</span>
</pre></div>

        </details>

            <div class="docstring"><p>Update the save_restore_connector of the model.</p>
</div>


                            </div>
                            <div class="inherited">
                                <h5>Inherited Members</h5>
                                <dl>
                                    <div><dt>pytorch_lightning.core.lightning.LightningModule</dt>
                                <dd id="ModelPT.optimizers" class="function">optimizers</dd>
                <dd id="ModelPT.lr_schedulers" class="function">lr_schedulers</dd>
                <dd id="ModelPT.example_input_array" class="variable">example_input_array</dd>
                <dd id="ModelPT.current_epoch" class="variable">current_epoch</dd>
                <dd id="ModelPT.global_step" class="variable">global_step</dd>
                <dd id="ModelPT.global_rank" class="variable">global_rank</dd>
                <dd id="ModelPT.local_rank" class="variable">local_rank</dd>
                <dd id="ModelPT.loaded_optimizer_states_dict" class="variable">loaded_optimizer_states_dict</dd>
                <dd id="ModelPT.on_gpu" class="variable">on_gpu</dd>
                <dd id="ModelPT.automatic_optimization" class="variable">automatic_optimization</dd>
                <dd id="ModelPT.truncated_bptt_steps" class="variable">truncated_bptt_steps</dd>
                <dd id="ModelPT.logger" class="variable">logger</dd>
                <dd id="ModelPT.print" class="function">print</dd>
                <dd id="ModelPT.log" class="function">log</dd>
                <dd id="ModelPT.log_dict" class="function">log_dict</dd>
                <dd id="ModelPT.log_grad_norm" class="function">log_grad_norm</dd>
                <dd id="ModelPT.all_gather" class="function">all_gather</dd>
                <dd id="ModelPT.forward" class="function">forward</dd>
                <dd id="ModelPT.training_step" class="function">training_step</dd>
                <dd id="ModelPT.training_step_end" class="function">training_step_end</dd>
                <dd id="ModelPT.training_epoch_end" class="function">training_epoch_end</dd>
                <dd id="ModelPT.validation_step" class="function">validation_step</dd>
                <dd id="ModelPT.validation_step_end" class="function">validation_step_end</dd>
                <dd id="ModelPT.test_step" class="function">test_step</dd>
                <dd id="ModelPT.test_step_end" class="function">test_step_end</dd>
                <dd id="ModelPT.predict_step" class="function">predict_step</dd>
                <dd id="ModelPT.configure_callbacks" class="function">configure_callbacks</dd>
                <dd id="ModelPT.manual_backward" class="function">manual_backward</dd>
                <dd id="ModelPT.backward" class="function">backward</dd>
                <dd id="ModelPT.toggle_optimizer" class="function">toggle_optimizer</dd>
                <dd id="ModelPT.untoggle_optimizer" class="function">untoggle_optimizer</dd>
                <dd id="ModelPT.clip_gradients" class="function">clip_gradients</dd>
                <dd id="ModelPT.configure_gradient_clipping" class="function">configure_gradient_clipping</dd>
                <dd id="ModelPT.optimizer_step" class="function">optimizer_step</dd>
                <dd id="ModelPT.optimizer_zero_grad" class="function">optimizer_zero_grad</dd>
                <dd id="ModelPT.tbptt_split_batch" class="function">tbptt_split_batch</dd>
                <dd id="ModelPT.summarize" class="function">summarize</dd>
                <dd id="ModelPT.freeze" class="function">freeze</dd>
                <dd id="ModelPT.unfreeze" class="function">unfreeze</dd>
                <dd id="ModelPT.get_progress_bar_dict" class="function">get_progress_bar_dict</dd>
                <dd id="ModelPT.to_onnx" class="function">to_onnx</dd>
                <dd id="ModelPT.to_torchscript" class="function">to_torchscript</dd>
                <dd id="ModelPT.model_size" class="variable">model_size</dd>
                <dd id="ModelPT.add_to_queue" class="function">add_to_queue</dd>
                <dd id="ModelPT.get_from_queue" class="function">get_from_queue</dd>

            </div>
            <div><dt>pytorch_lightning.core.mixins.device_dtype_mixin.DeviceDtypeModuleMixin</dt>
                                <dd id="ModelPT.dtype" class="variable">dtype</dd>
                <dd id="ModelPT.device" class="variable">device</dd>
                <dd id="ModelPT.to" class="function">to</dd>
                <dd id="ModelPT.cuda" class="function">cuda</dd>
                <dd id="ModelPT.cpu" class="function">cpu</dd>
                <dd id="ModelPT.type" class="function">type</dd>
                <dd id="ModelPT.float" class="function">float</dd>
                <dd id="ModelPT.double" class="function">double</dd>
                <dd id="ModelPT.half" class="function">half</dd>

            </div>
            <div><dt>pytorch_lightning.core.mixins.hparams_mixin.HyperparametersMixin</dt>
                                <dd id="ModelPT.save_hyperparameters" class="function">save_hyperparameters</dd>
                <dd id="ModelPT.hparams" class="variable">hparams</dd>
                <dd id="ModelPT.hparams_initial" class="variable">hparams_initial</dd>

            </div>
            <div><dt>pytorch_lightning.core.saving.ModelIO</dt>
                                <dd id="ModelPT.CHECKPOINT_HYPER_PARAMS_KEY" class="variable">CHECKPOINT_HYPER_PARAMS_KEY</dd>
                <dd id="ModelPT.CHECKPOINT_HYPER_PARAMS_NAME" class="variable">CHECKPOINT_HYPER_PARAMS_NAME</dd>
                <dd id="ModelPT.CHECKPOINT_HYPER_PARAMS_TYPE" class="variable">CHECKPOINT_HYPER_PARAMS_TYPE</dd>
                <dd id="ModelPT.on_hpc_save" class="function">on_hpc_save</dd>
                <dd id="ModelPT.on_hpc_load" class="function">on_hpc_load</dd>

            </div>
            <div><dt>pytorch_lightning.core.hooks.ModelHooks</dt>
                                <dd id="ModelPT.on_fit_start" class="function">on_fit_start</dd>
                <dd id="ModelPT.on_fit_end" class="function">on_fit_end</dd>
                <dd id="ModelPT.on_train_start" class="function">on_train_start</dd>
                <dd id="ModelPT.on_train_end" class="function">on_train_end</dd>
                <dd id="ModelPT.on_validation_start" class="function">on_validation_start</dd>
                <dd id="ModelPT.on_validation_end" class="function">on_validation_end</dd>
                <dd id="ModelPT.on_test_start" class="function">on_test_start</dd>
                <dd id="ModelPT.on_test_end" class="function">on_test_end</dd>
                <dd id="ModelPT.on_predict_start" class="function">on_predict_start</dd>
                <dd id="ModelPT.on_predict_end" class="function">on_predict_end</dd>
                <dd id="ModelPT.on_pretrain_routine_start" class="function">on_pretrain_routine_start</dd>
                <dd id="ModelPT.on_pretrain_routine_end" class="function">on_pretrain_routine_end</dd>
                <dd id="ModelPT.on_train_batch_start" class="function">on_train_batch_start</dd>
                <dd id="ModelPT.on_train_batch_end" class="function">on_train_batch_end</dd>
                <dd id="ModelPT.on_validation_batch_start" class="function">on_validation_batch_start</dd>
                <dd id="ModelPT.on_validation_batch_end" class="function">on_validation_batch_end</dd>
                <dd id="ModelPT.on_test_batch_start" class="function">on_test_batch_start</dd>
                <dd id="ModelPT.on_test_batch_end" class="function">on_test_batch_end</dd>
                <dd id="ModelPT.on_predict_batch_start" class="function">on_predict_batch_start</dd>
                <dd id="ModelPT.on_predict_batch_end" class="function">on_predict_batch_end</dd>
                <dd id="ModelPT.on_validation_model_eval" class="function">on_validation_model_eval</dd>
                <dd id="ModelPT.on_validation_model_train" class="function">on_validation_model_train</dd>
                <dd id="ModelPT.on_test_model_train" class="function">on_test_model_train</dd>
                <dd id="ModelPT.on_test_model_eval" class="function">on_test_model_eval</dd>
                <dd id="ModelPT.on_predict_model_eval" class="function">on_predict_model_eval</dd>
                <dd id="ModelPT.on_epoch_start" class="function">on_epoch_start</dd>
                <dd id="ModelPT.on_epoch_end" class="function">on_epoch_end</dd>
                <dd id="ModelPT.on_train_epoch_start" class="function">on_train_epoch_start</dd>
                <dd id="ModelPT.on_train_epoch_end" class="function">on_train_epoch_end</dd>
                <dd id="ModelPT.on_validation_epoch_start" class="function">on_validation_epoch_start</dd>
                <dd id="ModelPT.on_validation_epoch_end" class="function">on_validation_epoch_end</dd>
                <dd id="ModelPT.on_test_epoch_start" class="function">on_test_epoch_start</dd>
                <dd id="ModelPT.on_test_epoch_end" class="function">on_test_epoch_end</dd>
                <dd id="ModelPT.on_predict_epoch_start" class="function">on_predict_epoch_start</dd>
                <dd id="ModelPT.on_predict_epoch_end" class="function">on_predict_epoch_end</dd>
                <dd id="ModelPT.on_before_zero_grad" class="function">on_before_zero_grad</dd>
                <dd id="ModelPT.on_before_backward" class="function">on_before_backward</dd>
                <dd id="ModelPT.on_after_backward" class="function">on_after_backward</dd>
                <dd id="ModelPT.on_before_optimizer_step" class="function">on_before_optimizer_step</dd>
                <dd id="ModelPT.on_post_move_to_device" class="function">on_post_move_to_device</dd>
                <dd id="ModelPT.configure_sharded_model" class="function">configure_sharded_model</dd>

            </div>
            <div><dt>pytorch_lightning.core.hooks.DataHooks</dt>
                                <dd id="ModelPT.prepare_data" class="function">prepare_data</dd>
                <dd id="ModelPT.setup" class="function">setup</dd>
                <dd id="ModelPT.predict_dataloader" class="function">predict_dataloader</dd>
                <dd id="ModelPT.on_train_dataloader" class="function">on_train_dataloader</dd>
                <dd id="ModelPT.on_val_dataloader" class="function">on_val_dataloader</dd>
                <dd id="ModelPT.on_test_dataloader" class="function">on_test_dataloader</dd>
                <dd id="ModelPT.on_predict_dataloader" class="function">on_predict_dataloader</dd>
                <dd id="ModelPT.transfer_batch_to_device" class="function">transfer_batch_to_device</dd>
                <dd id="ModelPT.on_before_batch_transfer" class="function">on_before_batch_transfer</dd>
                <dd id="ModelPT.on_after_batch_transfer" class="function">on_after_batch_transfer</dd>

            </div>
            <div><dt>pytorch_lightning.core.hooks.CheckpointHooks</dt>
                                <dd id="ModelPT.on_load_checkpoint" class="function">on_load_checkpoint</dd>
                <dd id="ModelPT.on_save_checkpoint" class="function">on_save_checkpoint</dd>

            </div>
            <div><dt>torch.nn.modules.module.Module</dt>
                                <dd id="ModelPT.dump_patches" class="variable">dump_patches</dd>
                <dd id="ModelPT.register_buffer" class="function">register_buffer</dd>
                <dd id="ModelPT.register_parameter" class="function">register_parameter</dd>
                <dd id="ModelPT.add_module" class="function">add_module</dd>
                <dd id="ModelPT.get_submodule" class="function">get_submodule</dd>
                <dd id="ModelPT.get_parameter" class="function">get_parameter</dd>
                <dd id="ModelPT.get_buffer" class="function">get_buffer</dd>
                <dd id="ModelPT.get_extra_state" class="function">get_extra_state</dd>
                <dd id="ModelPT.set_extra_state" class="function">set_extra_state</dd>
                <dd id="ModelPT.apply" class="function">apply</dd>
                <dd id="ModelPT.xpu" class="function">xpu</dd>
                <dd id="ModelPT.bfloat16" class="function">bfloat16</dd>
                <dd id="ModelPT.to_empty" class="function">to_empty</dd>
                <dd id="ModelPT.register_backward_hook" class="function">register_backward_hook</dd>
                <dd id="ModelPT.register_full_backward_hook" class="function">register_full_backward_hook</dd>
                <dd id="ModelPT.register_forward_pre_hook" class="function">register_forward_pre_hook</dd>
                <dd id="ModelPT.register_forward_hook" class="function">register_forward_hook</dd>
                <dd id="ModelPT.T_destination" class="variable">T_destination</dd>
                <dd id="ModelPT.state_dict" class="function">state_dict</dd>
                <dd id="ModelPT.load_state_dict" class="function">load_state_dict</dd>
                <dd id="ModelPT.parameters" class="function">parameters</dd>
                <dd id="ModelPT.named_parameters" class="function">named_parameters</dd>
                <dd id="ModelPT.buffers" class="function">buffers</dd>
                <dd id="ModelPT.named_buffers" class="function">named_buffers</dd>
                <dd id="ModelPT.children" class="function">children</dd>
                <dd id="ModelPT.named_children" class="function">named_children</dd>
                <dd id="ModelPT.modules" class="function">modules</dd>
                <dd id="ModelPT.named_modules" class="function">named_modules</dd>
                <dd id="ModelPT.train" class="function">train</dd>
                <dd id="ModelPT.eval" class="function">eval</dd>
                <dd id="ModelPT.requires_grad_" class="function">requires_grad_</dd>
                <dd id="ModelPT.zero_grad" class="function">zero_grad</dd>
                <dd id="ModelPT.share_memory" class="function">share_memory</dd>
                <dd id="ModelPT.extra_repr" class="function">extra_repr</dd>

            </div>
            <div><dt><a href="common.html#Model">mridc.core.classes.common.Model</a></dt>
                                <dd id="ModelPT.list_available_models" class="function"><a href="common.html#Model.list_available_models">list_available_models</a></dd>
                <dd id="ModelPT.get_available_model_names" class="function"><a href="common.html#Model.get_available_model_names">get_available_model_names</a></dd>
                <dd id="ModelPT.from_pretrained" class="function"><a href="common.html#Model.from_pretrained">from_pretrained</a></dd>

            </div>
            <div><dt><a href="common.html#Typing">mridc.core.classes.common.Typing</a></dt>
                                <dd id="ModelPT.input_types" class="variable"><a href="common.html#Typing.input_types">input_types</a></dd>
                <dd id="ModelPT.output_types" class="variable"><a href="common.html#Typing.output_types">output_types</a></dd>

            </div>
            <div><dt><a href="common.html#Serialization">mridc.core.classes.common.Serialization</a></dt>
                                <dd id="ModelPT.from_config_dict" class="function"><a href="common.html#Serialization.from_config_dict">from_config_dict</a></dd>
                <dd id="ModelPT.to_config_dict" class="function"><a href="common.html#Serialization.to_config_dict">to_config_dict</a></dd>

            </div>
            <div><dt><a href="common.html#FileIO">mridc.core.classes.common.FileIO</a></dt>
                                <dd id="ModelPT.from_config_file" class="function"><a href="common.html#FileIO.from_config_file">from_config_file</a></dd>
                <dd id="ModelPT.to_config_file" class="function"><a href="common.html#FileIO.to_config_file">to_config_file</a></dd>

            </div>
                                </dl>
                            </div>
                </section>
    </main>
<script>
    function escapeHTML(html) {
        return document.createElement('div').appendChild(document.createTextNode(html)).parentNode.innerHTML;
    }

    const originalContent = document.querySelector("main.pdoc");
    let currentContent = originalContent;

    function setContent(innerHTML) {
        let elem;
        if (innerHTML) {
            elem = document.createElement("main");
            elem.classList.add("pdoc");
            elem.innerHTML = innerHTML;
        } else {
            elem = originalContent;
        }
        if (currentContent !== elem) {
            currentContent.replaceWith(elem);
            currentContent = elem;
        }
    }

    function getSearchTerm() {
        return (new URL(window.location)).searchParams.get("search");
    }

    const searchBox = document.querySelector(".pdoc input[type=search]");
    searchBox.addEventListener("input", function () {
        let url = new URL(window.location);
        if (searchBox.value.trim()) {
            url.hash = "";
            url.searchParams.set("search", searchBox.value);
        } else {
            url.searchParams.delete("search");
        }
        history.replaceState("", "", url.toString());
        onInput();
    });
    window.addEventListener("popstate", onInput);


    let search, searchErr;

    async function initialize() {
        try {
            search = await new Promise((resolve, reject) => {
                const script = document.createElement("script");
                script.type = "text/javascript";
                script.async = true;
                script.onload = () => resolve(window.pdocSearch);
                script.onerror = (e) => reject(e);
                script.src = "../../../search.js";
                document.getElementsByTagName("head")[0].appendChild(script);
            });
        } catch (e) {
            console.error("Cannot fetch pdoc search index");
            searchErr = "Cannot fetch search index.";
        }
        onInput();

        document.querySelector("nav.pdoc").addEventListener("click", e => {
            if (e.target.hash) {
                searchBox.value = "";
                searchBox.dispatchEvent(new Event("input"));
            }
        });
    }

    function onInput() {
        setContent((() => {
            const term = getSearchTerm();
            if (!term) {
                return null
            }
            if (searchErr) {
                return `<h3>Error: ${searchErr}</h3>`
            }
            if (!search) {
                return "<h3>Searching...</h3>"
            }

            window.scrollTo({top: 0, left: 0, behavior: 'auto'});

            const results = search(term);

            let html;
            if (results.length === 0) {
                html = `No search results for '${escapeHTML(term)}'.`
            } else {
                html = `<h4>${results.length} search result${results.length > 1 ? "s" : ""} for '${escapeHTML(term)}'.</h4>`;
            }
            for (let result of results.slice(0, 10)) {
                let doc = result.doc;
                let url = `../../../${doc.modulename.replaceAll(".", "/")}.html`;
                if (doc.qualname) {
                    url += `#${doc.qualname}`;
                }

                let heading;
                switch (result.doc.type) {
                    case "function":
                        heading = `<span class="def">${doc.funcdef}</span> <span class="name">${doc.fullname}</span><span class="signature">${doc.signature}:</span>`;
                        break;
                    case "class":
                        heading = `<span class="def">class</span> <span class="name">${doc.fullname}</span>`;
                        if (doc.bases)
                            heading += `<wbr>(<span class="base">${doc.bases}</span>)`;
                        heading += `:`;
                        break;
                    case "variable":
                        heading = `<span class="name">${doc.fullname}</span>`;
                        if (doc.annotation)
                            heading += `<span class="annotation">${doc.annotation}</span>`;
                        if (doc.default_value)
                            heading += `<span class="default_value">${doc.default_value}</span>`;
                        break;
                    default:
                        heading = `<span class="name">${doc.fullname}</span>`;
                        break;
                }
                html += `
                        <section class="search-result">
                        <a href="${url}" class="attr ${doc.type}">${heading}</a>
                        <div class="docstring">${doc.doc}</div>
                        </section>
                    `;

            }
            return html;
        })());
    }

    if (getSearchTerm()) {
        initialize();
        searchBox.value = getSearchTerm();
        onInput();
    } else {
        searchBox.addEventListener("focus", initialize, {once: true});
    }

    searchBox.addEventListener("keydown", e => {
        if (["ArrowDown", "ArrowUp", "Enter"].includes(e.key)) {
            let focused = currentContent.querySelector(".search-result.focused");
            if (!focused) {
                currentContent.querySelector(".search-result").classList.add("focused");
            } else if (
                e.key === "ArrowDown"
                && focused.nextElementSibling
                && focused.nextElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.nextElementSibling.classList.add("focused");
                focused.nextElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "ArrowUp"
                && focused.previousElementSibling
                && focused.previousElementSibling.classList.contains("search-result")
            ) {
                focused.classList.remove("focused");
                focused.previousElementSibling.classList.add("focused");
                focused.previousElementSibling.scrollIntoView({
                    behavior: "smooth",
                    block: "nearest",
                    inline: "nearest"
                });
            } else if (
                e.key === "Enter"
            ) {
                focused.querySelector("a").click();
            }
        }
    });
</script></body>
</html>
